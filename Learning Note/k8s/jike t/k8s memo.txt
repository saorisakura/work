Docker
	OS requirements
	To install Docker CE, you need a maintained version of CentOS 7. Archived versions aren’t supported or tested.

	The centos-extras repository must be enabled. This repository is enabled by default, but if you have disabled it, you need to re-enable it.

	The overlay2 storage driver is recommended.

	Uninstall old versions
	Older versions of Docker were called docker or docker-engine. If these are installed, uninstall them, along with associated dependencies.

	$ sudo yum remove docker \
					  docker-client \
					  docker-client-latest \
					  docker-common \
					  docker-latest \
					  docker-latest-logrotate \
					  docker-logrotate \
					  docker-engine
	It’s OK if yum reports that none of these packages are installed.

	The contents of /var/lib/docker/, including images, containers, volumes, and networks, are preserved. The Docker CE package is now called docker-ce.
	
	Install using the repository
	Before you install Docker CE for the first time on a new host machine, you need to set up the Docker repository. Afterward, you can install and update Docker from the repository.

	SET UP THE REPOSITORY
	Install required packages. yum-utils provides the yum-config-manager utility, and device-mapper-persistent-data and lvm2 are required by the devicemapper storage driver.

	$ sudo yum install -y yum-utils \
	  device-mapper-persistent-data \
	  lvm2
	Use the following command to set up the stable repository.

	$ sudo yum-config-manager \
		--add-repo \
		https://download.docker.com/linux/centos/docker-ce.repo
	
	INSTALL DOCKER CE
	yum install docker-ce docker-ce-cli containerd.io
	yum install docker-ce-<VERSION_STRING> docker-ce-cli-<VERSION_STRING> containerd.io
	systemctl start docker
	docker run hello-world
	
	UPGRADE DOCKER CE
	To upgrade Docker CE, follow the installation instructions, choosing the new version you want to install.
	
	Install from a package
	https://download.docker.com/linux/centos/7/x86_64/stable/Packages/ 
	
	yum install /path/to/package.rpm

Namespace
	$ docker run -it busybox /bin/sh
	/ #
	这个命令是 Docker 项目最重要的一个操作，即大名鼎鼎的 docker run。

	而 -it 参数告诉了 Docker 项目在启动容器后，需要给我们分配一个文本输入 / 输出环境，也就是 TTY，跟容器的标准输入相关联，这样我们就可以和这个 Docker 容器进行交互了。而 /bin/sh 就是我们要在 Docker 容器里运行的程序。

	所以，上面这条指令翻译成人类的语言就是：请帮我启动一个容器，在容器里执行 /bin/sh，并且给我分配一个命令行终端跟这个容器交互。

	这样，我的 Ubuntu 16.04 机器就变成了一个宿主机，而一个运行着 /bin/sh 的容器，就跑在了这个宿主机里面。

	上面的例子和原理，如果你已经玩过 Docker，一定不会感到陌生。此时，如果我们在容器里执行一下 ps 指令，就会发现一些更有趣的事情：

	/ # ps
	PID  USER   TIME COMMAND
	  1 root   0:00 /bin/sh
	  10 root   0:00 ps
	可以看到，我们在 Docker 里最开始执行的 /bin/sh，就是这个容器内部的第 1 号进程（PID=1），而这个容器里一共只有两个进程在运行。这就意味着，前面执行的 /bin/sh，以及我们刚刚执行的 ps，已经被 Docker 隔离在了一个跟宿主机完全不同的世界当中。

	这究竟是怎么做到呢？

	本来，每当我们在宿主机上运行了一个 /bin/sh 程序，操作系统都会给它分配一个进程编号，比如 PID=100。这个编号是进程的唯一标识，就像员工的工牌一样。所以 PID=100，可以粗略地理解为这个 /bin/sh 是我们公司里的第 100 号员工，而第 1 号员工就自然是比尔 · 盖茨这样统领全局的人物。

	而现在，我们要通过 Docker 把这个 /bin/sh 程序运行在一个容器当中。这时候，Docker 就会在这个第 100 号员工入职时给他施一个“障眼法”，让他永远看不到前面的其他 99 个员工，更看不到比尔 · 盖茨。这样，他就会错误地以为自己就是公司里的第 1 号员工。

	这种机制，其实就是对被隔离应用的进程空间做了手脚，使得这些进程只能看到重新计算过的进程编号，比如 PID=1。可实际上，他们在宿主机的操作系统里，还是原来的第 100 号进程。

	这种技术，就是 Linux 里面的 Namespace 机制。而 Namespace 的使用方式也非常有意思：它其实只是 Linux 创建新进程的一个可选参数。我们知道，在 Linux 系统中创建线程的系统调用是 clone()，比如：

	int pid = clone(main_function, stack_size, SIGCHLD, NULL); 
	这个系统调用就会为我们创建一个新的进程，并且返回它的进程号 pid。

	而当我们用 clone() 系统调用创建一个新进程时，就可以在参数中指定 CLONE_NEWPID 参数，比如：

	int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); 
	这时，新创建的这个进程将会“看到”一个全新的进程空间，在这个进程空间里，它的 PID 是 1。之所以说“看到”，是因为这只是一个“障眼法”，在宿主机真实的进程空间里，这个进程的 PID 还是真实的数值，比如 100。

	当然，我们还可以多次执行上面的 clone() 调用，这样就会创建多个 PID Namespace，而每个 Namespace 里的应用进程，都会认为自己是当前容器里的第 1 号进程，它们既看不到宿主机里真正的进程空间，也看不到其他 PID Namespace 里的具体情况。

	而除了我们刚刚用到的 PID Namespace，Linux 操作系统还提供了 Mount、UTS、IPC、Network 和 User 这些 Namespace，用来对各种不同的进程上下文进行“障眼法”操作。

	比如，Mount Namespace，用于让被隔离进程只看到当前 Namespace 里的挂载点信息；Network Namespace，用于让被隔离进程看到当前 Namespace 里的网络设备和配置。

	这，就是 Linux 容器最基本的实现原理了。

Cgroups
	Linux Cgroups 的设计还是比较易用的，简单粗暴地理解呢，它就是一个子系统目录加上一组资源限制文件的组合。而对于 Docker 等 Linux 容器项目来说，它们只需要在每个子系统下面，为每个容器创建一个控制组（即创建一个新目录），然后在启动容器进程之后，把这个进程的 PID 填写到对应控制组的 tasks 文件中就可以了。
	而至于在这些控制组下面的资源文件里填上什么值，就靠用户执行 docker run 时的参数指定了，比如这样一条命令：
	 $ docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash
	复制代码
	在启动这个容器后，我们可以通过查看 Cgroups 文件系统下，CPU 子系统中，“docker”这个控制组里的资源限制文件的内容来确认：
	 $ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_period_us
	 100000
	 $ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_quota_us
	 20000
	复制代码
	这就意味着这个 Docker 容器，只能使用到 20% 的 CPU 带宽。

Images
	见有道笔记（PDF）
		
		之前做的一个脚本把信息用json格式存到文本中。这样的好处是简便，易编辑，并且拥有强大的表达能力。
		不过从文本cat出来的是一堆很难看的字符串，没换行和缩进。这时候就在想，如果有个类似于IDE的格式化代码的工具来显示格式化的json数据，那就好了。
		这工具不用四处去找，python就提供了这样一个东西。
		从python2.6开始，多了个json.tool的东西。
		使用很简单。
		在命令行中，python -mjson.tool somejson.txt。
		我们用echo来测试下。
		>>echo '{"name": "lucy", "age": "18"}' | python -mjson.tool
		{
		"age": "18",
		"name": "lucy"
		}

image filesystem
	overlay on /var/lib/docker/overlay2/c4e1ec52589d9c947a4e1735d08520f453bd0c1d3416f7dd4c2a40caeb97196b/merged type overlay (rw,relatime,
	lowerdir=/var/lib/docker/overlay2/l/QA2NAQVLJBHHB6E2XNRYF2OTXH:/var/lib/docker/overlay2/l/P2FTPTUU2CNF2JFVCG3XW4AMA3:/var/lib/docker/overlay2/l/PPPOV7SQAFZBRGKQV6FEBJL4FM:/var/lib/docker/overlay2/l/MTO6CA5ETJBKLTLFXZZ3MHUSIK:/var/lib/docker/overlay2/l/ATUZP6HHVZBMMOP7OXTVXDTO5G:/var/lib/docker/overlay2/l/TJMAAUKGMGAHIHCNHABG3QW5HX:/var/lib/docker/overlay2/l/4MGFCJFRZLQQWIOJJYO3BAH65G:/var/lib/docker/overlay2/l/P22ZRVOEGYMUXH6IW6AG6R2MNR:/var/lib/docker/overlay2/l/2NEVYXKXM5EHFR4DQJMJHXYHQG:/var/lib/docker/overlay2/l/HF3GP7SGGVQFPYIW2TJOJVB6AC:/var/lib/docker/overlay2/l/XER74UQQ2JBHZ5QKXYZFJFP4DV:/var/lib/docker/overlay2/l/QTOSFGPZUSOS6CNLMWGDGHQQFJ:/var/lib/docker/overlay2/l/5IL2U3NPUXODK3N5FTAAXDJPZD:/var/lib/docker/overlay2/l/E6ON5G4VQ5NTP3NFNKXZHBSZ2C,
	
	upperdir=/var/lib/docker/overlay2/c4e1ec52589d9c947a4e1735d08520f453bd0c1d3416f7dd4c2a40caeb97196b/diff,
	
	workdir=/var/lib/docker/overlay2/c4e1ec52589d9c947a4e1735d08520f453bd0c1d3416f7dd4c2a40caeb97196b/work)


K8S

kubeadm config print init-defaults > kubeadm.conf
	[root@template 2019_k8s]# cat kubeadm.conf 
		apiVersion: kubeadm.k8s.io/v1beta1
		bootstrapTokens:
		- groups:
		  - system:bootstrappers:kubeadm:default-node-token
		  token: abcdef.0123456789abcdef
		  ttl: 24h0m0s
		  usages:
		  - signing
		  - authentication
		kind: InitConfiguration
		localAPIEndpoint:
		  advertiseAddress: 1.2.3.4
		  bindPort: 6443
		nodeRegistration:
		  criSocket: /var/run/dockershim.sock
		  name: template.me
		  taints:
		  - effect: NoSchedule
			key: node-role.kubernetes.io/master
		---
		apiServer:
		  timeoutForControlPlane: 4m0s
		apiVersion: kubeadm.k8s.io/v1beta1
		certificatesDir: /etc/kubernetes/pki
		clusterName: kubernetes
		controlPlaneEndpoint: ""
		controllerManager: {}
		dns:
		  type: CoreDNS
		etcd:
		  local:
			dataDir: /var/lib/etcd
		imageRepository: registry.aliyuncs.com/google_containers
		kind: ClusterConfiguration
		kubernetesVersion: v1.13.3
		networking:
		  dnsDomain: cluster.local
		  podSubnet: ""
		  serviceSubnet: 10.96.0.0/12
		scheduler: {}
		---
		apiVersion: kubeadm.k8s.io/v1alpha1
		kind: MasterConfiguration
		controllerManagerExtraArgs:
		  horizontal-pod-autoscaler-use-rest-clients: "true"
		  horizontal-pod-autoscaler-sync-period: "10s"
		  node-monitor-grace-period: "10s"
		apiServerExtraArgs:
		  runtime-config: "api/all=true"
kubeadm config images pull --config kubeadm.conf
[root@template 2019_k8s]# docker tag registry.aliyuncs.com/google_containers/kube-controller-manager:v1.13.3 k8s.gcr.io/kube-controller-manager:v1.13.3
[root@template 2019_k8s]# docker tag registry.aliyuncs.com/google_containers/kube-apiserver:v1.13.3 k8s.gcr.io/kube-apiserver:v1.13.3
[root@template 2019_k8s]# docker tag registry.aliyuncs.com/google_containers/kube-proxy:v1.13.3 k8s.gcr.io/kube-proxy:v1.13.3
[root@template 2019_k8s]# docker tag registry.aliyuncs.com/google_containers/kube-scheduler:v1.13.3 k8s.gcr.io/kube-scheduler:v1.13.3
[root@template 2019_k8s]# docker tag registry.aliyuncs.com/google_containers/coredns:1.2.6 k8s.gcr.io/coredns:1.2.6
[root@template 2019_k8s]# docker tag registry.aliyuncs.com/google_containers/etcd:3.2.24 k8s.gcr.io/etcd:3.2.24
[root@template 2019_k8s]# docker tag registry.aliyuncs.com/google_containers/pause:3.1 k8s.gcr.io/pause:3.1

kubeadm init --kubernetes-version=v1.13.3 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --apiserver-advertise-address=0.0.0.0 --ignore-preflight-errors=Swap
	[root@template 2019_k8s]# kubeadm init --config kubeadm.conf 
	W0224 00:16:48.353952   12785 strict.go:47] unknown configuration schema.GroupVersionKind{Group:"kubeadm.k8s.io", Version:"v1beta1", Kind:"MasterConfiguration"} for scheme definitions in "k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/scheme/scheme.go:31" and "k8s.io/kubernetes/cmd/kubeadm/app/componentconfigs/scheme.go:28"
	[config] WARNING: Ignored YAML document with GroupVersionKind kubeadm.k8s.io/v1beta1, Kind=MasterConfiguration
	[init] Using Kubernetes version: v1.13.3
	[preflight] Running pre-flight checks
			[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 17.12.1-ce. Latest validated version: 18.06
	[preflight] Pulling images required for setting up a Kubernetes cluster
	[preflight] This might take a minute or two, depending on the speed of your internet connection
	[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
	[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
	[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
	[kubelet-start] Activating the kubelet service
	[certs] Using certificateDir folder "/etc/kubernetes/pki"
	[certs] Generating "ca" certificate and key
	[certs] Generating "apiserver-kubelet-client" certificate and key
	[certs] Generating "apiserver" certificate and key
	[certs] apiserver serving cert is signed for DNS names [template.me kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.16.56.11]
	[certs] Generating "front-proxy-ca" certificate and key
	[certs] Generating "front-proxy-client" certificate and key
	[certs] Generating "etcd/ca" certificate and key
	[certs] Generating "etcd/peer" certificate and key
	[certs] etcd/peer serving cert is signed for DNS names [template.me localhost] and IPs [172.16.56.11 127.0.0.1 ::1]
	[certs] Generating "etcd/healthcheck-client" certificate and key
	[certs] Generating "apiserver-etcd-client" certificate and key
	[certs] Generating "etcd/server" certificate and key
	[certs] etcd/server serving cert is signed for DNS names [template.me localhost] and IPs [172.16.56.11 127.0.0.1 ::1]
	[certs] Generating "sa" key and public key
	[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
	[kubeconfig] Writing "admin.conf" kubeconfig file
	[kubeconfig] Writing "kubelet.conf" kubeconfig file
	[kubeconfig] Writing "controller-manager.conf" kubeconfig file
	[kubeconfig] Writing "scheduler.conf" kubeconfig file
	[control-plane] Using manifest folder "/etc/kubernetes/manifests"
	[control-plane] Creating static Pod manifest for "kube-apiserver"
	[control-plane] Creating static Pod manifest for "kube-controller-manager"
	[control-plane] Creating static Pod manifest for "kube-scheduler"
	[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
	[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
	[apiclient] All control plane components are healthy after 26.004421 seconds
	[uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
	[kubelet] Creating a ConfigMap "kubelet-config-1.13" in namespace kube-system with the configuration for the kubelets in the cluster
	[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "template.me" as an annotation
	[mark-control-plane] Marking the node template.me as control-plane by adding the label "node-role.kubernetes.io/master=''"
	[mark-control-plane] Marking the node template.me as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
	[bootstrap-token] Using token: abcdef.0123456789abcdef
	[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
	[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
	[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
	[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
	[bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace
	[addons] Applied essential addon: CoreDNS
	[addons] Applied essential addon: kube-proxy

	Your Kubernetes master has initialized successfully!

	To start using your cluster, you need to run the following as a regular user:

	  mkdir -p $HOME/.kube
	  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	  sudo chown $(id -u):$(id -g) $HOME/.kube/config

	You should now deploy a pod network to the cluster.
	Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
	  https://kubernetes.io/docs/concepts/cluster-administration/addons/

	You can now join any number of machines by running the following on each node
	as root:

	  kubeadm join 172.16.56.11:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:c98e6c2a6ba07023335991591c0e353ab5337e5496d11c45a3d2bf50db35b4d5  # 20190224
	  ##################################此处执行完之后，由于下载对应镜像，node1达到ready状态需要一段时间
	  [root@template 2019_k8s]# kubectl get nodes
			NAME          STATUS   ROLES    AGE   VERSION
			node1         Ready    <none>   10m   v1.13.3
			template.me   Ready    master   48m   v1.13.3
	  [root@node1 ~]# kubeadm join 172.16.56.11:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:c98e6c2a6ba07023335991591c0e353ab5337e5496d11c45a3d2bf50db35b4d5 
		[preflight] Running pre-flight checks
				[WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service'
				[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 17.12.1-ce. Latest validated version: 18.06
		[discovery] Trying to connect to API Server "172.16.56.11:6443"
		[discovery] Created cluster-info discovery client, requesting info from "https://172.16.56.11:6443"
		[discovery] Requesting info from "https://172.16.56.11:6443" again to validate TLS against the pinned public key
		[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server "172.16.56.11:6443"
		[discovery] Successfully established connection with API Server "172.16.56.11:6443"
		[join] Reading configuration from the cluster...
		[join] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
		[kubelet] Downloading configuration for the kubelet from the "kubelet-config-1.13" ConfigMap in the kube-system namespace
		[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
		[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
		[kubelet-start] Activating the kubelet service
		[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...
		[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "node1" as an annotation

		This node has joined the cluster:
		* Certificate signing request was sent to apiserver and a response was received.
		* The Kubelet was informed of the new secure connection details.

		Run 'kubectl get nodes' on the master to see this node join the cluster.
	
	kubectl https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
	
	Error response from daemon: Get https://registry.aliyuncs.com/v2/: x509: certificate has expired or is not yet valid
	可能是时间不对

	yaml example
	apiVersion: apps/v1
	kind: Deployment
	metadata:
	  name: nginx-deployment
	spec:
	  selector:
		matchLabels:
		  app: nginx
	  replicas: 2
	  template:
		metadata:
		  labels:
			app: nginx
		spec:
		  containers:
		  - name: nginx
			image: nginx:1.7.9
			ports:
			- containerPort: 80


	kubernates删除pod一直处于terminating状态
	使用命令

	kubectl delete pods --all --grace-period=0 –force

	强制删除
	
	使用自定义证书
	默认情况下，kubeadm 会生成集群运行所需的所有证书。您可以通过提供自己的证书来覆盖此行为。

	要做到这一点，您必须把它们放在 --cert-dir 参数或者配置文件中的 CertificatesDir 指定的目录。默认目录为 /etc/kubernetes/pki。

	如果存在一个给定的证书和密钥对，kubeadm 将会跳过生成步骤并且使用已存在的文件。例如，您可以拷贝一个已有的 CA 到 /etc/kubernetes/pki/ca.crt 和 /etc/kubernetes/pki/ca.key，kubeadm 将会使用这个 CA 来签署其余的证书。


	
实践--六间房

	iptables -P FORWARD ACCEPT
	
	[root@k8s-master-1 ~/k8s_liyang]# kubeadm reset
	[reset] WARNING: changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
	[reset] are you sure you want to proceed? [y/N]: y
	[preflight] running pre-flight checks
	[reset] Reading configuration from the cluster...
	[reset] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
	[reset] stopping the kubelet service
	[reset] unmounting mounted directories in "/var/lib/kubelet"
	[reset] deleting contents of stateful directories: [/var/lib/etcd /var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/run/kubernetes]
	[reset] deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
	[reset] deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]

	The reset process does not reset or clean up iptables rules or IPVS tables.
	If you wish to reset iptables, you must do so manually.
	For example: 
	iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X

	If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
	to reset your system's IPVS tables.

	[root@k8s-master-1 ~/k8s_liyang]# kubeadm init --config kubeadm.conf 
	[init] Using Kubernetes version: v1.13.4
	[preflight] Running pre-flight checks
			[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 17.12.1-ce. Latest validated version: 18.06
	[preflight] Pulling images required for setting up a Kubernetes cluster
	[preflight] This might take a minute or two, depending on the speed of your internet connection
	[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
	[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
	[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
	[kubelet-start] Activating the kubelet service
	[certs] Using certificateDir folder "/etc/kubernetes/pki"
	[certs] Generating "etcd/ca" certificate and key
	[certs] Generating "etcd/server" certificate and key
	[certs] etcd/server serving cert is signed for DNS names [k8s-master-1 localhost] and IPs [192.168.110.72 127.0.0.1 ::1]
	[certs] Generating "etcd/peer" certificate and key
	[certs] etcd/peer serving cert is signed for DNS names [k8s-master-1 localhost] and IPs [192.168.110.72 127.0.0.1 ::1]
	[certs] Generating "etcd/healthcheck-client" certificate and key
	[certs] Generating "apiserver-etcd-client" certificate and key
	[certs] Generating "ca" certificate and key
	[certs] Generating "apiserver-kubelet-client" certificate and key
	[certs] Generating "apiserver" certificate and key
	[certs] apiserver serving cert is signed for DNS names [k8s-master-1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.110.72]
	[certs] Generating "front-proxy-ca" certificate and key
	[certs] Generating "front-proxy-client" certificate and key
	[certs] Generating "sa" key and public key
	[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
	[kubeconfig] Writing "admin.conf" kubeconfig file
	[kubeconfig] Writing "kubelet.conf" kubeconfig file
	[kubeconfig] Writing "controller-manager.conf" kubeconfig file
	[kubeconfig] Writing "scheduler.conf" kubeconfig file
	[control-plane] Using manifest folder "/etc/kubernetes/manifests"
	[control-plane] Creating static Pod manifest for "kube-apiserver"
	[control-plane] Creating static Pod manifest for "kube-controller-manager"
	[control-plane] Creating static Pod manifest for "kube-scheduler"
	[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
	[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
	[apiclient] All control plane components are healthy after 19.001978 seconds
	[uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
	[kubelet] Creating a ConfigMap "kubelet-config-1.13" in namespace kube-system with the configuration for the kubelets in the cluster
	[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "k8s-master-1" as an annotation
	[mark-control-plane] Marking the node k8s-master-1 as control-plane by adding the label "node-role.kubernetes.io/master=''"
	[mark-control-plane] Marking the node k8s-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
	[bootstrap-token] Using token: abcdef.0123456789abcdef
	[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
	[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
	[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
	[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
	[bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace
	[addons] Applied essential addon: CoreDNS
	[addons] Applied essential addon: kube-proxy

	Your Kubernetes master has initialized successfully!

	To start using your cluster, you need to run the following as a regular user:

	  mkdir -p $HOME/.kube
	  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	  sudo chown $(id -u):$(id -g) $HOME/.kube/config

	You should now deploy a pod network to the cluster.
	Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
	  https://kubernetes.io/docs/concepts/cluster-administration/addons/

	You can now join any number of machines by running the following on each node
	as root:

	  kubeadm join 192.168.110.72:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:e8ce4259fdf49d04cccf683166ae48d9e35a73edebde668c730e36ed8d6eaf77


ETCD
	ETCDCTL_API=3 etcdctl --endpoints=[192.168.110.72:2379] --cacert=/etc/kubernetes/pki/etcd/ca.crt --key=/etc/kubernetes/pki/etcd/server.key --cert=/etc/kubernetes/pki/etcd/server.crt get / --prefix --keys-only

	ETCDCTL_API=3 etcdctl --endpoints=https://[127.0.0.1]:2379 --cacert=/etc/kubernetes/pki/etcd/ca.crt --cert=/etc/kubernetes/pki/etcd/healthcheck-client.crt --key=/etc/kubernetes/pki/etcd/healthcheck-client.key


问题
	
	kubectl exec -it redis-app-6594cf5f6f-7lsmr -- /bin/sh
	kubectl get pods -n kube-system -o wide
	kubectl -n kube-system logs kube-proxy-qwgrw
	nc -v -w 2 -z kubernetes.default.svc.cluster.local:443
	tcpdump -i cni0 -nn host 172.16.1.11 -vv

	kubectl -n dehong-test 




Calico

	yum install conntrack ipvsadm ipset jq sysstat curl iptables libseccomp bash-completion yum-utils device-mapper-persistent-data lvm2 net-tools conntrack-tools vim libtool-ltdl
	
	yum install kubeadm kubelet kubectl

	docker pull calico/node:v3.5.2
	docker pull calico/cni:v3.5.2
	docker pull calico/kube-controllers:v3.5.2
	
	docker pull registry.aliyuncs.com/google_containers/kube-apiserver:v1.14.0
	docker pull registry.aliyuncs.com/google_containers/kube-controller-manager:v1.14.0
	docker pull registry.aliyuncs.com/google_containers/kube-proxy:v1.14.0
	docker pull registry.aliyuncs.com/google_containers/kube-scheduler:v1.14.0
	docker pull registry.aliyuncs.com/google_containers/coredns:1.3.1
	docker pull registry.aliyuncs.com/google_containers/etcd:3.3.10
	docker pull registry.aliyuncs.com/google_containers/pause:3.1
	docker pull registry.cn-hangzhou.aliyuncs.com/kubernete/kubernetes-dashboard-amd64:v1.10.0
	
	docker tag registry.aliyuncs.com/google_containers/kube-apiserver:v1.14.0 k8s.gcr.io/kube-apiserver:v1.14.0
	docker tag registry.aliyuncs.com/google_containers/kube-controller-manager:v1.14.0 k8s.gcr.io/kube-controller-manager:v1.14.0
	docker tag registry.aliyuncs.com/google_containers/kube-proxy:v1.14.0 k8s.gcr.io/kube-proxy:v1.14.0
	docker tag registry.aliyuncs.com/google_containers/kube-scheduler:v1.14.0 k8s.gcr.io/kube-scheduler:v1.14.0
	docker tag registry.aliyuncs.com/google_containers/coredns:1.3.1 k8s.gcr.io/coredns:1.3.1
	docker tag registry.aliyuncs.com/google_containers/etcd:3.3.10 k8s.gcr.io/etcd:3.3.10
	docker tag registry.cn-hangzhou.aliyuncs.com/kubernete/kubernetes-dashboard-amd64:v1.10.0 k8s.gcr.io/kubernetes-dashboard-amd64:v1.10.0
	docker tag registry.aliyuncs.com/google_containers/pause:3.1 k8s.gcr.io/pause:3.1
	
	docker rmi registry.aliyuncs.com/google_containers/kube-apiserver:v1.14.0
	docker rmi registry.aliyuncs.com/google_containers/kube-controller-manager:v1.14.0
	docker rmi registry.aliyuncs.com/google_containers/kube-scheduler:v1.14.0
	docker rmi registry.aliyuncs.com/google_containers/kube-proxy:v1.14.0
	docker rmi registry.aliyuncs.com/google_containers/etcd:3.3.10
	docker rmi registry.aliyuncs.com/google_containers/coredns:1.3.1
	
	kubeadm config print init-defaults > kubeadm.conf
	kubeadm config images pull --config kubeadm.conf
	kubeadm config images pull --config kubeadm.conf
	
	kubeadm init --kubernetes-version=v1.14.0 --pod-network-cidr=172.22.0.0/16 --apiserver-advertise-address=192.168.122.89

	kubeadm join 192.168.122.89:6443 --token d9yqld.l743ws8hzhgc52tb     --discovery-token-ca-cert-hash sha256:7beae0395e6d48a5b4d6a986bb8043f934a62e4fe58e48933c3644e4b3364564
	
	W0327 17:45:45.351122    4240 strict.go:47] unknown configuration schema.GroupVersionKind{Group:"kubeadm.k8s.io", Version:"v1beta1", Kind:"MasterConfiguration"} for scheme definitions in "k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/scheme/scheme.go:31" and "k8s.io/kubernetes/cmd/kubeadm/app/componentconfigs/scheme.go:28"
	[config] WARNING: Ignored YAML document with GroupVersionKind kubeadm.k8s.io/v1beta1, Kind=MasterConfiguration
	[init] Using Kubernetes version: v1.13.3
	[preflight] Running pre-flight checks
			[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 17.12.1-ce. Latest validated version: 18.06
	[preflight] Pulling images required for setting up a Kubernetes cluster
	[preflight] This might take a minute or two, depending on the speed of your internet connection
	[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
	[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
	[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
	[kubelet-start] Activating the kubelet service
	[certs] Using certificateDir folder "/etc/kubernetes/pki"
	[certs] Generating "ca" certificate and key
	[certs] Generating "apiserver" certificate and key
	[certs] apiserver serving cert is signed for DNS names [template.me kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.16.56.11]
	[certs] Generating "apiserver-kubelet-client" certificate and key
	[certs] Generating "front-proxy-ca" certificate and key
	[certs] Generating "front-proxy-client" certificate and key
	[certs] Generating "etcd/ca" certificate and key
	[certs] Generating "etcd/peer" certificate and key
	[certs] etcd/peer serving cert is signed for DNS names [template.me localhost] and IPs [172.16.56.11 127.0.0.1 ::1]
	[certs] Generating "etcd/server" certificate and key
	[certs] etcd/server serving cert is signed for DNS names [template.me localhost] and IPs [172.16.56.11 127.0.0.1 ::1]
	[certs] Generating "etcd/healthcheck-client" certificate and key
	[certs] Generating "apiserver-etcd-client" certificate and key
	[certs] Generating "sa" key and public key
	[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
	[kubeconfig] Writing "admin.conf" kubeconfig file
	[kubeconfig] Writing "kubelet.conf" kubeconfig file
	[kubeconfig] Writing "controller-manager.conf" kubeconfig file
	[kubeconfig] Writing "scheduler.conf" kubeconfig file
	[control-plane] Using manifest folder "/etc/kubernetes/manifests"
	[control-plane] Creating static Pod manifest for "kube-apiserver"
	[control-plane] Creating static Pod manifest for "kube-controller-manager"
	[control-plane] Creating static Pod manifest for "kube-scheduler"
	[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
	[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
	[apiclient] All control plane components are healthy after 30.504099 seconds
	[uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
	[kubelet] Creating a ConfigMap "kubelet-config-1.13" in namespace kube-system with the configuration for the kubelets in the cluster
	[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "template.me" as an annotation
	[mark-control-plane] Marking the node template.me as control-plane by adding the label "node-role.kubernetes.io/master=''"
	[mark-control-plane] Marking the node template.me as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
	[bootstrap-token] Using token: abcdef.0123456789abcdef
	[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
	[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
	[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
	[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
	[bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace
	[addons] Applied essential addon: CoreDNS
	[addons] Applied essential addon: kube-proxy

	Your Kubernetes master has initialized successfully!

	To start using your cluster, you need to run the following as a regular user:

	  mkdir -p $HOME/.kube
	  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	  sudo chown $(id -u):$(id -g) $HOME/.kube/config

	You should now deploy a pod network to the cluster.
	Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
	  https://kubernetes.io/docs/concepts/cluster-administration/addons/

	You can now join any number of machines by running the following on each node
	as root:

	  kubeadm join 172.16.56.11:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:df5aaf07f470f716aaf8a2e505f896928c31c7edcd3a40039a8b55c5a139bd03


	# kubeadm join 172.16.56.11:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:df5aaf07f470f716aaf8a2e505f896928c31c7edcd3a40039a8b55c5a139bd03
	[preflight] Running pre-flight checks
			[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 17.12.1-ce. Latest validated version: 18.06
	[discovery] Trying to connect to API Server "172.16.56.11:6443"
	[discovery] Created cluster-info discovery client, requesting info from "https://172.16.56.11:6443"
	[discovery] Requesting info from "https://172.16.56.11:6443" again to validate TLS against the pinned public key
	[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server "172.16.56.11:6443"
	[discovery] Successfully established connection with API Server "172.16.56.11:6443"
	[join] Reading configuration from the cluster...
	[join] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
	[kubelet] Downloading configuration for the kubelet from the "kubelet-config-1.13" ConfigMap in the kube-system namespace
	[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
	[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
	[kubelet-start] Activating the kubelet service
	[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...
	[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "node1" as an annotation

	This node has joined the cluster:
	* Certificate signing request was sent to apiserver and a response was received.
	* The Kubelet was informed of the new secure connection details.

	Run 'kubectl get nodes' on the master to see this node join the cluster.

Calico Problem
	iptables -I INPUT -p tcp --dport 179 -j ACCEPT
	Warning  Unhealthy  8s (x15 over 2m28s)  kubelet, node1     Readiness probe failed: calico/node is not ready: BIRD is not ready: BGP not established with 192.168.122.84
	
	The calico/node container may report an “unready” status in Kubernetes with this message. In most cases, this means a particular peer is unreachable in the cluster. Users should ensure BGP connectivity between the two peers is allowed in their environment.

	This can also occur when inactive Node resources are configured when using node-to-node mesh. Resolve cases like this by decomissioning the stale nodes.

	Lastly this can occur when BGP connections to non-mesh peers go down. If this is a common occurance in your BGP topology, you can disable BIRD readiness checks. See node readiness for more information.
	
Calico INSTALL
	# cat calico/etcd.yaml 
	# This manifest installs the Calico etcd on the kubeadm master.  This uses a DaemonSet
	# to force it to run on the master even when the master isn't schedulable, and uses
	# nodeSelector to ensure it only runs on the master.
	apiVersion: extensions/v1beta1
	kind: DaemonSet
	metadata:
	  name: calico-etcd
	  namespace: kube-system
	  labels:
		k8s-app: calico-etcd
	spec:
	  template:
		metadata:
		  labels:
			k8s-app: calico-etcd
		  annotations:
			# Mark this pod as a critical add-on; when enabled, the critical add-on scheduler
			# reserves resources for critical add-on pods so that they can be rescheduled after
			# a failure.  This annotation works in tandem with the toleration below.
			scheduler.alpha.kubernetes.io/critical-pod: ''
		spec:
		  tolerations:
			# This taint is set by all kubelets running `--cloud-provider=external`
			# so we should tolerate it to schedule the Calico pods
			- key: node.cloudprovider.kubernetes.io/uninitialized
			  value: "true"
			  effect: NoSchedule
			# Allow this pod to run on the master.
			- key: node-role.kubernetes.io/master
			  effect: NoSchedule
			# Allow this pod to be rescheduled while the node is in "critical add-ons only" mode.
			# This, along with the annotation above marks this pod as a critical add-on.
			- key: CriticalAddonsOnly
			  operator: Exists
		  # Only run this pod on the master.
		  nodeSelector:
			node-role.kubernetes.io/master: ""
		  hostNetwork: true
		  containers:
			- name: calico-etcd
			  image: quay.io/coreos/etcd:v3.3.9
			  imagePullPolicy: IfNotPresent
			  env:
				- name: CALICO_ETCD_IP
				  valueFrom:
					fieldRef:
					  fieldPath: status.podIP
			  command:
			  - /usr/local/bin/etcd
			  args:
			  - --name=calico
			  - --data-dir=/var/etcd/calico-data
			  - --advertise-client-urls=http://$(CALICO_ETCD_IP):6666
			  - --listen-client-urls=http://0.0.0.0:6666
			  - --listen-peer-urls=http://0.0.0.0:6667
			  - --auto-compaction-retention=1
			  volumeMounts:
				- name: var-etcd
				  mountPath: /var/etcd
		  volumes:
			- name: var-etcd
			  hostPath:
				path: /var/etcd

	---

	# This manifest installs the Service which gets traffic to the Calico
	# etcd.
	apiVersion: v1
	kind: Service
	metadata:
	  labels:
		k8s-app: calico-etcd
	  name: calico-etcd
	  namespace: kube-system
	spec:
	  # Select the calico-etcd pod running on the master.
	  selector:
		k8s-app: calico-etcd
	  # This ClusterIP needs to be known in advance, since we cannot rely
	  # on DNS to get access to etcd.
	  clusterIP: 10.96.232.136
	  ports:
		- port: 6666

	-----------------------------------------------------------------------------------
	# cat calico/calico.yaml 
	# Calico Version v3.5.2
	# https://docs.projectcalico.org/v3.5/releases#v3.5.2
	# This manifest includes the following component versions:
	#   calico/node:v3.5.2
	#   calico/cni:v3.5.2
	#   calico/kube-controllers:v3.5.2

	# This ConfigMap is used to configure a self-hosted Calico installation.
	kind: ConfigMap
	apiVersion: v1
	metadata:
	  name: calico-config
	  namespace: kube-system
	data:
	  # Configure this with the location of your etcd cluster.
	  etcd_endpoints: "http://10.96.232.136:6666"

	  # If you're using TLS enabled etcd uncomment the following.
	  # You must also populate the Secret below with these files.
	  etcd_ca: ""   # "/calico-secrets/etcd-ca"
	  etcd_cert: "" # "/calico-secrets/etcd-cert"
	  etcd_key: ""  # "/calico-secrets/etcd-key"
	  # Typha is disabled.
	  typha_service_name: "none"
	  # Configure the Calico backend to use.
	  calico_backend: "bird"

	  # Configure the MTU to use
	  veth_mtu: "1440"

	  # The CNI network configuration to install on each node.  The special
	  # values in this config will be automatically populated.
	  cni_network_config: |-
		{
		  "name": "k8s-pod-network",
		  "cniVersion": "0.3.0",
		  "plugins": [
			{
			  "type": "calico",
			  "log_level": "info",
			  "etcd_endpoints": "__ETCD_ENDPOINTS__",
			  "etcd_key_file": "__ETCD_KEY_FILE__",
			  "etcd_cert_file": "__ETCD_CERT_FILE__",
			  "etcd_ca_cert_file": "__ETCD_CA_CERT_FILE__",
			  "mtu": __CNI_MTU__,
			  "ipam": {
				  "type": "calico-ipam"
			  },
			  "policy": {
				  "type": "k8s"
			  },
			  "kubernetes": {
				  "kubeconfig": "__KUBECONFIG_FILEPATH__"
			  }
			},
			{
			  "type": "portmap",
			  "snat": true,
			  "capabilities": {"portMappings": true}
			}
		  ]
		}

	---

	# The following contains k8s Secrets for use with a TLS enabled etcd cluster.
	# For information on populating Secrets, see http://kubernetes.io/docs/user-guide/secrets/
	apiVersion: v1
	kind: Secret
	type: Opaque
	metadata:
	  name: calico-etcd-secrets
	  namespace: kube-system
	data:
	  # Populate the following with etcd TLS configuration if desired, but leave blank if
	  # not using TLS for etcd.
	  # The keys below should be uncommented and the values populated with the base64
	  # encoded contents of each file that would be associated with the TLS data.
	  # Example command for encoding a file contents: cat <file> | base64 -w 0
	  # etcd-key: null
	  # etcd-cert: null
	  # etcd-ca: null

	---
	# This manifest installs the calico/node container, as well
	# as the Calico CNI plugins and network config on
	# each master and worker node in a Kubernetes cluster.
	kind: DaemonSet
	apiVersion: extensions/v1beta1
	metadata:
	  name: calico-node
	  namespace: kube-system
	  labels:
		k8s-app: calico-node
	spec:
	  selector:
		matchLabels:
		  k8s-app: calico-node
	  updateStrategy:
		type: RollingUpdate
		rollingUpdate:
		  maxUnavailable: 1
	  template:
		metadata:
		  labels:
			k8s-app: calico-node
		  annotations:
			# This, along with the CriticalAddonsOnly toleration below,
			# marks the pod as a critical add-on, ensuring it gets
			# priority scheduling and that its resources are reserved
			# if it ever gets evicted.
			scheduler.alpha.kubernetes.io/critical-pod: ''
		spec:
		  nodeSelector:
			beta.kubernetes.io/os: linux
		  hostNetwork: true
		  tolerations:
			# Make sure calico-node gets scheduled on all nodes.
			- effect: NoSchedule
			  operator: Exists
			# Mark the pod as a critical add-on for rescheduling.
			- key: CriticalAddonsOnly
			  operator: Exists
			- effect: NoExecute
			  operator: Exists
		  serviceAccountName: calico-node
		  # Minimize downtime during a rolling upgrade or deletion; tell Kubernetes to do a "force
		  # deletion": https://kubernetes.io/docs/concepts/workloads/pods/pod/#termination-of-pods.
		  terminationGracePeriodSeconds: 0
		  initContainers:
			# This container installs the Calico CNI binaries
			# and CNI network config file on each node.
			- name: install-cni
			  image: calico/cni:v3.5.2
			  imagePullPolicy: IfNotPresent
			  command: ["/install-cni.sh"]
			  env:
				# Name of the CNI config file to create.
				- name: CNI_CONF_NAME
				  value: "10-calico.conflist"
				# The CNI network config to install on each node.
				- name: CNI_NETWORK_CONFIG
				  valueFrom:
					configMapKeyRef:
					  name: calico-config
					  key: cni_network_config
				# The location of the Calico etcd cluster.
				- name: ETCD_ENDPOINTS
				  valueFrom:
					configMapKeyRef:
					  name: calico-config
					  key: etcd_endpoints
				# CNI MTU Config variable
				- name: CNI_MTU
				  valueFrom:
					configMapKeyRef:
					  name: calico-config
					  key: veth_mtu
				# Prevents the container from sleeping forever.
				- name: SLEEP
				  value: "false"
			  volumeMounts:
				- mountPath: /host/opt/cni/bin
				  name: cni-bin-dir
				- mountPath: /host/etc/cni/net.d
				  name: cni-net-dir
				- mountPath: /calico-secrets
				  name: etcd-certs
		  containers:
			# Runs calico/node container on each Kubernetes node.  This
			# container programs network policy and routes on each
			# host.
			- name: calico-node
			  image: calico/node:v3.5.2
			  imagePullPolicy: IfNotPresent
			  env:
				# The location of the Calico etcd cluster.
				- name: ETCD_ENDPOINTS
				  valueFrom:
					configMapKeyRef:
					  name: calico-config
					  key: etcd_endpoints
				# Location of the CA certificate for etcd.
				- name: ETCD_CA_CERT_FILE
				  valueFrom:
					configMapKeyRef:
					  name: calico-config
					  key: etcd_ca
				# Location of the client key for etcd.
				- name: ETCD_KEY_FILE
				  valueFrom:
					configMapKeyRef:
					  name: calico-config
					  key: etcd_key
				# Location of the client certificate for etcd.
				- name: ETCD_CERT_FILE
				  valueFrom:
					configMapKeyRef:
					  name: calico-config
					  key: etcd_cert
				# Set noderef for node controller.
				- name: CALICO_K8S_NODE_REF
				  valueFrom:
					fieldRef:
					  fieldPath: spec.nodeName
				# Choose the backend to use.
				- name: CALICO_NETWORKING_BACKEND
				  valueFrom:
					configMapKeyRef:
					  name: calico-config
					  key: calico_backend
				# Cluster type to identify the deployment type
				- name: CLUSTER_TYPE
				  value: "k8s,bgp"
				# Auto-detect the BGP IP address.
				- name: IP
				  value: "autodetect"
				# Enable IPIP
				- name: CALICO_IPV4POOL_IPIP
				  value: "Always"
				# Set MTU for tunnel device used if ipip is enabled
				- name: FELIX_IPINIPMTU
				  valueFrom:
					configMapKeyRef:
					  name: calico-config
					  key: veth_mtu
				# The default IPv4 pool to create on startup if none exists. Pod IPs will be
				# chosen from this range. Changing this value after installation will have
				# no effect. This should fall within `--cluster-cidr`.
				- name: CALICO_IPV4POOL_CIDR
				  value: "172.22.0.0/16"
				# Disable file logging so `kubectl logs` works.
				- name: CALICO_DISABLE_FILE_LOGGING
				  value: "true"
				# Set Felix endpoint to host default action to ACCEPT.
				- name: FELIX_DEFAULTENDPOINTTOHOSTACTION
				  value: "ACCEPT"
				# Disable IPv6 on Kubernetes.
				- name: FELIX_IPV6SUPPORT
				  value: "false"
				# Set Felix logging to "info"
				- name: FELIX_LOGSEVERITYSCREEN
				  value: "info"
				- name: FELIX_HEALTHENABLED
				  value: "true"
			  securityContext:
				privileged: true
			  resources:
				requests:
				  cpu: 250m
			  livenessProbe:
				httpGet:
				  path: /liveness
				  port: 9099
				  host: localhost
				periodSeconds: 10
				initialDelaySeconds: 10
				failureThreshold: 6
			  readinessProbe:
				exec:
				  command:
				  - /bin/calico-node
				  - -bird-ready
				  - -felix-ready
				periodSeconds: 10
			  volumeMounts:
				- mountPath: /lib/modules
				  name: lib-modules
				  readOnly: true
				- mountPath: /run/xtables.lock
				  name: xtables-lock
				  readOnly: false
				- mountPath: /var/run/calico
				  name: var-run-calico
				  readOnly: false
				- mountPath: /var/lib/calico
				  name: var-lib-calico
				  readOnly: false
				- mountPath: /calico-secrets
				  name: etcd-certs
		  volumes:
			# Used by calico/node.
			- name: lib-modules
			  hostPath:
				path: /lib/modules
			- name: var-run-calico
			  hostPath:
				path: /var/run/calico
			- name: var-lib-calico
			  hostPath:
				path: /var/lib/calico
			- name: xtables-lock
			  hostPath:
				path: /run/xtables.lock
				type: FileOrCreate
			# Used to install CNI.
			- name: cni-bin-dir
			  hostPath:
				path: /opt/cni/bin
			- name: cni-net-dir
			  hostPath:
				path: /etc/cni/net.d
			# Mount in the etcd TLS secrets with mode 400.
			# See https://kubernetes.io/docs/concepts/configuration/secret/
			- name: etcd-certs
			  secret:
				secretName: calico-etcd-secrets
				defaultMode: 0400
	---

	apiVersion: v1
	kind: ServiceAccount
	metadata:
	  name: calico-node
	  namespace: kube-system

	---
	# This manifest deploys the Calico Kubernetes controllers.
	# See https://github.com/projectcalico/kube-controllers
	apiVersion: extensions/v1beta1
	kind: Deployment
	metadata:
	  name: calico-kube-controllers
	  namespace: kube-system
	  labels:
		k8s-app: calico-kube-controllers
	  annotations:
		scheduler.alpha.kubernetes.io/critical-pod: ''
	spec:
	  # The controllers can only have a single active instance.
	  replicas: 1
	  strategy:
		type: Recreate
	  template:
		metadata:
		  name: calico-kube-controllers
		  namespace: kube-system
		  labels:
			k8s-app: calico-kube-controllers
		spec:
		  nodeSelector:
			beta.kubernetes.io/os: linux
		  # The controllers must run in the host network namespace so that
		  # it isn't governed by policy that would prevent it from working.
		  hostNetwork: true
		  tolerations:
			# Mark the pod as a critical add-on for rescheduling.
			- key: CriticalAddonsOnly
			  operator: Exists
			- key: node-role.kubernetes.io/master
			  effect: NoSchedule
		  serviceAccountName: calico-kube-controllers
		  containers:
			- name: calico-kube-controllers
			  image: calico/kube-controllers:v3.5.2
			  imagePullPolicy: IfNotPresent
			  env:
				# The location of the Calico etcd cluster.
				- name: ETCD_ENDPOINTS
				  valueFrom:
					configMapKeyRef:
					  name: calico-config
					  key: etcd_endpoints
				# Location of the CA certificate for etcd.
				- name: ETCD_CA_CERT_FILE
				  valueFrom:
					configMapKeyRef:
					  name: calico-config
					  key: etcd_ca
				# Location of the client key for etcd.
				- name: ETCD_KEY_FILE
				  valueFrom:
					configMapKeyRef:
					  name: calico-config
					  key: etcd_key
				# Location of the client certificate for etcd.
				- name: ETCD_CERT_FILE
				  valueFrom:
					configMapKeyRef:
					  name: calico-config
					  key: etcd_cert
				# Choose which controllers to run.
				- name: ENABLED_CONTROLLERS
				  value: policy,namespace,serviceaccount,workloadendpoint,node
			  volumeMounts:
				# Mount in the etcd TLS secrets.
				- mountPath: /calico-secrets
				  name: etcd-certs
			  readinessProbe:
				exec:
				  command:
				  - /usr/bin/check-status
				  - -r
		  volumes:
			# Mount in the etcd TLS secrets with mode 400.
			# See https://kubernetes.io/docs/concepts/configuration/secret/
			- name: etcd-certs
			  secret:
				secretName: calico-etcd-secrets
				defaultMode: 0400

	---

	apiVersion: v1
	kind: ServiceAccount
	metadata:
	  name: calico-kube-controllers
	  namespace: kube-system
	---

	# Include a clusterrole for the kube-controllers component,
	# and bind it to the calico-kube-controllers serviceaccount.
	kind: ClusterRole
	apiVersion: rbac.authorization.k8s.io/v1beta1
	metadata:
	  name: calico-kube-controllers
	rules:
	  # Pods are monitored for changing labels.
	  # The node controller monitors Kubernetes nodes.
	  # Namespace and serviceaccount labels are used for policy.
	  - apiGroups:
		  - ""
		resources:
		  - pods
		  - nodes
		  - namespaces
		  - serviceaccounts
		verbs:
		  - watch
		  - list
	  # Watch for changes to Kubernetes NetworkPolicies.
	  - apiGroups:
		  - networking.k8s.io
		resources:
		  - networkpolicies
		verbs:
		  - watch
		  - list
	---
	kind: ClusterRoleBinding
	apiVersion: rbac.authorization.k8s.io/v1beta1
	metadata:
	  name: calico-kube-controllers
	roleRef:
	  apiGroup: rbac.authorization.k8s.io
	  kind: ClusterRole
	  name: calico-kube-controllers
	subjects:
	- kind: ServiceAccount
	  name: calico-kube-controllers
	  namespace: kube-system
	---
	# Include a clusterrole for the calico-node DaemonSet,
	# and bind it to the calico-node serviceaccount.
	kind: ClusterRole
	apiVersion: rbac.authorization.k8s.io/v1beta1
	metadata:
	  name: calico-node
	rules:
	  # The CNI plugin needs to get pods, nodes, and namespaces.
	  - apiGroups: [""]
		resources:
		  - pods
		  - nodes
		  - namespaces
		verbs:
		  - get
	  - apiGroups: [""]
		resources:
		  - endpoints
		  - services
		verbs:
		  # Used to discover service IPs for advertisement.
		  - watch
		  - list
	  - apiGroups: [""]
		resources:
		  - nodes/status
		verbs:
		  # Needed for clearing NodeNetworkUnavailable flag.
		  - patch
	---
	apiVersion: rbac.authorization.k8s.io/v1beta1
	kind: ClusterRoleBinding
	metadata:
	  name: calico-node
	roleRef:
	  apiGroup: rbac.authorization.k8s.io
	  kind: ClusterRole
	  name: calico-node
	subjects:
	- kind: ServiceAccount
	  name: calico-node
	  namespace: kube-system
	---













