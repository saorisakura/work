Docker
	OS requirements
	To install Docker CE, you need a maintained version of CentOS 7. Archived versions aren’t supported or tested.

	The centos-extras repository must be enabled. This repository is enabled by default, but if you have disabled it, you need to re-enable it.

	The overlay2 storage driver is recommended.

	Uninstall old versions
	Older versions of Docker were called docker or docker-engine. If these are installed, uninstall them, along with associated dependencies.

	$ sudo yum remove docker \
					  docker-client \
					  docker-client-latest \
					  docker-common \
					  docker-latest \
					  docker-latest-logrotate \
					  docker-logrotate \
					  docker-engine
	It’s OK if yum reports that none of these packages are installed.

	The contents of /var/lib/docker/, including images, containers, volumes, and networks, are preserved. The Docker CE package is now called docker-ce.
	
	Install using the repository
	Before you install Docker CE for the first time on a new host machine, you need to set up the Docker repository. Afterward, you can install and update Docker from the repository.

	SET UP THE REPOSITORY
	Install required packages. yum-utils provides the yum-config-manager utility, and device-mapper-persistent-data and lvm2 are required by the devicemapper storage driver.

	$ sudo yum install -y yum-utils \
	  device-mapper-persistent-data \
	  lvm2
	Use the following command to set up the stable repository.

	$ sudo yum-config-manager \
		--add-repo \
		https://download.docker.com/linux/centos/docker-ce.repo
	
	INSTALL DOCKER CE
	yum install docker-ce docker-ce-cli containerd.io
	yum install docker-ce-<VERSION_STRING> docker-ce-cli-<VERSION_STRING> containerd.io
	systemctl start docker
	docker run hello-world
	
	UPGRADE DOCKER CE
	To upgrade Docker CE, follow the installation instructions, choosing the new version you want to install.
	
	Install from a package
	https://download.docker.com/linux/centos/7/x86_64/stable/Packages/ 
	
	yum install /path/to/package.rpm

Namespace
	$ docker run -it busybox /bin/sh
	/ #
	这个命令是 Docker 项目最重要的一个操作，即大名鼎鼎的 docker run。

	而 -it 参数告诉了 Docker 项目在启动容器后，需要给我们分配一个文本输入 / 输出环境，也就是 TTY，跟容器的标准输入相关联，这样我们就可以和这个 Docker 容器进行交互了。而 /bin/sh 就是我们要在 Docker 容器里运行的程序。

	所以，上面这条指令翻译成人类的语言就是：请帮我启动一个容器，在容器里执行 /bin/sh，并且给我分配一个命令行终端跟这个容器交互。

	这样，我的 Ubuntu 16.04 机器就变成了一个宿主机，而一个运行着 /bin/sh 的容器，就跑在了这个宿主机里面。

	上面的例子和原理，如果你已经玩过 Docker，一定不会感到陌生。此时，如果我们在容器里执行一下 ps 指令，就会发现一些更有趣的事情：

	/ # ps
	PID  USER   TIME COMMAND
	  1 root   0:00 /bin/sh
	  10 root   0:00 ps
	可以看到，我们在 Docker 里最开始执行的 /bin/sh，就是这个容器内部的第 1 号进程（PID=1），而这个容器里一共只有两个进程在运行。这就意味着，前面执行的 /bin/sh，以及我们刚刚执行的 ps，已经被 Docker 隔离在了一个跟宿主机完全不同的世界当中。

	这究竟是怎么做到呢？

	本来，每当我们在宿主机上运行了一个 /bin/sh 程序，操作系统都会给它分配一个进程编号，比如 PID=100。这个编号是进程的唯一标识，就像员工的工牌一样。所以 PID=100，可以粗略地理解为这个 /bin/sh 是我们公司里的第 100 号员工，而第 1 号员工就自然是比尔 · 盖茨这样统领全局的人物。

	而现在，我们要通过 Docker 把这个 /bin/sh 程序运行在一个容器当中。这时候，Docker 就会在这个第 100 号员工入职时给他施一个“障眼法”，让他永远看不到前面的其他 99 个员工，更看不到比尔 · 盖茨。这样，他就会错误地以为自己就是公司里的第 1 号员工。

	这种机制，其实就是对被隔离应用的进程空间做了手脚，使得这些进程只能看到重新计算过的进程编号，比如 PID=1。可实际上，他们在宿主机的操作系统里，还是原来的第 100 号进程。

	这种技术，就是 Linux 里面的 Namespace 机制。而 Namespace 的使用方式也非常有意思：它其实只是 Linux 创建新进程的一个可选参数。我们知道，在 Linux 系统中创建线程的系统调用是 clone()，比如：

	int pid = clone(main_function, stack_size, SIGCHLD, NULL); 
	这个系统调用就会为我们创建一个新的进程，并且返回它的进程号 pid。

	而当我们用 clone() 系统调用创建一个新进程时，就可以在参数中指定 CLONE_NEWPID 参数，比如：

	int pid = clone(main_function, stack_size, CLONE_NEWPID | SIGCHLD, NULL); 
	这时，新创建的这个进程将会“看到”一个全新的进程空间，在这个进程空间里，它的 PID 是 1。之所以说“看到”，是因为这只是一个“障眼法”，在宿主机真实的进程空间里，这个进程的 PID 还是真实的数值，比如 100。

	当然，我们还可以多次执行上面的 clone() 调用，这样就会创建多个 PID Namespace，而每个 Namespace 里的应用进程，都会认为自己是当前容器里的第 1 号进程，它们既看不到宿主机里真正的进程空间，也看不到其他 PID Namespace 里的具体情况。

	而除了我们刚刚用到的 PID Namespace，Linux 操作系统还提供了 Mount、UTS、IPC、Network 和 User 这些 Namespace，用来对各种不同的进程上下文进行“障眼法”操作。

	比如，Mount Namespace，用于让被隔离进程只看到当前 Namespace 里的挂载点信息；Network Namespace，用于让被隔离进程看到当前 Namespace 里的网络设备和配置。

	这，就是 Linux 容器最基本的实现原理了。

Cgroups
	Linux Cgroups 的设计还是比较易用的，简单粗暴地理解呢，它就是一个子系统目录加上一组资源限制文件的组合。而对于 Docker 等 Linux 容器项目来说，它们只需要在每个子系统下面，为每个容器创建一个控制组（即创建一个新目录），然后在启动容器进程之后，把这个进程的 PID 填写到对应控制组的 tasks 文件中就可以了。
	而至于在这些控制组下面的资源文件里填上什么值，就靠用户执行 docker run 时的参数指定了，比如这样一条命令：
	 $ docker run -it --cpu-period=100000 --cpu-quota=20000 ubuntu /bin/bash
	复制代码
	在启动这个容器后，我们可以通过查看 Cgroups 文件系统下，CPU 子系统中，“docker”这个控制组里的资源限制文件的内容来确认：
	 $ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_period_us
	 100000
	 $ cat /sys/fs/cgroup/cpu/docker/5d5c9f67d/cpu.cfs_quota_us
	 20000
	复制代码
	这就意味着这个 Docker 容器，只能使用到 20% 的 CPU 带宽。

Images
	见有道笔记（PDF）
		
		之前做的一个脚本把信息用json格式存到文本中。这样的好处是简便，易编辑，并且拥有强大的表达能力。
		不过从文本cat出来的是一堆很难看的字符串，没换行和缩进。这时候就在想，如果有个类似于IDE的格式化代码的工具来显示格式化的json数据，那就好了。
		这工具不用四处去找，python就提供了这样一个东西。
		从python2.6开始，多了个json.tool的东西。
		使用很简单。
		在命令行中，python -mjson.tool somejson.txt。
		我们用echo来测试下。
		>>echo '{"name": "lucy", "age": "18"}' | python -mjson.tool
		{
		"age": "18",
		"name": "lucy"
		}

image filesystem
	overlay on /var/lib/docker/overlay2/c4e1ec52589d9c947a4e1735d08520f453bd0c1d3416f7dd4c2a40caeb97196b/merged type overlay (rw,relatime,
	lowerdir=/var/lib/docker/overlay2/l/QA2NAQVLJBHHB6E2XNRYF2OTXH:/var/lib/docker/overlay2/l/P2FTPTUU2CNF2JFVCG3XW4AMA3:/var/lib/docker/overlay2/l/PPPOV7SQAFZBRGKQV6FEBJL4FM:/var/lib/docker/overlay2/l/MTO6CA5ETJBKLTLFXZZ3MHUSIK:/var/lib/docker/overlay2/l/ATUZP6HHVZBMMOP7OXTVXDTO5G:/var/lib/docker/overlay2/l/TJMAAUKGMGAHIHCNHABG3QW5HX:/var/lib/docker/overlay2/l/4MGFCJFRZLQQWIOJJYO3BAH65G:/var/lib/docker/overlay2/l/P22ZRVOEGYMUXH6IW6AG6R2MNR:/var/lib/docker/overlay2/l/2NEVYXKXM5EHFR4DQJMJHXYHQG:/var/lib/docker/overlay2/l/HF3GP7SGGVQFPYIW2TJOJVB6AC:/var/lib/docker/overlay2/l/XER74UQQ2JBHZ5QKXYZFJFP4DV:/var/lib/docker/overlay2/l/QTOSFGPZUSOS6CNLMWGDGHQQFJ:/var/lib/docker/overlay2/l/5IL2U3NPUXODK3N5FTAAXDJPZD:/var/lib/docker/overlay2/l/E6ON5G4VQ5NTP3NFNKXZHBSZ2C,
	
	upperdir=/var/lib/docker/overlay2/c4e1ec52589d9c947a4e1735d08520f453bd0c1d3416f7dd4c2a40caeb97196b/diff,
	
	workdir=/var/lib/docker/overlay2/c4e1ec52589d9c947a4e1735d08520f453bd0c1d3416f7dd4c2a40caeb97196b/work)


K8S

kubeadm config print init-defaults > kubeadm.conf
	[root@template 2019_k8s]# cat kubeadm.conf 
		apiVersion: kubeadm.k8s.io/v1beta1
		bootstrapTokens:
		- groups:
		  - system:bootstrappers:kubeadm:default-node-token
		  token: abcdef.0123456789abcdef
		  ttl: 24h0m0s
		  usages:
		  - signing
		  - authentication
		kind: InitConfiguration
		localAPIEndpoint:
		  advertiseAddress: 1.2.3.4
		  bindPort: 6443
		nodeRegistration:
		  criSocket: /var/run/dockershim.sock
		  name: template.me
		  taints:
		  - effect: NoSchedule
			key: node-role.kubernetes.io/master
		---
		apiServer:
		  timeoutForControlPlane: 4m0s
		apiVersion: kubeadm.k8s.io/v1beta1
		certificatesDir: /etc/kubernetes/pki
		clusterName: kubernetes
		controlPlaneEndpoint: ""
		controllerManager: {}
		dns:
		  type: CoreDNS
		etcd:
		  local:
			dataDir: /var/lib/etcd
		imageRepository: registry.aliyuncs.com/google_containers
		kind: ClusterConfiguration
		kubernetesVersion: v1.13.3
		networking:
		  dnsDomain: cluster.local
		  podSubnet: ""
		  serviceSubnet: 10.96.0.0/12
		scheduler: {}
		---
		apiVersion: kubeadm.k8s.io/v1alpha1
		kind: MasterConfiguration
		controllerManagerExtraArgs:
		  horizontal-pod-autoscaler-use-rest-clients: "true"
		  horizontal-pod-autoscaler-sync-period: "10s"
		  node-monitor-grace-period: "10s"
		apiServerExtraArgs:
		  runtime-config: "api/all=true"
kubeadm config images pull --config kubeadm.conf
[root@template 2019_k8s]# docker tag registry.aliyuncs.com/google_containers/kube-controller-manager:v1.13.3 k8s.gcr.io/kube-controller-manager:v1.13.3
[root@template 2019_k8s]# docker tag registry.aliyuncs.com/google_containers/kube-apiserver:v1.13.3 k8s.gcr.io/kube-apiserver:v1.13.3
[root@template 2019_k8s]# docker tag registry.aliyuncs.com/google_containers/kube-proxy:v1.13.3 k8s.gcr.io/kube-proxy:v1.13.3
[root@template 2019_k8s]# docker tag registry.aliyuncs.com/google_containers/kube-scheduler:v1.13.3 k8s.gcr.io/kube-scheduler:v1.13.3
[root@template 2019_k8s]# docker tag registry.aliyuncs.com/google_containers/coredns:1.2.6 k8s.gcr.io/coredns:1.2.6
[root@template 2019_k8s]# docker tag registry.aliyuncs.com/google_containers/etcd:3.2.24 k8s.gcr.io/etcd:3.2.24
[root@template 2019_k8s]# docker tag registry.aliyuncs.com/google_containers/pause:3.1 k8s.gcr.io/pause:3.1

kubeadm init --kubernetes-version=v1.13.3 --pod-network-cidr=10.244.0.0/16 --service-cidr=10.96.0.0/12 --apiserver-advertise-address=0.0.0.0 --ignore-preflight-errors=Swap
	[root@template 2019_k8s]# kubeadm init --config kubeadm.conf 
	W0224 00:16:48.353952   12785 strict.go:47] unknown configuration schema.GroupVersionKind{Group:"kubeadm.k8s.io", Version:"v1beta1", Kind:"MasterConfiguration"} for scheme definitions in "k8s.io/kubernetes/cmd/kubeadm/app/apis/kubeadm/scheme/scheme.go:31" and "k8s.io/kubernetes/cmd/kubeadm/app/componentconfigs/scheme.go:28"
	[config] WARNING: Ignored YAML document with GroupVersionKind kubeadm.k8s.io/v1beta1, Kind=MasterConfiguration
	[init] Using Kubernetes version: v1.13.3
	[preflight] Running pre-flight checks
			[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 17.12.1-ce. Latest validated version: 18.06
	[preflight] Pulling images required for setting up a Kubernetes cluster
	[preflight] This might take a minute or two, depending on the speed of your internet connection
	[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
	[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
	[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
	[kubelet-start] Activating the kubelet service
	[certs] Using certificateDir folder "/etc/kubernetes/pki"
	[certs] Generating "ca" certificate and key
	[certs] Generating "apiserver-kubelet-client" certificate and key
	[certs] Generating "apiserver" certificate and key
	[certs] apiserver serving cert is signed for DNS names [template.me kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 172.16.56.11]
	[certs] Generating "front-proxy-ca" certificate and key
	[certs] Generating "front-proxy-client" certificate and key
	[certs] Generating "etcd/ca" certificate and key
	[certs] Generating "etcd/peer" certificate and key
	[certs] etcd/peer serving cert is signed for DNS names [template.me localhost] and IPs [172.16.56.11 127.0.0.1 ::1]
	[certs] Generating "etcd/healthcheck-client" certificate and key
	[certs] Generating "apiserver-etcd-client" certificate and key
	[certs] Generating "etcd/server" certificate and key
	[certs] etcd/server serving cert is signed for DNS names [template.me localhost] and IPs [172.16.56.11 127.0.0.1 ::1]
	[certs] Generating "sa" key and public key
	[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
	[kubeconfig] Writing "admin.conf" kubeconfig file
	[kubeconfig] Writing "kubelet.conf" kubeconfig file
	[kubeconfig] Writing "controller-manager.conf" kubeconfig file
	[kubeconfig] Writing "scheduler.conf" kubeconfig file
	[control-plane] Using manifest folder "/etc/kubernetes/manifests"
	[control-plane] Creating static Pod manifest for "kube-apiserver"
	[control-plane] Creating static Pod manifest for "kube-controller-manager"
	[control-plane] Creating static Pod manifest for "kube-scheduler"
	[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
	[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
	[apiclient] All control plane components are healthy after 26.004421 seconds
	[uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
	[kubelet] Creating a ConfigMap "kubelet-config-1.13" in namespace kube-system with the configuration for the kubelets in the cluster
	[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "template.me" as an annotation
	[mark-control-plane] Marking the node template.me as control-plane by adding the label "node-role.kubernetes.io/master=''"
	[mark-control-plane] Marking the node template.me as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
	[bootstrap-token] Using token: abcdef.0123456789abcdef
	[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
	[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
	[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
	[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
	[bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace
	[addons] Applied essential addon: CoreDNS
	[addons] Applied essential addon: kube-proxy

	Your Kubernetes master has initialized successfully!

	To start using your cluster, you need to run the following as a regular user:

	  mkdir -p $HOME/.kube
	  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	  sudo chown $(id -u):$(id -g) $HOME/.kube/config

	You should now deploy a pod network to the cluster.
	Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
	  https://kubernetes.io/docs/concepts/cluster-administration/addons/

	You can now join any number of machines by running the following on each node
	as root:

	  kubeadm join 172.16.56.11:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:c98e6c2a6ba07023335991591c0e353ab5337e5496d11c45a3d2bf50db35b4d5  # 20190224
	  ##################################此处执行完之后，由于下载对应镜像，node1达到ready状态需要一段时间
	  [root@template 2019_k8s]# kubectl get nodes
			NAME          STATUS   ROLES    AGE   VERSION
			node1         Ready    <none>   10m   v1.13.3
			template.me   Ready    master   48m   v1.13.3
	  [root@node1 ~]# kubeadm join 172.16.56.11:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:c98e6c2a6ba07023335991591c0e353ab5337e5496d11c45a3d2bf50db35b4d5 
		[preflight] Running pre-flight checks
				[WARNING Service-Docker]: docker service is not enabled, please run 'systemctl enable docker.service'
				[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 17.12.1-ce. Latest validated version: 18.06
		[discovery] Trying to connect to API Server "172.16.56.11:6443"
		[discovery] Created cluster-info discovery client, requesting info from "https://172.16.56.11:6443"
		[discovery] Requesting info from "https://172.16.56.11:6443" again to validate TLS against the pinned public key
		[discovery] Cluster info signature and contents are valid and TLS certificate validates against pinned roots, will use API Server "172.16.56.11:6443"
		[discovery] Successfully established connection with API Server "172.16.56.11:6443"
		[join] Reading configuration from the cluster...
		[join] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
		[kubelet] Downloading configuration for the kubelet from the "kubelet-config-1.13" ConfigMap in the kube-system namespace
		[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
		[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
		[kubelet-start] Activating the kubelet service
		[tlsbootstrap] Waiting for the kubelet to perform the TLS Bootstrap...
		[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "node1" as an annotation

		This node has joined the cluster:
		* Certificate signing request was sent to apiserver and a response was received.
		* The Kubelet was informed of the new secure connection details.

		Run 'kubectl get nodes' on the master to see this node join the cluster.
	
	kubectl https://raw.githubusercontent.com/coreos/flannel/master/Documentation/kube-flannel.yml
	
	Error response from daemon: Get https://registry.aliyuncs.com/v2/: x509: certificate has expired or is not yet valid
	可能是时间不对

	yaml example
	apiVersion: apps/v1
	kind: Deployment
	metadata:
	  name: nginx-deployment
	spec:
	  selector:
		matchLabels:
		  app: nginx
	  replicas: 2
	  template:
		metadata:
		  labels:
			app: nginx
		spec:
		  containers:
		  - name: nginx
			image: nginx:1.7.9
			ports:
			- containerPort: 80


	kubernates删除pod一直处于terminating状态
	使用命令

	kubectl delete pods --all --grace-period=0 –force

	强制删除
	
	使用自定义证书
	默认情况下，kubeadm 会生成集群运行所需的所有证书。您可以通过提供自己的证书来覆盖此行为。

	要做到这一点，您必须把它们放在 --cert-dir 参数或者配置文件中的 CertificatesDir 指定的目录。默认目录为 /etc/kubernetes/pki。

	如果存在一个给定的证书和密钥对，kubeadm 将会跳过生成步骤并且使用已存在的文件。例如，您可以拷贝一个已有的 CA 到 /etc/kubernetes/pki/ca.crt 和 /etc/kubernetes/pki/ca.key，kubeadm 将会使用这个 CA 来签署其余的证书。


	
实践--六间房

	iptables -P FORWARD ACCEPT
	
	[root@k8s-master-1 ~/k8s_liyang]# kubeadm reset
	[reset] WARNING: changes made to this host by 'kubeadm init' or 'kubeadm join' will be reverted.
	[reset] are you sure you want to proceed? [y/N]: y
	[preflight] running pre-flight checks
	[reset] Reading configuration from the cluster...
	[reset] FYI: You can look at this config file with 'kubectl -n kube-system get cm kubeadm-config -oyaml'
	[reset] stopping the kubelet service
	[reset] unmounting mounted directories in "/var/lib/kubelet"
	[reset] deleting contents of stateful directories: [/var/lib/etcd /var/lib/kubelet /etc/cni/net.d /var/lib/dockershim /var/run/kubernetes]
	[reset] deleting contents of config directories: [/etc/kubernetes/manifests /etc/kubernetes/pki]
	[reset] deleting files: [/etc/kubernetes/admin.conf /etc/kubernetes/kubelet.conf /etc/kubernetes/bootstrap-kubelet.conf /etc/kubernetes/controller-manager.conf /etc/kubernetes/scheduler.conf]

	The reset process does not reset or clean up iptables rules or IPVS tables.
	If you wish to reset iptables, you must do so manually.
	For example: 
	iptables -F && iptables -t nat -F && iptables -t mangle -F && iptables -X

	If your cluster was setup to utilize IPVS, run ipvsadm --clear (or similar)
	to reset your system's IPVS tables.

	[root@k8s-master-1 ~/k8s_liyang]# kubeadm init --config kubeadm.conf 
	[init] Using Kubernetes version: v1.13.4
	[preflight] Running pre-flight checks
			[WARNING SystemVerification]: this Docker version is not on the list of validated versions: 17.12.1-ce. Latest validated version: 18.06
	[preflight] Pulling images required for setting up a Kubernetes cluster
	[preflight] This might take a minute or two, depending on the speed of your internet connection
	[preflight] You can also perform this action in beforehand using 'kubeadm config images pull'
	[kubelet-start] Writing kubelet environment file with flags to file "/var/lib/kubelet/kubeadm-flags.env"
	[kubelet-start] Writing kubelet configuration to file "/var/lib/kubelet/config.yaml"
	[kubelet-start] Activating the kubelet service
	[certs] Using certificateDir folder "/etc/kubernetes/pki"
	[certs] Generating "etcd/ca" certificate and key
	[certs] Generating "etcd/server" certificate and key
	[certs] etcd/server serving cert is signed for DNS names [k8s-master-1 localhost] and IPs [192.168.110.72 127.0.0.1 ::1]
	[certs] Generating "etcd/peer" certificate and key
	[certs] etcd/peer serving cert is signed for DNS names [k8s-master-1 localhost] and IPs [192.168.110.72 127.0.0.1 ::1]
	[certs] Generating "etcd/healthcheck-client" certificate and key
	[certs] Generating "apiserver-etcd-client" certificate and key
	[certs] Generating "ca" certificate and key
	[certs] Generating "apiserver-kubelet-client" certificate and key
	[certs] Generating "apiserver" certificate and key
	[certs] apiserver serving cert is signed for DNS names [k8s-master-1 kubernetes kubernetes.default kubernetes.default.svc kubernetes.default.svc.cluster.local] and IPs [10.96.0.1 192.168.110.72]
	[certs] Generating "front-proxy-ca" certificate and key
	[certs] Generating "front-proxy-client" certificate and key
	[certs] Generating "sa" key and public key
	[kubeconfig] Using kubeconfig folder "/etc/kubernetes"
	[kubeconfig] Writing "admin.conf" kubeconfig file
	[kubeconfig] Writing "kubelet.conf" kubeconfig file
	[kubeconfig] Writing "controller-manager.conf" kubeconfig file
	[kubeconfig] Writing "scheduler.conf" kubeconfig file
	[control-plane] Using manifest folder "/etc/kubernetes/manifests"
	[control-plane] Creating static Pod manifest for "kube-apiserver"
	[control-plane] Creating static Pod manifest for "kube-controller-manager"
	[control-plane] Creating static Pod manifest for "kube-scheduler"
	[etcd] Creating static Pod manifest for local etcd in "/etc/kubernetes/manifests"
	[wait-control-plane] Waiting for the kubelet to boot up the control plane as static Pods from directory "/etc/kubernetes/manifests". This can take up to 4m0s
	[apiclient] All control plane components are healthy after 19.001978 seconds
	[uploadconfig] storing the configuration used in ConfigMap "kubeadm-config" in the "kube-system" Namespace
	[kubelet] Creating a ConfigMap "kubelet-config-1.13" in namespace kube-system with the configuration for the kubelets in the cluster
	[patchnode] Uploading the CRI Socket information "/var/run/dockershim.sock" to the Node API object "k8s-master-1" as an annotation
	[mark-control-plane] Marking the node k8s-master-1 as control-plane by adding the label "node-role.kubernetes.io/master=''"
	[mark-control-plane] Marking the node k8s-master-1 as control-plane by adding the taints [node-role.kubernetes.io/master:NoSchedule]
	[bootstrap-token] Using token: abcdef.0123456789abcdef
	[bootstrap-token] Configuring bootstrap tokens, cluster-info ConfigMap, RBAC Roles
	[bootstraptoken] configured RBAC rules to allow Node Bootstrap tokens to post CSRs in order for nodes to get long term certificate credentials
	[bootstraptoken] configured RBAC rules to allow the csrapprover controller automatically approve CSRs from a Node Bootstrap Token
	[bootstraptoken] configured RBAC rules to allow certificate rotation for all node client certificates in the cluster
	[bootstraptoken] creating the "cluster-info" ConfigMap in the "kube-public" namespace
	[addons] Applied essential addon: CoreDNS
	[addons] Applied essential addon: kube-proxy

	Your Kubernetes master has initialized successfully!

	To start using your cluster, you need to run the following as a regular user:

	  mkdir -p $HOME/.kube
	  sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
	  sudo chown $(id -u):$(id -g) $HOME/.kube/config

	You should now deploy a pod network to the cluster.
	Run "kubectl apply -f [podnetwork].yaml" with one of the options listed at:
	  https://kubernetes.io/docs/concepts/cluster-administration/addons/

	You can now join any number of machines by running the following on each node
	as root:

	  kubeadm join 192.168.110.72:6443 --token abcdef.0123456789abcdef --discovery-token-ca-cert-hash sha256:e8ce4259fdf49d04cccf683166ae48d9e35a73edebde668c730e36ed8d6eaf77


ETCD
	ETCDCTL_API=3 etcdctl --endpoints=[192.168.110.72:2379] --cacert=/etc/kubernetes/pki/etcd/ca.crt --key=/etc/kubernetes/pki/etcd/server.key --cert=/etc/kubernetes/pki/etcd/server.crt get / --prefix --keys-only































