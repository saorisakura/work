kubeadm 故障排查
在安装的时候提示找不到 ebtables 或者 ethtool
如果您在运行 kubeadm init 的时候看到如下警告

[preflight] WARNING: ebtables not found in system path                          
[preflight] WARNING: ethtool not found in system path                           
那么可能是在您的 Linux 机器上缺少了 ebtables 和 ethtool。您可以通过以下命令进行安装：

# For ubuntu/debian users, try 
apt install ebtables ethtool

# For CentOS/Fedora users, try 
yum install ebtables ethtool
Pod 处于 RunContainerError、CrashLoopBackOff 或者 Error 状态
kubeadm init 结束之后是不应该有这样状态的 Pod 的。如果在 kubeadm init 结束之后 出现这样的 Pod，那么请在 kubeadm 库上创建一个 issue 描述此问题。在您部署网络解决方案之前，kube-dns 应该处于 Pending 状态。但是，如果在部署网络解决方案后 Pod 仍然处于 RunContainerError、CrashLoopBackOff 或者 Error 状态，并且 kube-dns 状态没有任何变化，那么很可能是您安装的 Pod 网络解决方案出了问题。您可能需要赋予它更多的 RBAC 权限或者使用一个更新的版本。请您在 Pod 网络提供商的 issue 跟踪处创建一个 issue 并对它进行归类。

kube-dns 阻塞在 Pending 状态
这是 期望 的状态并且也是设计的一部分。kubeadm 并不提供网络解决方案，所以管理员需要 安装 pod 网络解决方案。您需要在 kube-dns 完全部署之前安装一个 pod 网络。因此 kube-dns 在网络建立之前是处于 Pending 状态的。

HostPort 服务不可用
HostPort 和 HostIP 功能是否可用取决于您的 Pod 网络提供商。请联系该 Pod 网络解决方案的作者以确定 HostPort 和 HostIP 功能是否可用。

已经过验证的 HostPort CNI 提供商：

Calico
Canal
Flannel
如果需要更多信息，请参阅 CNI portmap documentation.

如果您的网络提供商不支持 portmap CNI 插件，那么您可能需要使用 NodePort feature of services 或者使用 HostNetwork=true。

通过 Service IP 无法访问 Pod
目前一些网络组件还没开启 hairpin 模式，该模式能够让 pod 在不知道 podIP 的情况下通过 Service IP 访问到它们自身。这是一个和 CNI 相关的 issue。请联系网络组件的提供商，及时获得它们是否支持 hairpin 模式的信息。

如果您正在使用 VirtualBox （直接使用或通过 Vagrant），您需要确定 hostname -i 返回的是一个可路由的 IP 地址 （即第二个网络接口的地址，而不是第一个）。默认情况下，VirtualBox 并不这样做，从而使得 kubelet 因为使用第一个非回环的网络接口地址而停止，该地址通常是可 NAT 的。解决方法：修改 /etc/hosts 文件，可参阅 Vagrantfileubuntu-vagrantfile 查看修改方法。

TLS 证书错误
以下错误表明证书可能不匹配

# kubectl get po
Unable to connect to the server: x509: certificate signed by unknown authority (possibly because of "crypto/rsa: verification error" while trying to verify candidate authority certificate "kubernetes")
验证 $HOME/.kube/config 文件是否包含有效证书，并在必要时重新生成证书。另外一个解决方法是覆盖 “admin” 用户的默认 kubeconfig 文件：

mv  $HOME/.kube $HOME/.kube.bak
mkdir -p $HOME/.kube
sudo cp -i /etc/kubernetes/admin.conf $HOME/.kube/config
sudo chown $(id -u):$(id -g) $HOME/.kube/config
在 CentOS 上建立 master 时出错
如果您正在使用 CentOS 并且在建立 master 节点时遇到问题，请确认您的 Docker cgroup 驱动匹配 kubelet 的配置：

docker info | grep -i cgroup
cat /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
如果 Docker cgroup 驱动和 kubelet 的配置不匹配，那么需要修改 kubelet 的配置使其能够匹配 Docker cgroup 驱动。您需要设置 --cgroup-driver 参数。如果参数已经被设置了，您可以通过以下方式更新：

sed -i "s/cgroup-driver=systemd/cgroup-driver=cgroupfs/g /etc/systemd/system/kubelet.service.d/10-kubeadm.conf
否则，您需要打开 systemd 文件然后添加该参数到现有的环境变量中。

然后重启 kubelet：

systemctl daemon-reload
systemctl restart kubelet
kubectl describe pod 或者 kubectl logs 命令能够帮助你分析错误。例如：

kubectl -n ${NAMESPACE} describe pod ${POD_NAME}

kubectl -n ${NAMESPACE} logs ${POD_NAME} -c ${CONTAINER_NAME}


kubernetes中网络报错问题

kubernetes中网络报错问题
系统环境
#系统版本
cat /etc/redhat-release
CentOS Linux release 7.4.1708 (Core)
#kubelet版本
kubelet --version
Kubernetes v1.10.0
#selinux状态
getenforce
Disabled
#系统防火墙状态
systemctl status firewalld
● firewalld.service - firewalld - dynamic firewall daemon
Loaded: loaded (/usr/lib/systemd/system/firewalld.service; disabled; vendor preset: enabled)
Active: inactive (dead)
 Docs: man:firewalld(1)
Pod 异常问题
#dns的Pod 一直处于 Waiting 或 ContainerCreating 状态
kubectl get po -n kube-system
NAME                                    READY     STATUS             RESTARTS   AGE
kube-dns-86f4d74b45-ffwjf        0/3       ContainerCreating   0          6m
#查看Pod详细情况
kubectl  describe pod kube-dns-86f4d74b45-ffwjf  -n kube-system
##我们看到如下信息：
Error syncing pod
Pod sandbox changed, it will be killed and re-created.
##可以发现，该 Pod 的 Sandbox 容器无法正常启动，具体原因需要查看 Kubelet 日志。
#查看Pod的log
journalctl -u kubelet
##看到如下报错内容：
RunPodSandbox from runtime service failed: rpc error: code = 2 desc = NetworkPlugin cni failed to set up pod "kube-dns-86f4d74b45-ffwjf" network: failed to set bridge addr: "cni0" already has an IP address different from 10.244.4.1/24
##说明
这里的一个Pod中启动了多个容器，所以，我们使用kubectl logs 命令查看日志很有局限性，关于kubectl logs的使用，请参考kubernetes中的Pod简述与实践和kubernetes中文文档。
处理步骤
#在master节点之外的节点进行操作
kubeadm reset
systemctl stop kubelet
systemctl stop docker
rm -rf /var/lib/cni/
rm -rf /var/lib/kubelet/*
rm -rf /etc/cni/
ifconfig cni0 down
ifconfig flannel.1 down
ifconfig docker0 down
ip link delete cni0
ip link delete flannel.1
##重启kubelet
systemctl restart kubelet
##重启docker
systemctl restart docker
#说明
##如果上面操作之后还是报相同的错误或是如下错误：
"CreatePodSandbox for pod \" kube-dns-86f4d74b45-ffwjf _default(78e796f5-e
b7c-11e7-b903-b827ebd42d30)\" failed: rpc error: code = Unknown desc = N
etworkPlugin cni failed to set up pod \" kube-dns-86f4d74b45-ffwjf _default\"
network: failed to allocate for range 0: no IP addresses available in range set:
10.244.1.1-10.244.1.254"
#执行如下操作步骤：
##在master主机上
kubeadm reset
systemctl stop kubelet
systemctl stop docker
rm -rf /var/lib/cni/
rm -rf /var/lib/kubelet/*
rm -rf /etc/cni/
ifconfig cni0 down
ifconfig flannel.1 down
ifconfig docker0 down
ip link delete cni0
ip link delete flannel.1
##重启kubelet
systemctl restart kubelet
##重启docker
systemctl restart docker
##初始化
kubeadm init --kubernetes-version=v1.10.1 --pod-network-cidr=10.244.0.0/16
--apiserver-advertise-address=10.0.0.39
##说明：
最后给出了将节点加入集群的命令：
kubeadm join 10.0.0.39:6443 --token 4g0p8w.w5p29ukwvitim2ti 
--discovery-token-ca-cert-hash sha256:21d0adbfcb409dca97e65564
1573b2ee51c
77a212f194e20a307cb459e5f77c8
这条命令一定保存好，因为后期没法重现的！！
##建立.kube
rm -rf /root/.kube/
mkdir -p /root/.kube/
cp -i /etc/kubernetes/admin.conf /root/.kube/config
chown root:root /root/.kube/config
#在node（非master）节点上
kubeadm reset
systemctl stop kubelet
systemctl stop docker
rm -rf /var/lib/cni/
rm -rf /var/lib/kubelet/*
rm -rf /etc/cni/
ifconfig cni0 down
ifconfig flannel.1 down
ifconfig docker0 down
ip link delete cni0
ip link delete flannel.1
##重启kubelet
systemctl restart kubelet
##重启docker
systemctl restart docker
## kubeadm join
kubeadm join 10.0.0.39:6443 --token 4g0p8w.w5p29ukwvitim2ti 
--discovery-token-ca-cert-hash sha256:21d0adbfcb409dca97e65564
1573b2ee51c
77a212f194e20a307cb459e5f77c8
总结
除了以上错误，其他可能的原因还有：
镜像拉取失败，比如：
（1）配置了错误的镜像
（2）Kubelet 无法访问镜像（国内环境访问 gcr.io 需要特殊处理
（3）私有镜像的密钥配置错误
（4）镜像太大，拉取超时（可以适当调整 kubelet 的 --image-pull-progress-deadline 和 --runtime-request-timeout 选项）
CNI 网络错误，一般需要检查 CNI 网络插件的配置，比如：
（1）无法配置 Pod 网络
（2）无法分配 IP 地址
容器无法启动，需要检查是否打包了正确的镜像或者是否配置了正确的容器参数等。