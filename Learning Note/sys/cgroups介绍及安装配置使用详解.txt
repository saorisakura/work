cgroups介绍及安装配置使用详解

	libcgroup libcgroup-devel libcgroup-tools
	Installing:
	 libcgroup-devel           x86_64           0.41-20.el7           base            37 k
	 libcgroup-tools           x86_64           0.41-20.el7           base           100 k
	Updating for dependencies:
	 libcgroup                 x86_64           0.41-20.el7           base            66 k


容器之 CGroup
 2015-09-21 Monday      linux , misc
在 CentOS 7 中，已经通过 systemd 替换了之前的 cgroup-tools 工具，为了防止两者冲突，建议只使用 systemd ，只有对于一些不支持的，例如 net_prio ，才使用 cgroup-tools 工具。

在此，简单介绍其使用。

简介
在系统设计时，经常会遇到类似的需求，就是希望能限制某个或者某些进程的分配资源。

由此，就有了容器的概念，在容器中，有分配好的特定比例的 CPU、IO、内存、网络等资源，这就是 controller group ，简称为 cgroup ，最初由 Google 工程师提出，后来被整合进 Linux 内核中。

cgroup 本身提供了将进程进行分组化管理的功能和接口的基础结构。

使用简介
在 CentOS 7 中需要通过 yum install libcgroup libcgroup-tools 安装额外的 cgroup 工具，对系统来说，默认会挂载到 /sys/fs/cgroup/ 目录下。

----- 查看系统已经存在cgroup子系统及其挂载点
# lssubsys -am
----- 或者通过mount查看cgroup类型的挂载点
# mount -t cgroup

----- 可以命令行挂载和卸载子系统，此时再次执行上述命令将看不到memory挂载点
# umount /sys/fs/cgroup/memory/
----- 挂载cgroup的memory子系统，其中最后的cgroup参数是在/proc/mounts中显示的名称
# mount -t cgroup -o memory cgroup /sys/fs/cgroup/memory/
# mount -t cgroup -o memory none /sys/fs/cgroup/memory/
另外，在 CentOS 中有 /etc/cgconfig.conf 配置文件，该文件中可用来配置开机自动启动时挂载的条目：

mount {
    net_prio = /sys/fs/cgroup/net_prio;
}
然后，通过 systemctl restart cgconfig.service 重启服务即可，然后可以通过如下方式使用。

使用步骤
简单介绍如何通过 libcgroup-tools 创建分组并设置资源配置参数。

1. 创建控制组群
可以通过如下方式创建以及删除群组，创建后会在 cpu 挂载目录下 /sys/fs/cgroup/cpu/ 目录下看到一个新的目录 test，这个就是新创建的 cpu 子控制组群。

# cgcreate -g cpu:/test
# cgdelete -g cpu:/test
2. 设置组群参数
cpu.shares 是控制 CPU 的一个属性，更多的属性可以到 /sys/fs/cgroup/cpu 目录下查看，默认值是 1024，值越大，能获得更多的 CPU 时间。

# cgset -r cpu.shares=512 test
3. 将进程添加到控制组群
可以直接将需要执行的命令添加到分组中。

----- 直接在cgroup中执行
# cgexec -g cpu:small some-program
----- 将现有的进程添加到cgroup中
# cgclassify -g subsystems:path_to_cgroups pidlist
例如，想把 sshd 添加到一个分组中，可以通过如下方式操作。

# cgclassify -g cpu:/test `pidof sshd`
# cat /sys/fs/cgroup/cpu/test/tasks
就会看到相应的进程在这个文件中。

CPU
在 CGroup 中，与 CPU 相关的子系统有 cpusets、cpuacct 和 cpu 。

cpuset 用于设置 CPU 的亲和性，可以限制该组中的进程只在(或不在)指定的 CPU 上运行，同时还能设置内存的亲和性，一般只会在一些高性能场景使用。
另外两个，cpuaccct 和 cpu 保存在相同的目录下，其中前者用来统计当前组的 CPU 统计信息。
这里简单介绍 cpu 子系统，包括怎么限制 cgroup 的 CPU 使用上限及与其它 cgroup 的相对值。

cpu.cfs_period_us & cpu.cfs_quota_us
其中 cfs_period_us 用来配置时间周期长度；cfs_quota_us 用来配置当前 cgroup 在设置的周期长度内所能使用的 CPU 时间数，两个文件配合起来设置 CPU 的使用上限。

两个文件单位是微秒，cfs_period_us 的取值范围为 [1ms, 1s]，默认 100ms ；cfs_quota_us 的取值大于 1ms 即可，如果 cfs_quota_us 的值为 -1(默认值)，表示不受 cpu 时间的限制。

下面是几个例子：

----- 1.限制只能使用1个CPU，每100ms能使用100ms的CPU时间
# echo 100000 > cpu.cfs_quota_us
# echo 100000 > cpu.cfs_period_us

------ 2.限制使用2个CPU核，每100ms能使用200ms的CPU时间，即使用两个内核
# echo 200000 > cpu.cfs_quota_us
# echo 100000 > cpu.cfs_period_us

------ 3.限制使用1个CPU的50%，每100ms能使用50ms的CPU时间，即使用一个CPU核心的50%
# echo 50000 > cpu.cfs_quota_us
# echo 100000 > cpu.cfs_period_us
cpu.shares
用于设置相对值，这里针对的是所有 CPU (多核)，默认是 1024，假如系统中有两个 A(1024) 和 B(512)，那么 A 将获得 1024/(1204+512)=66.67% 的 CPU 资源，而 B 将获得 33% 的 CPU 资源。

对于 shares 有两个特点：

如果A不忙，没有使用到66%的CPU时间，那么剩余的CPU时间将会被系统分配给B，即B的CPU使用率可以超过33%；
添加了一个新的C，它的shares值是1024，那么A和C的限额变为1024/(1204+512+1024)=40%，B的资源变成了20%；
也就是说，在空闲时 shares 基本上不起作用，只有在 CPU 忙的时候起作用。但是这里设置的值是需要与其它系统进行比较，而非设置了一个绝对值。

示例
演示一下如何控制CPU的使用率。

----- 创建并查看当前的分组
# cgcreate -g cpu:/small
# ls /sys/fs/cgroup/cpu/small

----- 查看当前值，默认是1024
# cat /sys/fs/cgroup/cpu/small/cpu.shares
# cgset -r cpu.shares=512 small

----- 执行需要运行的程序，或者将正在运行程序添加到分组
# cgexec -g cpu:small ./foobar
# cgclassify -g cpu:small <PID>

----- 设置只能使用1个cpu的20%的时间
# echo 50000 > cpu.cfs_period_us
# echo 10000 > cpu.cfs_quota_us

----- 将当前bash加入到该cgroup
# echo $$
5456
# echo 5456 > cgroup.procs

----- 启动一个bash内的死循环，正常情况下应该使用100%的cpu，消耗一个核
# while :; do echo test > /dev/null; done
注意，如果是在启动进程之后添加的，实际上 CPU 资源限制的速度会比较慢，不是立即就会限制死的，而且不是严格准确。如果起了多个子进程，那么各个进程之间的资源是共享的。

其它
可以通过如下命令查看进程属于哪个 cgroup 。

# ps -O cgroup
# cat /proc/PID/cgroup
内存
相比来说，内存控制要简单的多，只需要注意物理内存和 SWAP 即可。

----- 创建并查看当前的分组
# cgcreate -g memory:/small
# ls /sys/fs/cgroup/memory/small

----- 查看当前值，默认是一个很大很大的值，设置为1M
# cat /sys/fs/cgroup/memory/small/memory.limit_in_bytes
# cgset -r memory.limit_in_bytes=10485760 small

----- 如果开启了swap之后，会发现实际上内存只限制了RSS，设置时需要确保没有进程在使用
# cgset -r memory.memsw.limit_in_bytes=104857600 small

----- 启动测试程序
# cgexec -g cpu:small -g memory:small ./foobar
# cgexec -g cpu,memory:small ./foobar
OOM
当进程试图占用的内存超过了 cgroups 的限制时，会触发 out of memory 导致进程被强制 kill 掉。

----- 关闭默认的OOM
# echo 1 > memory.oom_control
# cgset -r memory.oom_control=1 small
注意，及时关闭了 OOM，对应的进程会处于 uninterruptible sleep 状态。

systemd
CentOS 7 中默认的资源隔离是通过 systemd 进行资源控制的，systemd 内部使用 cgroups 对其下的单元进行资源管理，包括 CPU、BlcokIO 以及 MEM，通过 cgroup 可以 。

systemd 的资源管理主要基于三个单元 service、scope 以及 slice：

service
通过 unit 配置文件定义，包括了一个或者多个进程，可以作为整体启停。
scope
任意进程可以通过 fork() 方式创建进程，常见的有 session、container 等。
slice
按照层级对 service、scope 组织的运行单元，不单独包含进程资源，进程包含在 service 和 scope 中。
常用的 slice 有 A) system.slice，系统服务进程可能是开机启动或者登陆后手动启动的服务，例如crond、mysqld、nginx等服务；B) user.slice 用户登陆后的管理，一般为 session；C) machine.slice 虚机或者容器的管理。

对于 cgroup 默认相关的参数会保存在 /sys/fs/cgroup/ 目录下，当前系统支持的 subsys 可以通过 cat /proc/cgroups 或者 lssubsys 查看。

常见命令
常用命令可以参考如下。

----- 查看slice、scope、service层级关系
# systemd-cgls

----- 当前资源使用情况
# systemd-cgtop

----- 启动一个服务
# systemd-run --unit=name --scope --slice=slice_name command
   unit   用于标示，如果不使用会自动生成一个，通过systemctl会输出；
   scope  默认使用service，该参数指定使用scope ；
   slice  将新启动的service或者scope添加到slice中，默认添加到system.slice，
          也可以添加到已有slice(systemctl -t slice)或者新建一个。
# systemd-run --unit=toptest --slice=test top -b
# systemctl stop toptest

----- 查看当前资源使用状态
$ systemctl show toptest
各服务配置保存在 /usr/lib/systemd/system/ 目录下，可以通过如下命令设置各个服务的参数。

----- 会自动保存到配置文件中做持久化
# systemctl set-property name parameter=value

----- 只临时修改不做持久化
# systemctl set-property --runtime name property=value

----- 设置CPU和内存使用率
# systemctl set-property httpd.service CPUShares=600 MemoryLimit=500M
另外，在 213 版本之后才开始支持 CPUQuota 参数，可直接修改 cpu.cfs_{quota,period}_us 参数，也就是在 /sys/fs/cgroup/cpu/ 目录下。

libcgroup
基于 libcgroup 实现一套容器的管理，详细的文档可以参考 libcg Documentation 中的相关介绍。

可以参考 Github - cgfy 中的实现，该程序是通过 libcgroup 实现，功能类似于 cgexec 。

另外，也可以参考 Github - clique，是直接利用 DBus 与 Systemd 进行通讯。

参考
关于 systemd 的资源控制，详细可以通过 man 5 systemd.resource-control 命令查看帮助，或者查看 systemd.resource-control 中文手册；详细的内容可以参考 Resource Management Guide 。

cgroups使用

# 下载一个可以模拟cpu高负载的工具,mathomatic是质数生成器，会加大处理器的负荷
 wget http://mathomatic.orgserve.de/mathomatic-16.0.5.tar.bz2 

tar xf mathomatic-16.0.5.tar.bz2
cd mathomatic-16.0.5/primes/
make && make install        #  编译安装

# centos7系统需要安装libcgroup-tools包，才有cgroup配置命令
yum install -y libcgroup-tools.x86_64

# 创建两个不同cpu资源分配的组
cgcreate -g cpu:/large      # 具体命令使用cgcreate -h
cgcreate -g cpu:/small

# cpu.shares是cpu控制的一个属性,更多的属性
# 可以到/sys/fs/cgroup/cpu目录下查看，默认值是1024，值越大，能获得更多的cpu时间
cgset -r cpu.shares=512 small 

# cgexec启动一个cgroup任务
matho-primes 0 999999999 > /dev/null &  # 生成一个从0到999999999的质数列表
cgexec -g cpu:small /usr/local/bin/matho-primes 0 999999999 > /dev/null & # 后台运行

# 使用top命令查看发现cpu被100%占用，因为是一个单独的进程，它使用尽可能多的cpu
# cgroups限制仅在两个或多个以上进程竞争cpu资源时起作用
 PID USER      PR  NI    VIRT    RES    SHR S  %CPU %MEM     TIME+ COMMAND     
 6726 root      20   0    9208   2540    488 R  99.9  0.0   0:34.47 matho-primes

cgexec -g cpu:large /usr/local/bin/matho-primes 0 9999999999 > /dev/null
cgexec -g cpu:small /usr/local/bin/matho-primes 0 9999999999 > /dev/null
cgexec -g cpu:small /usr/local/bin/matho-primes 0 9999999999 > /dev/null  
# 在small组中起两个matho-primes进程，然后top观察cpu时间
# small组两个matho-primes进程差不多共享cpu，large组的进程得到更多的cpu时间

下次打算将cgroups应用到我们的计算存储一体机上，用来限制glusterfs进程和nova-compute，避免存储服务和计算服务抢占cpu等资源。

cgroups限制gluster进程例子：http://www.andrewklau.com/controlling-glusterfsd-cpu-outbreaks-with-cgroups/
Controlling glusterfsd CPU outbreaks with cgroups

 
Some of you may that same feeling when adding a new brick to your gluster replicated volume which already has an excess of 1TB data already on there and suddenly your gluster server has shot up to 500% CPU usage. What's worse is when my hosts run along side oVirt so while gluster hogged all the CPU, my VMs started to crawl, even running simple commands like top would take 30+ seconds. Not a good feeling.

My first attempt I limited the NIC's bandwidth to 200Mbps rather than the 2x1Gbps aggregated link and this calmed glusterfsd down to a healthy 50%. A temporary fix which however meant clients accessing gluster storage would be bottlenecked by that shared limit.

So off to the mailing list - a great suggestion from James/purpleidea (https://ttboj.wordpress.com/code/puppet-gluster/) on using cgroups.

The concept is simple, we limit the total CPU glusterfsd sees so when it comes to doing the checksums for self heals, replication etc. They won't have the high priority which other services such as running VMs would have. This effectively slows down replication rate in return for lower CPU usage.

First make sure you have the package (RHEL/CentOS) libcgroup

Now you want to modify /etc/cgconfig.conf so you've got something like this (keep in mind comments MUST be at the start of the line or you may get parser errors):

mount {  
    cpuset  = /cgroup/cpuset;
    cpu = /cgroup/cpu;
    cpuacct = /cgroup/cpuacct;
    memory  = /cgroup/memory;
    devices = /cgroup/devices;
    freezer = /cgroup/freezer;
    net_cls = /cgroup/net_cls;
    blkio   = /cgroup/blkio;
}
group glusterfsd {  
        cpu {
# half of what libvirt assigns individual VMs (1024) - approximately 50% cpu share
                cpu.shares="512";
        }
        cpuacct {
                cpuacct.usage="0";
        }
        memory {
# limit the max ram to 4GB and 1GB swap
                memory.limit_in_bytes="4G";
                memory.memsw.limit_in_bytes="5G";
        }
}

group glusterd {  
        cpu {
# half of what libvirt assigns individual VMs (1024) - approximately 50% cpu share
                cpu.shares="512";
        }
        cpuacct {
                cpuacct.usage="0";
        }
        memory {
# limit the max ram to 4GB and 1GB swap
                memory.limit_in_bytes="4G";
                memory.memsw.limit_in_bytes="5G";
        }
}
Now apply the changes to the running service: 
service cgconfig restart

What this has done is defined two cgroup groups (glusterfsd and glusterd). I've gone and assigned the CPU share of the group to half of what libvirt assigns a VM along with some fixed memory limits just in case. The important one here is cpu.shares.

One last thing to do is modify the services so they start up in the cgroups. You can easily do this manually, but the recommended way (according to Red Hat docs) was to modify /etc/sysconfig/service

[root@hv01 ~]# cat /etc/sysconfig/glusterd 
# Change the glusterd service defaults here.
# See "glusterd --help" outpout for defaults and possible values.

#GLUSTERD_LOGFILE="/var/log/gluster/gluster.log"
#GLUSTERD_LOGLEVEL="NORMAL"

CGROUP_DAEMON="cpu:/glusterd cpuacct:/glusterd memory:/glusterd"  
[root@hv01 ~]# cat /etc/sysconfig/glusterfsd
# Change the glusterfsd service defaults here.
# See "glusterfsd --help" outpout for defaults and possible values.

#GLUSTERFSD_CONFIG="/etc/glusterfs/glusterfsd.vol"
#GLUSTERFSD_LOGFILE="/var/log/glusterfs/glusterfs.log"
#GLUSTERFSD_LOGLEVEL="NORMAL"

CGROUP_DAEMON="cpu:/glusterfsd cpuacct:/glusterfsd memory:/glusterfsd"  
Quick sum-up: We assign the gluster{d,fsd} service into the gluster{d,fsd} cgroup and define the resource groups we want to limit them to.

Now make sure cgconfig comes on at boot chkconfig cgconfig on

Ideally now, you should just send the host for a reboot to make sure everything's working the way it should.

When it comes back up, you can try the command cgsnapshot -s to see what your current rules are. -s will just ignore the undefined values.

Alternatively, before you define the "CGROUPDAEMON" in the sysconfig files shutdown the gluster services, then define "CGROUPDAEMON" and try start the gluster services again this should properly put them in the correct cgroups.

Note: I've only really tested this for a day - and so far I'm pretty impressed as the replication is no longer eating up my CPU and I haven't seen any performance drop in terms of read/write as all we've done is limited CPU and Memory. Bandwidth is untouched.

If you do your Google research you can also find the non-persistent method where you modify the files in /cgroup/ and create the groups there. I recommend doing that first to find the best config values for your systems.

For those interested, with my config values on a 2x Quad Core Server I cleaned out a brick and forced a re-replicate of the 1TB and glusterfsd happily chugged away at around 50% CPU and 200Mbps data transfer. I'm quite happy with that result, the obvious trade off cpu for replication rate is worth it for my scenario.

Please leave your suggestions/feedback and whether you found any possible ideal values for cgconfig.

HTH

Update (July 2014):

After a few months in using cgroups, I've removed the memory limits as gluster isn't that memory intensive. Similar to a comment as well, with a memory limit sometimes we hit an oom killer which is not great!

CPU performance DOES effect the read/write speed, so tweaking is required! The recent 3.5 version seems to be much better with CPU usage, making this appear to become obsolete. So kudos to the gluster devs!!