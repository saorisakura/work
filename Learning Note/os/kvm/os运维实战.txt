OpenStack运维实战


openstack运维实战系列(二十)之neutron创建网络并指定vlan号码

1. 背景说明
  neutron在openstack中负责instance的网络，如虚拟机内部网络，虚拟机外部网络等，和实体网络相类似，openstack中的网络也存在路由器router，交换机switch，网络network，子网subnet，端口port等概念，这些功能都有neutron来完成，neutron由有个不同的插件plugins组成，如二层插件neutron-openvswitch-agent，三层插件neutron-l3-agent,动态地址分配neutron-dhcp-agent,元数据服务neutron-metadata-agent等。
  此外，为了保障租户tenant之间的网络隔离，neutron支持多种不同的网络隔离技术，包括:Linux-bridge，Flat，vlan，gre和vxlan，对于大规模的环境来说，使用gre和vxlan比较多，linux-bridge和flat在小环境中使用，vlan则能够满足可扩展性且能够和现有的环境对接，我所在的环境中，使用vlan的网络模式，关于neutron各种网络模式的特点对别如下：

网络模式	功能说明	优点	缺点
linux-bridge	Linux网桥，和KVM网桥相类似	配置简单，易于实现，管理	可扩展性差
flat/flat+dhcp	和桥接相类似，扁平网络模式	配置简单，易于实现，管理	扁平，随着规模扩大，性能易出现瓶颈
vlan	通过vlan号隔离网络，划分广播域	和现有网络对接，易于理解，可扩展性强	vlan号只支持4096个，大规模易爆
gre	隧道封装技术，节点之间构建gre隧道	较容易实现流量隔离，没有限制	GRE包头添加网络开销
vxlan	和GRE技术相类似，隧道技术	没有范围限制，可扩展性强	需要增加IP包头开销


2.创建网络，并指定VLAN号  
  由于我所在环境中的opentstack云平台使用了vlan的网络模式，随着业务增长，外网IP会耗尽，此时，会向运营商申请外网IP，申请完之后，需要在openstack中扩容网络号，或者租户tenant自己需要内部的网络，也可以创建网络(tenant没法指定vlan号码，只有管理员才可以),具体操作如下：
1.创建网络，指定vlan范围和桥接的物理接口

a、创建网络,并指定网络模式和vlan号码，以及物理桥接网桥
[root@controller ~]# neutron net-create --provider:network_type=vlan --provider:physical_network=physnet0 --provider:segmentation_id=101 --shared public
Created a new network:
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| id                        | 0d30322d-8d87-43c3-b4e2-5a2969d3c42e |    #网络id号
| name                      | public                               |    #网络名字
| provider:network_type     | vlan                                 |    #网络类型为vlan
| provider:physical_network | physnet0                             |    #物理桥接网口
| provider:segmentation_id  | 101                                  |    #vlan的号码
| shared                    | True                                 |    #所有的tenant共享
| status                    | ACTIVE                               |    
| subnets                   |                                      |    #暂时没有加入子网，所以为空
| tenant_id                 | 842ab3268a2c47e6a4b0d8774de805ae     |    #网络所在的tenant
+---------------------------+--------------------------------------+
 
b、查看创建的网络列表
[root@controller ~]# neutron net-list
+--------------------------------------+---------------+-------------------------------------------------------+
| id                                   | name          | subnets                                               |
+--------------------------------------+---------------+-------------------------------------------------------+
| 0d30322d-8d87-43c3-b4e2-5a2969d3c42e | public        |                                                       |  #刚创建的网络
| 99c68a93-336a-4605-aa78-343d41ca1206 | vmTest        | 79cb82a1-eac1-4311-8e6d-badcabd22e44 192.168.100.0/24 |
+--------------------------------------+---------------+-------------------------------------------------------+
 
c、查看网络的详细信息
[root@controller ~]# neutron net-show 0d30322d-8d87-43c3-b4e2-5a2969d3c42e 
+---------------------------+--------------------------------------+
| Field                     | Value                                |
+---------------------------+--------------------------------------+
| admin_state_up            | True                                 |
| id                        | 0d30322d-8d87-43c3-b4e2-5a2969d3c42e |
| name                      | public                               |
| provider:network_type     | vlan                                 |
| provider:physical_network | physnet0                             |
| provider:segmentation_id  | 101                                  |
| router:external           | False                                |
| shared                    | True                                 |
| status                    | ACTIVE                               |
| subnets                   |                                      |
| tenant_id                 | 842ab3268a2c47e6a4b0d8774de805ae     |
+---------------------------+--------------------------------------+
2.创建子网，并将子网加入到网络内

a、创建子网subnet
[root@controller ~]# neutron subnet-create --name public_subnet  \
--ip-version 4 \
--gateway 192.168.101.1 \
--allocation-pool start=192.168.101.10,end=192.168.101.250 \
0d30322d-8d87-43c3-b4e2-5a2969d3c42e 192.168.101.0/24
Created a new subnet:
+------------------+-------------------------------------------------------+
| Field            | Value                                                 |
+------------------+-------------------------------------------------------+
| allocation_pools | {"start": "192.168.101.10", "end": "192.168.101.250"} |    #地址pools起始范围
| cidr             | 192.168.101.0/24                                      |    #网络地址块
| dns_nameservers  |                                                       |    
| enable_dhcp      | True                                                  |    #启用DHCP
| gateway_ip       | 192.168.101.1                                         |    #子网所在的网关
| host_routes      |                                                       |    
| id               | 3d715769-73ce-4984-81b2-ae1ffb284a74                  |    #subnet ID号
| ip_version       | 4                                                     |    #IP地址版本为ipv4
| name             | public_subnet                                         |    #subnet的名字
| network_id       | 0d30322d-8d87-43c3-b4e2-5a2969d3c42e                  |    #subnet所在的network
| tenant_id        | 842ab3268a2c47e6a4b0d8774de805ae                      |    #subnet所在tenant
+------------------+-------------------------------------------------------+
 
b、查看subnet的列表
[root@controller ~]# neutron subnet-list
+--------------------------------------+----------------+------------------+-------------------------------------------------------+
| id                                   | name           | cidr             | allocation_pools                                      |
+--------------------------------------+----------------+------------------+-------------------------------------------------------+
| 3d715769-73ce-4984-81b2-ae1ffb284a74 | public_subnet  | 192.168.101.0/24 | {"start": "192.168.101.10", "end": "192.168.101.250"} | #创建成功
| 79cb82a1-eac1-4311-8e6d-badcabd22e44 | ForTest        | 192.168.100.0/24 | {"start": "192.168.100.2", "end": "192.168.100.254"}  |
+--------------------------------------+----------------+------------------+-------------------------------------------------------+
 
c、查看subnet详情
[root@controller ~]# neutron subnet-show 3d715769-73ce-4984-81b2-ae1ffb284a74
+------------------+-------------------------------------------------------+
| Field            | Value                                                 |
+------------------+-------------------------------------------------------+
| allocation_pools | {"start": "192.168.101.10", "end": "192.168.101.250"} |
| cidr             | 192.168.101.0/24                                      |
| dns_nameservers  |                                                       |
| enable_dhcp      | True                                                  |
| gateway_ip       | 192.168.101.1                                         |
| host_routes      |                                                       |
| id               | 3d715769-73ce-4984-81b2-ae1ffb284a74                  |
| ip_version       | 4                                                     |
| name             | public_subnet                                         |
| network_id       | 0d30322d-8d87-43c3-b4e2-5a2969d3c42e                  |
| tenant_id        | 842ab3268a2c47e6a4b0d8774de805ae                      |
+------------------+-------------------------------------------------------+
3.交换机配置vlan与云平台联动
  云平台中配置了网络，使用vlan模式，此时，需要在交换机层面配置vlan信息和openstack云平台联动，需要配置的信息有:vlan地址，即网络的gateway，所有的compute接口所在的交换机接口，设置为trunk模式，并配置允许vlan101通过(关于具体配置，可以将需求和网络工程师说明)。

4.测试新创建的network

a、查看network和subnet的号码
[root@controller ~]# neutron net-list
+--------------------------------------+---------------+-------------------------------------------------------+
| id                                   | name          | subnets                                               |
+--------------------------------------+---------------+-------------------------------------------------------+
| 0d30322d-8d87-43c3-b4e2-5a2969d3c42e | public        | 3d715769-73ce-4984-81b2-ae1ffb284a74 192.168.101.0/24 |
+--------------------------------------+---------------+-------------------------------------------------------+
 
 
[root@controller ~]# neutron subnet-list
+--------------------------------------+----------------+------------------+-------------------------------------------------------+
| id                                   | name           | cidr             | allocation_pools                                      |
+--------------------------------------+----------------+------------------+-------------------------------------------------------+
| 3d715769-73ce-4984-81b2-ae1ffb284a74 | public_subnet  | 192.168.101.0/24 | {"start": "192.168.101.10", "end": "192.168.101.250"} |
+--------------------------------------+----------------+------------------+-------------------------------------------------------+
 
b、创建端口
[root@controller ~]# neutron port-create --name port_1 \
--fixed-ip subnet_id=3d715769-73ce-4984-81b2-ae1ffb284a74,ip_address=192.168.101.11 0d30322d-8d87-43c3-b4e2-5a2969d3c42e 
Created a new port:
+-----------------------+---------------------------------------------------------------------------------------+
| Field                 | Value                                                                                 |
+-----------------------+---------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                  |
| allowed_address_pairs |                                                                                       |
| binding:host_id       |                                                                                       |
| binding:profile       | {}                                                                                    |
| binding:vif_details   | {}                                                                                    |
| binding:vif_type      | unbound                                                                               |
| binding:vnic_type     | normal                                                                                |
| device_id             |                                                                                       |
| device_owner          |                                                                                       |
| fixed_ips             | {"subnet_id": "3d715769-73ce-4984-81b2-ae1ffb284a74", "ip_address": "192.168.101.11"} |    #端口的地址
| id                    | 9b860e7f-4327-4777-8f80-3a5a3c6672ad                                                  |    #端口id号
| mac_address           | fa:16:3e:af:73:66                                                                     |    #端口对应MAC
| name                  | port_1                                                                                |    #port名字
| network_id            | 0d30322d-8d87-43c3-b4e2-5a2969d3c42e                                                  |    #port所在network
| security_groups       | 663468d9-73b1-4b04-8d4c-dac1bf21a94d                                                  |    #所在安全组
| status                | DOWN                                                                                  |
| tenant_id             | 842ab3268a2c47e6a4b0d8774de805ae                                                      |    #所在subnet
+-----------------------+---------------------------------------------------------------------------------------+
 
c、查看port列表
[root@controller ~]# neutron port-list |grep 192.168.101
| 9b860e7f-4327-4777-8f80-3a5a3c6672ad | port_1 | fa:16:3e:af:73:66 | {"subnet_id": "3d715769-73ce-4984-81b2-ae1ffb284a74", "ip_address": "192.168.101.11"}  |
| fb5f8996-c025-4fdd-80dc-7d0d117a7cd6 |        | fa:16:3e:19:8f:f8 | {"subnet_id": "3d715769-73ce-4984-81b2-ae1ffb284a74", "ip_address": "192.168.101.10"}  |
 
d、查看port详情
[root@controller ~]# neutron port-show 9b860e7f-4327-4777-8f80-3a5a3c6672ad
+-----------------------+---------------------------------------------------------------------------------------+
| Field                 | Value                                                                                 |
+-----------------------+---------------------------------------------------------------------------------------+
| admin_state_up        | True                                                                                  |
| allowed_address_pairs |                                                                                       |
| binding:host_id       |                                                                                       |
| binding:profile       | {}                                                                                    |
| binding:vif_details   | {}                                                                                    |
| binding:vif_type      | unbound                                                                               |
| binding:vnic_type     | normal                                                                                |
| device_id             |                                                                                       |
| device_owner          |                                                                                       |
| extra_dhcp_opts       |                                                                                       |
| fixed_ips             | {"subnet_id": "3d715769-73ce-4984-81b2-ae1ffb284a74", "ip_address": "192.168.101.11"} |
| id                    | 9b860e7f-4327-4777-8f80-3a5a3c6672ad                                                  |
| mac_address           | fa:16:3e:af:73:66                                                                     |
| name                  | port_1                                                                                |
| network_id            | 0d30322d-8d87-43c3-b4e2-5a2969d3c42e                                                  |
| security_groups       | 663468d9-73b1-4b04-8d4c-dac1bf21a94d                                                  |
| status                | DOWN                                                                                  |
| tenant_id             | 842ab3268a2c47e6a4b0d8774de805ae                                                      |
+-----------------------+---------------------------------------------------------------------------------------+
4.将端口attach到intance中

a、执行attach操作
[root@controller ~]# nova list |grep happy
| 3f694eaf-aa87-456a-99ce-90dd9f4e45ee | happy_test              | SHUTOFF | -          | Shutdown    |                          | ChuangYiYuan_10_16_2_11 |
[root@controller ~]# nova interface-attach 3f694eaf-aa87-456a-99ce-90dd9f4e45ee  --port-id 9b860e7f-4327-4777-8f80-3a5a3c6672ad
 
b、attach成功，虚拟机和端口成功关联
[root@controller ~]# nova list |grep happy
| 3f694eaf-aa87-456a-99ce-90dd9f4e45ee | happy_test              | SHUTOFF | -          | Shutdown    | public=192.168.101.11    | ChuangYiYuan_10_16_2_11 |

5. 总结
 关于网络的扩容，可以通过租户自己创建，也可以通过管理员手动指定，对于租户自己创建来说，自动分配vlan号码，而管理员则可以针对业务需求，手动定制vlan号，由于openstack的应用场景和配置千奇百怪，不同的环境和场景都有所不同，读者根据自己所在的环境设置。

6. 附录

[root@controller ~]# neutron -h
usage: neutron [--version] [-v] [-q] [-h] [--os-auth-strategy <auth-strategy>]
               [--os-auth-url <auth-url>]
               [--os-tenant-name <auth-tenant-name>]
               [--os-tenant-id <auth-tenant-id>]
               [--os-username <auth-username>] [--os-password <auth-password>]
               [--os-region-name <auth-region-name>] [--os-token <token>]
               [--endpoint-type <endpoint-type>] [--os-url <url>]
               [--os-cacert <ca-certificate>] [--insecure]
Command-line interface to the Neutron APIs
optional arguments:
 --version             show program's version number and exit
  -v, --verbose, --debug
                        Increase verbosity of output and show tracebacks on
                        errors. Can be repeated.
  -q, --quiet           Suppress output except warnings and errors
  -h, --help            Show this help message and exit
  --os-auth-strategy <auth-strategy>
                        Authentication strategy (Env: OS_AUTH_STRATEGY,
                        default keystone). For now, any other value will
                        disable the authentication
  --os-auth-url <auth-url>
                        Authentication URL (Env: OS_AUTH_URL)
  --os-tenant-name <auth-tenant-name>
                        Authentication tenant name (Env: OS_TENANT_NAME)
  --os-tenant-id <auth-tenant-id>
                        Authentication tenant name (Env: OS_TENANT_ID)
  --os-username <auth-username>
                        Authentication username (Env: OS_USERNAME)
  --os-password <auth-password>
                        Authentication password (Env: OS_PASSWORD)
  --os-region-name <auth-region-name>
                        Authentication region name (Env: OS_REGION_NAME)
  --os-token <token>    Defaults to env[OS_TOKEN]
  --endpoint-type <endpoint-type>
                        Defaults to env[OS_ENDPOINT_TYPE] or publicURL.
  --os-url <url>        Defaults to env[OS_URL]
  --os-cacert <ca-certificate>
                        Specify a CA bundle file to use in verifying a TLS
                        (https) server certificate. Defaults to env[OS_CACERT]
  --insecure            Explicitly allow neutronclient to perform "insecure"
                        SSL (https) requests. The server's certificate will
                        not be verified against any certificate authorities.
                        This option should be used with caution.
Commands for API v2.0:
  agent-delete                   Delete a given agent.                 #agent管理
  agent-list                     List agents.
  agent-show                     Show information of a given agent.
  agent-update                   Update a given agent.
  cisco-credential-create        Creates a credential.
  cisco-credential-delete        Delete a  given credential.
  cisco-credential-list          List credentials that belong to a given tenant.
  cisco-credential-show          Show information of a given credential.
  cisco-network-profile-create   Creates a network profile.
  cisco-network-profile-delete   Delete a given network profile.
  cisco-network-profile-list     List network profiles that belong to a given tenant.
  cisco-network-profile-show     Show information of a given network profile.
  cisco-network-profile-update   Update network profile's information.
  cisco-policy-profile-list      List policy profiles that belong to a given tenant.
  cisco-policy-profile-show      Show information of a given policy profile.
  cisco-policy-profile-update    Update policy profile's information.
  dhcp-agent-list-hosting-net    List DHCP agents hosting a network.
  dhcp-agent-network-add         Add a network to a DHCP agent.
  dhcp-agent-network-remove      Remove a network from a DHCP agent.
  ext-list                       List all extensions.
  ext-show                       Show information of a given resource.
  firewall-create                Create a firewall.                      #防火墙管理
  firewall-delete                Delete a given firewall.
  firewall-list                  List firewalls that belong to a given tenant.
  firewall-policy-create         Create a firewall policy.
  firewall-policy-delete         Delete a given firewall policy.
  firewall-policy-insert-rule    Insert a rule into a given firewall policy.
  firewall-policy-list           List firewall policies that belong to a given tenant.
  firewall-policy-remove-rule    Remove a rule from a given firewall policy.
  firewall-policy-show           Show information of a given firewall policy.
  firewall-policy-update         Update a given firewall policy.
  firewall-rule-create           Create a firewall rule.
  firewall-rule-delete           Delete a given firewall rule.
  firewall-rule-list             List firewall rules that belong to a given tenant.
  firewall-rule-show             Show information of a given firewall rule.
  firewall-rule-update           Update a given firewall rule.
  firewall-show                  Show information of a given firewall.
  firewall-update                Update a given firewall.
  floatingip-associate           Create a mapping between a floating ip and a fixed ip. #浮动IP管理
  floatingip-create              Create a floating ip for a given tenant.
  floatingip-delete              Delete a given floating ip.
  floatingip-disassociate        Remove a mapping from a floating ip to a fixed ip.
  floatingip-list                List floating ips that belong to a given tenant.
  floatingip-show                Show information of a given floating ip.
  help                           print detailed help for another command
  ipsec-site-connection-create   Create an IPsecSiteConnection.               #VPN站点管理
  ipsec-site-connection-delete   Delete a given IPsecSiteConnection.
  ipsec-site-connection-list     List IPsecSiteConnections that belong to a given tenant.
  ipsec-site-connection-show     Show information of a given IPsecSiteConnection.
  ipsec-site-connection-update   Update a given IPsecSiteConnection.
  l3-agent-list-hosting-router   List L3 agents hosting a router.
  l3-agent-router-add            Add a router to a L3 agent.
  l3-agent-router-remove         Remove a router from a L3 agent.
  lb-agent-hosting-pool          Get loadbalancer agent hosting a pool.        #负载均衡相关管理
  lb-healthmonitor-associate     Create a mapping between a health monitor and a pool.
  lb-healthmonitor-create        Create a healthmonitor.
  lb-healthmonitor-delete        Delete a given healthmonitor.
  lb-healthmonitor-disassociate  Remove a mapping from a health monitor to a pool.
  lb-healthmonitor-list          List healthmonitors that belong to a given tenant.
  lb-healthmonitor-show          Show information of a given healthmonitor.
  lb-healthmonitor-update        Update a given healthmonitor.
  lb-member-create               Create a member.
  lb-member-delete               Delete a given member.
  lb-member-list                 List members that belong to a given tenant.
  lb-member-show                 Show information of a given member.
  lb-member-update               Update a given member.
  lb-pool-create                 Create a pool.
  lb-pool-delete                 Delete a given pool.
  lb-pool-list                   List pools that belong to a given tenant.
  lb-pool-list-on-agent          List the pools on a loadbalancer agent.
  lb-pool-show                   Show information of a given pool.
  lb-pool-stats                  Retrieve stats for a given pool.
  lb-pool-update                 Update a given pool.
  lb-vip-create                  Create a vip.
  lb-vip-delete                  Delete a given vip.
  lb-vip-list                    List vips that belong to a given tenant.
  lb-vip-show                    Show information of a given vip.
  lb-vip-update                  Update a given vip.
  meter-label-create             Create a metering label for a given tenant.
  meter-label-delete             Delete a given metering label.
  meter-label-list               List metering labels that belong to a given tenant.
  meter-label-rule-create        Create a metering label rule for a given label.
  meter-label-rule-delete        Delete a given metering label.
  meter-label-rule-list          List metering labels that belong to a given label.
  meter-label-rule-show          Show information of a given metering label rule.
  meter-label-show               Show information of a given metering label.
  net-create                     Create a network for a given tenant.    #网络相关管理
  net-delete                     Delete a given network.
  net-external-list              List external networks that belong to a given tenant.
  net-gateway-connect            Add an internal network interface to a router.
  net-gateway-create             Create a network gateway.
  net-gateway-delete             Delete a given network gateway.
  net-gateway-disconnect         Remove a network from a network gateway.
  net-gateway-list               List network gateways for a given tenant.
  net-gateway-show               Show information of a given network gateway.
  net-gateway-update             Update the name for a network gateway.
  net-list                       List networks that belong to a given tenant.
  net-list-on-dhcp-agent         List the networks on a DHCP agent.
  net-show                       Show information of a given network.
  net-update                     Update network's information.
  port-create                    Create a port for a given tenant.     #端口相关管理
  port-delete                    Delete a given port.
  port-list                      List ports that belong to a given tenant.
  port-show                      Show information of a given port.
  port-update                    Update port's information.
  queue-create                   Create a queue.
  queue-delete                   Delete a given queue.
  queue-list                     List queues that belong to a given tenant.
  queue-show                     Show information of a given queue.     #quota相关管理
  quota-delete                   Delete defined quotas of a given tenant.
  quota-list                     List quotas of all tenants who have non-default quota values.
  quota-show                     Show quotas of a given tenant
  quota-update                   Define tenant's quotas not to use defaults.
  router-create                  Create a router for a given tenant.      #路由器相关管理
  router-delete                  Delete a given router.
  router-gateway-clear           Remove an external network gateway from a router.
  router-gateway-set             Set the external network gateway for a router.
  router-interface-add           Add an internal network interface to a router.
  router-interface-delete        Remove an internal network interface from a router.
  router-list                    List routers that belong to a given tenant.
  router-list-on-l3-agent        List the routers on a L3 agent.
  router-port-list               List ports that belong to a given tenant, with specified router.
  router-show                    Show information of a given router.
  router-update                  Update router's information.
  security-group-create          Create a security group.             #安全组相关管理
  security-group-delete          Delete a given security group.
  security-group-list            List security groups that belong to a given tenant.
  security-group-rule-create     Create a security group rule.
  security-group-rule-delete     Delete a given security group rule.
  security-group-rule-list       List security group rules that belong to a given tenant.
  security-group-rule-show       Show information of a given security group rule.
  security-group-show            Show information of a given security group.
  security-group-update          Update a given security group.
  service-provider-list          List service providers.              #子网相关管理
  subnet-create                  Create a subnet for a given tenant.
  subnet-delete                  Delete a given subnet.
  subnet-list                    List subnets that belong to a given tenant.
  subnet-show                    Show information of a given subnet.
  subnet-update                  Update subnet's information.
  vpn-ikepolicy-create           Create an IKEPolicy.                #VPN相关的管理
  vpn-ikepolicy-delete           Delete a given IKE Policy.
  vpn-ikepolicy-list             List IKEPolicies that belong to a tenant.
  vpn-ikepolicy-show             Show information of a given IKEPolicy.
  vpn-ikepolicy-update           Update a given IKE Policy.
  vpn-ipsecpolicy-create         Create an ipsecpolicy.
  vpn-ipsecpolicy-delete         Delete a given ipsecpolicy.
  vpn-ipsecpolicy-list           List ipsecpolicies that belongs to a given tenant connection.
  vpn-ipsecpolicy-show           Show information of a given ipsecpolicy.
  vpn-ipsecpolicy-update         Update a given ipsec policy.
  vpn-service-create             Create a VPNService.
  vpn-service-delete             Delete a given VPNService.
  vpn-service-list               List VPNService configurations that belong to a given tenant.
  vpn-service-show               Show information of a given VPNService.
  vpn-service-update             Update a given VPNService.

openstack运维实战系列(十八)nova与ceph结合

1. 背景说明
   nova负责虚拟机的生命周期管理，包括创建，删除，重建，开机，关机，重启，快照等，作为openstack的核心，nova负责IaaS中计算重要的职责，其中nova的存储格外重要，默认情况下，nova将instance的数据存放在/var/lib/nova/instances/%UUID目录下，使用本地的存储空间。使用这种方式带来的好处是:简单，易实现，速度快，故障域在一个可控制的范围内。然而，缺点也非常明显：compute出故障，上面的虚拟机down机时间长，没法快速恢复，此外，一些特性如热迁移live-migration,虚拟机容灾nova evacuate等高级特性，将无法使用，对于后期的云平台建设，有明显的缺陷。
2.关于分布式存储
  使用openstack时，是否使用分布式文件系统，是一个非常值得思考和深思的问题，使用本地的优点非常明显，同时带来的缺点也显而易见，使用分布式存储，技术上的空白和技术上的难度，也是值得思考的一个问题。同时，分布式存储又改如何选择，应该选择简单的gluserfs还是呼声较高的ceph，这些都是构建云平台需要考虑的地方，同时openstack和ceph都存在一些坑，需要在技术上有足够的积累底蕴才能够控制起来。如果有需要通过openstack在生产环境下构建云平台，我建议初期使用本地的存储，充分利用KVM和本地文件系统的稳定，性能，后续有足够的技术可以逐步灰度至分布式存储glusterfs或者ceph中。建议使用ceph，如果基于技术层面的考虑，可以考虑使用glusterfs，glusterfs以其简单，易管理。
3.nova与ceph结合
1、ceph中创建存储池pool

[root@controller_10_1_2_230 ~]# ceph osd pool create vms 128    #创建一个pools，名字为vms，128个pg
pool 'vms' created
 
[root@controller_10_1_2_230 ~]# ceph osd lspools                #查看pools创建的情况
0 rbd,1 images,2 vms,
[root@controller_10_1_2_230 ~]# ceph osd pool stats
pool rbd id 0
  nothing is going on
 
pool images id 1
  nothing is going on
 
pool vms id 2
  nothing is going on
2、nova-compute节点安装和配置客户端
1
2
[root@compute1_10_1_2_232 ~]# yum install python-rbd ceph -y     #安装客户端包
[root@controller_10_1_2_230 ~]# scp  /etc/ceph/ceph.conf  root@10.1.2.232:/etc/ceph/ceph.conf #拷贝ceph配置文件
3、配置ceph认证，让nova用户能够访问vms池、images池

[root@controller_10_1_2_230 ~]# ceph auth get-or-create client.nova mon 'allow r' osd 'allow class-read object_prefix rbd_children, allow rwx pool=vms, allow rwx pool=images'                       
[client.nova]
        key = AQBLXqpWB8HsChAA6hGUBT5JNrFGD116uy+nmg==
       
#查看ceph的认证信息
[root@controller_10_1_2_230 ~]# ceph auth list
installed auth entries:
 
osd.0
        key: AQDsx6lWYGehDxAAGwcYP9jDvH2Zaa8JlGwj1Q==
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.1
        key: AQD1x6lWQCYBERAAjIKO1LVpj8FvVefDvNQZSA==
        caps: [mon] allow profile osd
        caps: [osd] allow *
client.admin
        key: AQCexqlWQL6OGBAA2v5LsYEB5VgLyq/K2huY3A==
        caps: [mds] allow
        caps: [mon] allow *
        caps: [osd] allow *
client.bootstrap-mds
        key: AQCexqlWUMNRMRAAZEp/UlhQuaixMcNy5d5pPw==
        caps: [mon] allow profile bootstrap-mds
client.bootstrap-osd
        key: AQCexqlWQFfpJBAAfPCx4sTLNztBESyFKys9LQ==
        caps: [mon] allow profile bootstrap-osd
client.bootstrap-rgw
        key: AQAR7alWok0SGhAAFtOo0PFsZuVzczMvJox1Wg==
        caps: [mon] allow profile bootstrap-rgw
client.glance
        key: AQAl76lWHMySHxAANTfXv3JQ70GCEBOZI5abcQ==
        caps: [mon] allow r
        caps: [osd] allow class-read object_prefix rbd_children, allow rwx pool=images
client.nova
        key: AQBLXqpWB8HsChAA6hGUBT5JNrFGD116uy+nmg==
        caps: [mon] allow r
        caps: [osd] allow class-read object_prefix rbd_children, allow rwx pool=vms, allow rwx pool=images    #添加了nova用户的认证信息
4、将ceph认证的key拷贝至计算节点

a、查看client.nova的key
[root@controller_10_1_2_230 ~]# ceph  auth get-or-create client.nova
[client.nova]
        key = AQBLXqpWB8HsChAA6hGUBT5JNrFGD116uy+nmg==
 
b、将key拷贝至远端
[root@controller_10_1_2_230 ~]# scp  ceph.client.nova.kering  root@10.1.2.232:/etc/ceph/
ceph.client.nova.kering  
 
c、生成nova临时的key
 
[root@controller_10_1_2_230 ~]# ceph auth get-key client.nova | ssh root@10.1.2.232 tee client.nova.key
5、计算节点的libvirt使用ceph的key

a、生成uuid号
[root@compute1_10_1_2_232 ~]# uuidgen 
0d154ad2-ec21-4200-952f-7551503da8a1
 
b、生成加密文件
vim secret.xml
 
<secret ephemeral='no' private='no'>
        <uuid>0d154ad2-ec21-4200-952f-7551503da8a1</uuid>
        <usage type='ceph'>
                <name>client.cinder secret</name>
        </usage>
</secret>
 
c、加载加密文件
[root@compute1_10_1_2_232 ~]# virsh secret-define --file secret.xml 
Secret 0d154ad2-ec21-4200-952f-7551503da8a1 created
 
d、配置libvirt加密，使用client.nova.key
[root@compute1_10_1_2_232 ~]# virsh secret-set-value --secret 0d154ad2-ec21-4200-952f-7551503da8a1 --base64 $(cat /root/client.nova.key) 
Secret value set
 
e、查看libvirt定义的key
[root@compute1_10_1_2_232 ~]# virsh secret-list
UUID                                 Usage
-----------------------------------------------------------
0d154ad2-ec21-4200-952f-7551503da8a1 Unused


openstack运维实战系列(十七)之glance与ceph结合

1. 需求说明
   glance作为openstack中image服务，支持多种适配器，支持将image存放到本地文件系统，http服务器，ceph分布式文件系统，glusterfs和sleepdog等开源的分布式文件系统上，本文，通过将讲述glance如何和ceph结合。
   目前glance采用的是本地filesystem的方式存储，存放在默认的路径/var/lib/glance/images下，当把本地的文件系统修改为分布式的文件系统ceph之后，原本在系统中镜像将无法使用，所以建议当前的镜像删除，部署好ceph之后，再统一上传至ceph中存储。
2.原理解析
   使用ceph的rbd接口，需要通过libvirt，所以需要在客户端机器上安装libvirt和qemu，关于ceph和openstack结合的结构如下，同时，在openstack中，需要用到存储的地方有三个:1. glance的镜像，默认的本地存储，路径在/var/lib/glance/images目录下，2. nova虚拟机存储，默认本地，路径位于/var/lib/nova/instances目录下，3. cinder存储，默认采用LVM的存储方式。
ditaa-e4a4957f90e4d8ebac2608e1544c34bf78
3. glance与ceph联动
1.创建资源池pool

1、ceph默认创建了一个pool:rbd
[root@controller_10_1_2_230 ~]# ceph osd lspools
0 rbd,
 
[root@controller_10_1_2_230 ~]# ceph osd pool stats
pool rbd id 0
  nothing is going on
 
2、创建一个pool，指定pg_num的大小为128
[root@controller_10_1_2_230 ~]# ceph osd pool create images 128
pool 'images' created
 
3、查看pool的pg_num和pgp_num大小
[root@controller_10_1_2_230 ~]# ceph osd pool get images pg_num
pg_num: 128
[root@controller_10_1_2_230 ~]# ceph osd pool get images pgp_num
pgp_num: 128
 
4、查看ceph中的pools
[root@controller_10_1_2_230 ~]# ceph osd lspools
0 rbd,1 images,            
[root@controller_10_1_2_230 ~]# ceph osd pool stats
pool rbd id 0
  nothing is going on
 
pool images id 1                #增加了一个pool，id号码是1
  nothing is going on
2.配置ceph客户端

1. glance作为ceph的客户端，即glance-api，需要有ceph的配置文件，从ceph的monitor节点复制一份配置文件过去即可，我所在环境中控制节点和ceph monitor为同一台机器，不需要操作
 
#如果controller节点和ceph的monitor节点是分开，则需要复制
[root@controller_10_1_2_230 ~]# scp /etc/ceph/ceph.conf root@controller_10_1_2_230:/etc/ceph/
ceph.conf  
 
2. 安装客户端rpm包
 
[root@controller_10_1_2_230 ~]# yum install python-rbd -y
3.配置ceph认证

1. 添加认证的key
[root@controller_10_1_2_230 ~]# ceph auth get-or-create client.glance mon 'allow r' osd 'class-read object_prefix rbd_children,allow rwx pool=images'   
[client.glance]
        key = AQB526lWOOIQBxAA0ZSk30DBShtti3fKkm4aeA==
 
2. 查看认证列表
[root@controller_10_1_2_230 ~]# ceph auth list
installed auth entries:
 
osd.0
        key: AQDsx6lWYGehDxAAGwcYP9jDvH2Zaa8JlGwj1Q==
        caps: [mon] allow profile osd
        caps: [osd] allow *
osd.1
        key: AQD1x6lWQCYBERAAjIKO1LVpj8FvVefDvNQZSA==
        caps: [mon] allow profile osd
        caps: [osd] allow *
client.admin
        key: AQCexqlWQL6OGBAA2v5LsYEB5VgLyq/K2huY3A==
        caps: [mds] allow
        caps: [mon] allow *
        caps: [osd] allow *
client.bootstrap-mds
        key: AQCexqlWUMNRMRAAZEp/UlhQuaixMcNy5d5pPw==
        caps: [mon] allow profile bootstrap-mds
client.bootstrap-osd
        key: AQCexqlWQFfpJBAAfPCx4sTLNztBESyFKys9LQ==
        caps: [mon] allow profile bootstrap-osd
client.glance                                             #glance连接ceph的认证信息
        key: AQB526lWOOIQBxAA0ZSk30DBShtti3fKkm4aeA==
        caps: [mon] allow r
        caps: [osd] class-read object_prefix rbd_children,allow rwx pool=images 
  
3. 将glance生成的key拷贝至
[root@controller_10_1_2_230 ~]# ceph auth get-or-create client.glance
[client.glance]
        key = AQB526lWOOIQBxAA0ZSk30DBShtti3fKkm4aeA==
 
#将key导出到客户端              
[root@controller_10_1_2_230 ~]# ceph auth get-or-create client.glance | tee /etc/ceph/ceph.client.glance.keyring
[client.glance]
        key = AQB526lWOOIQBxAA0ZSk30DBShtti3fKkm4aeA==
[root@controller_10_1_2_230 ~]# chown glance:glance /etc/ceph/ceph.client.glance.keyring 
[root@controller_10_1_2_230 ~]# ll /etc/ceph/ceph.client.glance.keyring 
-rw-r--r-- 1 glance glance 64 Jan 28 17:17 /etc/ceph/ceph.client.glance.keyring
4. 配置glance使用ceph做为后端存储

1、备份glance-api的配置文件，以便于恢复
[root@controller_10_1_2_230 ~]# cp /etc/glance/glance-api.conf /etc/glance/glance-api.conf.orig
 
2、修改glance配置文件，连接至ceph
[root@controller_10_1_2_230 ~]# vim /etc/glance/glance-api.conf
[DEFAULT]
notification_driver = messaging
rabbit_hosts = 10.1.2.230:5672
rabbit_retry_interval = 1
rabbit_retry_backoff = 2
rabbit_max_retries = 0
rabbit_ha_queues = True
rabbit_durable_queues = False
rabbit_userid = glance
rabbit_password = GLANCE_MQPASS
rabbit_virtual_host = /glance
 
default_store=rbd             #glance使用的后端存储
known_stores=glance.store.rbd.Store      #配置rbd的驱动
 
rbd_store_ceph_conf=/etc/ceph/ceph.conf    #ceph的配置文件，包含有monitor的地址，通过查找monitor，可以获取认证信息
rbd_store_user=glance                      #认证用户，即是刚创建的用户
rbd_store_pool=images                      #连接的存储池
rbd_store_chunk_size=8                     #设置chunk size，即切割的大小
 
3. 重启glance服务
[root@controller_10_1_2_230 ~]# /etc/init.d/openstack-glance-api restart                 
Stopping openstack-glance-api:                             [  OK  ]
Starting openstack-glance-api:                             [  OK  ]
[root@controller_10_1_2_230 ~]# /etc/init.d/openstack-glance-registry restart
Stopping openstack-glance-registry:                        [  OK  ]
Starting openstack-glance-registry:                        [  OK  ]
[root@controller_10_1_2_230 ~]# tail -2 /etc/glance/glance-api.conf
# location strategy defined by the 'location_strategy' config option.
#store_type_preference =
[root@controller_10_1_2_230 ~]# tail -2 /var/log/glance/registry.log
2016-01-28 18:40:25.231 21890 INFO glance.wsgi.server [-] Started child 21896
2016-01-28 18:40:25.232 21896 INFO glance.wsgi.server [-] (21896) wsgi starting up on http://0.0.0.0:9191/
5. 测试glance和ceph联动情况

[root@controller_10_1_2_230 ~]# glance --debug image-create --name glance_ceph_test --disk-format qcow2  --container-format bare  --file  cirros-0.3.3-x86_64-disk.img    
curl -i -X POST -H 'x-image-meta-container_format: bare' -H 'Transfer-Encoding: chunked' -H 'User-Agent: python-glanceclient' -H 'x-image-meta-size: 13200896' -H 'x-image-meta-is_public: False' -H 'X-Auth-Token: 062af9027a85487997d176c9f1e963f2' -H 'Content-Type: application/octet-stream' -H 'x-image-meta-disk_format: qcow2' -H 'x-image-meta-name: glance_ceph_test' -d '<open file u'cirros-0.3.3-x86_64-disk.img', mode 'rb' at 0x1ba24b0>' http://controller:9292/v1/images
 
HTTP/1.1 201 Created
content-length: 489
etag: 133eae9fb1c98f45894a4e60d8736619
location: http://controller:9292/v1/images/348a90e8-3631-4a66-a45d-590ec6413e7d
date: Thu, 28 Jan 2016 10:42:06 GMT
content-type: application/json
x-openstack-request-id: req-b993bc0b-447e-49b4-a8ce-bd7765199d5a
 
{"image": {"status": "active", "deleted": false, "container_format": "bare", "min_ram": 0, "updated_at": "2016-01-28T10:42:06", "owner": "ef4b83a909dc4689b663ff2c70022478", "min_disk": 0, "is_public": false, "deleted_at": null, "id": "348a90e8-3631-4a66-a45d-590ec6413e7d", "size": 13200896, "virtual_size": null, "name": "glance_ceph_test", "checksum": "133eae9fb1c98f45894a4e60d8736619", "created_at": "2016-01-28T10:42:04", "disk_format": "qcow2", "properties": {}, "protected": false}}
 
+------------------+--------------------------------------+
| Property         | Value                                |
+------------------+--------------------------------------+
| checksum         | 133eae9fb1c98f45894a4e60d8736619     |
| container_format | bare                                 |
| created_at       | 2016-01-28T10:42:04                  |
| deleted          | False                                |
| deleted_at       | None                                 |
| disk_format      | qcow2                                |
| id               | 348a90e8-3631-4a66-a45d-590ec6413e7d |
| is_public        | False                                |
| min_disk         | 0                                    |
| min_ram          | 0                                    |
| name             | glance_ceph_test                     |
| owner            | ef4b83a909dc4689b663ff2c70022478     |
| protected        | False                                |
| size             | 13200896                             |
| status           | active                               |
| updated_at       | 2016-01-28T10:42:06                  |
| virtual_size     | None                                 |
+------------------+--------------------------------------+
 
[root@controller_10_1_2_230 ~]# glance image-list
+--------------------------------------+---------------------+-------------+------------------+----------+--------+
| ID                                   | Name                | Disk Format | Container Format | Size     | Status |
+--------------------------------------+---------------------+-------------+------------------+----------+--------+
| 56e96957-1308-45c7-9c66-1afff680b217 | cirros-0.3.3-x86_64 | qcow2       | bare             | 13200896 | active |
| 348a90e8-3631-4a66-a45d-590ec6413e7d | glance_ceph_test    | qcow2       | bare             | 13200896 | active |    #上传成功
+--------------------------------------+---------------------+-------------+------------------+----------+--------+
6.查看ceph池的数据

[root@controller_10_1_2_230 ~]# rados -p images ls
rbd_directory
rbd_header.10d7caaf292
rbd_data.10dd1fd73446.0000000000000001
rbd_id.348a90e8-3631-4a66-a45d-590ec6413e7d
rbd_header.10dd1fd73446
rbd_data.10d7caaf292.0000000000000000
rbd_data.10dd1fd73446.0000000000000000
rbd_id.8a09b280-5916-44c6-9ce8-33bb57a09dad    @@@glance中的数据存储到了ceph文件系统中@@@
4. 总结
   将openstack的glance的数据存储到ceph中是一种非常好的解决方案，既能够保障image数据的安全性，同时glance和nova在同个存储池中，能够基于copy-on-write的方式快速创建虚拟机，能够在秒级为单位实现vm的创建。


openstack运维实战系列(十六)之ceph存储安装

1.前言
  企业上线openstack，必须要思考和解决三方面的难题:1.控制集群的高可用和负载均衡，保障集群没有单点故障，持续可用，2.网络的规划和neutron L3的高可用和负载均衡，3. 存储的高可用性和性能问题。存储openstack中的痛点与难点之一，在上线和运维中，值得考虑和规划的重要点，openstack支持各种存储，包括分布式的文件系统，常见的有:ceph,glusterfs和sheepdog，同时也支持商业的FC存储，如IBM,EMC,NetApp和huawei的专业存储设备，一方面能够满足企业的利旧和资源的统一管理。
2. ceph概述
  ceph作为近年来呼声最高的统一存储，在云环境下适应而生，ceph成就了openstack和cloudstack这类的开源的平台方案，同时openstack的快速发展，也吸引了越来越多的人参与到ceph的研究中来，在过去的2015年中，ceph在整个社区的活跃度越来越高，越来越多的企业，使用ceph做为openstack的glance，nova，cinder的存储。
  ceph是一种统一的分布式文件系统，能够支持三种常用的接口:1.对象存储接口，兼容于S3，用于存储结构化的数据，如图片，视频，音频等文件，其他对象存储有:S3,Swift,FastDFS等；2. 文件系统接口，通过cephfs来完成，能够实现类似于nfs的挂载文件系统，需要由MDS来完成，类似的文件系存储有:nfs,samba,glusterfs等；3. 块存储，通过rbd实现，专门用于存储云环境下块设备，如openstack的cinder卷存储，这也是目前ceph应用最广泛的地方。
wKioL1apsbuyr7GVAABnHliER58431.png
ceph的体系结构：
../_images/stack.png
3.ceph的安装
环境介绍，通过ceph-depoly的方式部署ceph，三台角色如下,按照需求配置好hostname并格式化磁盘为xfs并挂载
ditaa-5d5cab6fc315585e5057a743b5af7946fb
ip	主机名	角色	磁盘
10.1.2.230	controller_10_1_2_230	Monitor/admin-deploy	
10.1.2.231	network_10_1_2_231	OSD	/data1/osd0,格式化为xfs
10.1.2.232	compute1_10_1_2_232	OSD	/data1/osd1,格式化为xfs

2.yum源的准备，需要配置好Epel源和ceph源

vim /etc/yum.repos.d/ceph.repo
[ceph-noarch]
name=Ceph noarch packages
baseurl=http://download.ceph.com/rpm-{ceph-release}/{distro}/noarch
enabled=1
gpgcheck=1
type=rpm-md
gpgkey=https://download.ceph.com/keys/release.asc

[root@controller_10_1_2_230 ~]# yum repolist
Loaded plugins: fastestmirror, priorities
Repository epel is listed more than once in the configuration
Repository epel-debuginfo is listed more than once in the configuration
Repository epel-source is listed more than once in the configuration
Determining fastest mirrors
base                                                                                                                                                     | 3.7 kB     00:00     
base/primary_db                                                                                                                                          | 4.4 MB     00:01     
ceph                                                                                                                                                     | 2.9 kB     00:00     
ceph/primary_db                                                                                                                                          |  24 kB     00:00     
epel                                                                                                                                                     | 4.4 kB     00:00     
epel/primary_db                                                                                                                                          | 6.3 MB     00:00     
extras                                                                                                                                                   | 3.3 kB     00:00     
extras/primary_db                                                                                                                                        |  19 kB     00:00     
openstack-icehouse                                                                                                                                       | 2.9 kB     00:00     
openstack-icehouse/primary_db                                                                                                                            | 902 kB     00:00     
updates                                                                                                                                                  | 3.4 kB     00:00     
updates/primary_db                                                                                                                                       | 5.3 MB     00:00     
211 packages excluded due to repository priority protections
repo id                                                              repo name                                                                                         status
base                                                                 CentOS-6 - Base                                                                                    6,311+56
ceph                                                                 ceph                                                                                                   21+1
epel                                                                 Extra Packages for Enterprise Linux 6 - x86_64                                                    11,112+36
extras                                                               CentOS-6 - Extras                                                                                        15
openstack-icehouse                                                   OpenStack Icehouse Repository                                                                     1,353+309
updates                                                              CentOS-6 - Updates                                                                                1,397+118
repolist: 20,209

3.配置ntp，所有的机器均需要配置ntp服务器，统一同步到内网的ntp服务器，参考:

[root@compute1_10_1_2_232 ~]# vim /etc/ntp.conf
server 10.1.2.230
[root@compute1_10_1_2_232 ~]# /etc/init.d/ntpd restart
Shutting down ntpd:                                        [  OK  ]
Starting ntpd:                                             [  OK  ]
[root@compute1_10_1_2_232 ~]# chkconfig ntpd on
[root@compute1_10_1_2_232 ~]# chkconfig --list ntpd
ntpd            0:off   1:off   2:on    3:on    4:on    5:on    6:off

4.安装ceph，在所有的节点上都需要安装ceph，在admin节点，需要安装另外一个ceph-deploy，用于软件部署

[root@controller_10_1_2_230 ~]# yum install ceph ceph-deploy  #管理节点
[root@network_10_1_2_231 ~]# yum install ceph -y         #OSD节点
[root@compute1_10_1_2_232 ~]# yum install ceph -y          #OSD节点
5.生成SSH key秘钥，并拷贝至远端

a、生成key
[root@controller_10_1_2_230 ~]# ssh-keygen 
Generating public/private rsa key pair.
Enter file in which to save the key (/root/.ssh/id_rsa): 
Enter passphrase (empty for no passphrase): 
Enter same passphrase again: 
Your identification has been saved in /root/.ssh/id_rsa.
Your public key has been saved in /root/.ssh/id_rsa.pub.
The key fingerprint is:
75:b4:24:cd:24:a6:a4:a4:a5:c7:da:6f:e7:29:ce:0f root@controller_10_1_2_230
The key's randomart image is:
+--[ RSA 2048]----+
|      o . +++    |
|     * o o =o.   |
|    o + . . o    |
|     +   . .     |
|    . . S        |
|       .         |
|        E .      |
|       o.+ .     |
|       .oo+      |
+-----------------+
 
b、将key拷贝至远端
[root@controller_10_1_2_230 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root@controller_10_1_2_230
The authenticity of host '[controller_10_1_2_230]:32200 ([10.1.2.230]:32200)' can't be established.
RSA key fingerprint is 7c:6b:e6:d5:b9:cc:09:d2:b7:bb:db:a4:41:aa:5e:34.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '[controller_10_1_2_230]:32200,[10.1.2.230]:32200' (RSA) to the list of known hosts.
root@controller_10_1_2_230's password: 
Now try logging into the machine, with "ssh 'root@controller_10_1_2_230'", and check in:
  .ssh/authorized_keys
to make sure we haven't added extra keys that you weren't expecting.
[root@controller_10_1_2_230 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root@network_10_1_2_231 
The authenticity of host '[network_10_1_2_231]:32200 ([10.1.2.231]:32200)' can't be established.
RSA key fingerprint is de:27:84:74:d3:9c:cf:d5:d8:3e:c4:65:d5:9d:dc:9a.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '[network_10_1_2_231]:32200,[10.1.2.231]:32200' (RSA) to the list of known hosts.
root@network_10_1_2_231's password: 
Now try logging into the machine, with "ssh 'root@network_10_1_2_231'", and check in:
  .ssh/authorized_keys
to make sure we haven't added extra keys that you weren't expecting.
[root@controller_10_1_2_230 ~]# ssh-copy-id -i /root/.ssh/id_rsa.pub root@compute1_10_1_2_232 
The authenticity of host '[compute1_10_1_2_232]:32200 ([10.1.2.232]:32200)' can't be established.
RSA key fingerprint is d7:3a:1a:3d:b5:26:78:6a:39:5e:bd:5d:d4:96:29:0f.
Are you sure you want to continue connecting (yes/no)? yes
Warning: Permanently added '[compute1_10_1_2_232]:32200,[10.1.2.232]:32200' (RSA) to the list of known hosts.
root@compute1_10_1_2_232's password: 
Now try logging into the machine, with "ssh 'root@compute1_10_1_2_232'", and check in:
  .ssh/authorized_keys
to make sure we haven't added extra keys that you weren't expecting.
 
c、测试是否能无密码登陆
[root@controller_10_1_2_230 ~]# ssh root@compute1_10_1_2_232 'df -h'
Filesystem      Size  Used Avail Use% Mounted on
/dev/sda2       9.9G  3.3G  6.1G  35% /
tmpfs           3.9G  8.0K  3.9G   1% /dev/shm
/dev/sda1      1008M   82M  876M   9% /boot
/dev/sda4       913G   33M  913G   1% /data1
6.创建初始Monitor节点

a、创建配置目录，后续所有的操作都在该目录下执行
[root@controller_10_1_2_230 ~]# mkdir ceph-deploy
[root@controller_10_1_2_230 ~]# cd ceph-deploy/
 
b、新建一个集群
[root@controller_10_1_2_230 ceph-deploy]# ceph-deploy new  --cluster-network 10.1.2.0/24  --public-network 10.1.2.0/24 controller_10_1_2_230   
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.19): /usr/bin/ceph-deploy new --cluster-network 10.1.2.0/24 --public-network 10.1.2.0/24 controller_10_1_2_230
[ceph_deploy.new][DEBUG ] Creating new cluster named ceph
[ceph_deploy.new][INFO  ] making sure passwordless SSH succeeds
[controller_10_1_2_230][DEBUG ] connected to host: controller_10_1_2_230 
[controller_10_1_2_230][DEBUG ] detect platform information from remote host
[controller_10_1_2_230][DEBUG ] detect machine type
[controller_10_1_2_230][DEBUG ] find the location of an executable
[controller_10_1_2_230][INFO  ] Running command: /sbin/ip link show
[controller_10_1_2_230][INFO  ] Running command: /sbin/ip addr show
[controller_10_1_2_230][DEBUG ] IP addresses found: ['10.1.2.230']
[ceph_deploy.new][DEBUG ] Resolving host controller_10_1_2_230
[ceph_deploy.new][DEBUG ] Monitor controller_10_1_2_230 at 10.1.2.230
[ceph_deploy.new][DEBUG ] Monitor initial members are ['controller_10_1_2_230']
[ceph_deploy.new][DEBUG ] Monitor addrs are ['10.1.2.230']
[ceph_deploy.new][DEBUG ] Creating a random mon key...
[ceph_deploy.new][DEBUG ] Writing monitor keyring to ceph.mon.keyring...
[ceph_deploy.new][DEBUG ] Writing initial config to ceph.conf...
Error in sys.exitfunc:
 
c、修改副本的个数
[root@controller_10_1_2_230 ceph-deploy]# vim ceph.conf 
[global]
auth_service_required = cephx            
filestore_xattr_use_omap = true
auth_client_required = cephx
auth_cluster_required = cephx                    #开启ceph认证
mon_host = 10.1.2.230                            #monitor的ip地址
public_network = 10.1.2.0/24                     #ceph共有网络，即访问的网络
mon_initial_members = controller_10_1_2_230      #初始的monitor，建议是奇数，防止脑裂
cluster_network = 10.1.2.0/24                    #ceph集群内网
fsid = 07462638-a00f-476f-8257-3f4c9ec12d6e      #fsid号码
osd pool default size = 2                        #replicate的个数
 
d、monitor初始化
[root@controller_10_1_2_230 ceph-deploy]# ceph-deploy mon create-initial
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.19): /usr/bin/ceph-deploy mon create-initial
[ceph_deploy.mon][DEBUG ] Deploying mon, cluster ceph hosts controller_10_1_2_230
[ceph_deploy.mon][DEBUG ] detecting platform for host controller_10_1_2_230 ...
[controller_10_1_2_230][DEBUG ] connected to host: controller_10_1_2_230 
[controller_10_1_2_230][DEBUG ] detect platform information from remote host
[controller_10_1_2_230][DEBUG ] detect machine type
[ceph_deploy.mon][INFO  ] distro info: CentOS 6.5 Final
[controller_10_1_2_230][DEBUG ] determining if provided host has same hostname in remote
[controller_10_1_2_230][DEBUG ] get remote short hostname
[controller_10_1_2_230][DEBUG ] deploying mon to controller_10_1_2_230
[controller_10_1_2_230][DEBUG ] get remote short hostname
[controller_10_1_2_230][DEBUG ] remote hostname: controller_10_1_2_230
[controller_10_1_2_230][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[controller_10_1_2_230][DEBUG ] create the mon path if it does not exist
[controller_10_1_2_230][DEBUG ] checking for done path: /var/lib/ceph/mon/ceph-controller_10_1_2_230/done
[controller_10_1_2_230][DEBUG ] done path does not exist: /var/lib/ceph/mon/ceph-controller_10_1_2_230/done
[controller_10_1_2_230][INFO  ] creating keyring file: /var/lib/ceph/tmp/ceph-controller_10_1_2_230.mon.keyring
[controller_10_1_2_230][DEBUG ] create the monitor keyring file
[controller_10_1_2_230][INFO  ] Running command: ceph-mon --cluster ceph --mkfs -i controller_10_1_2_230 --keyring /var/lib/ceph/tmp/ceph-controller_10_1_2_230.mon.keyring
[controller_10_1_2_230][DEBUG ] ceph-mon: renaming mon.noname-a 10.1.2.230:6789/0 to mon.controller_10_1_2_230
[controller_10_1_2_230][DEBUG ] ceph-mon: set fsid to 07462638-a00f-476f-8257-3f4c9ec12d6e
[controller_10_1_2_230][DEBUG ] ceph-mon: created monfs at /var/lib/ceph/mon/ceph-controller_10_1_2_230 for mon.controller_10_1_2_230
[controller_10_1_2_230][INFO  ] unlinking keyring file /var/lib/ceph/tmp/ceph-controller_10_1_2_230.mon.keyring
[controller_10_1_2_230][DEBUG ] create a done file to avoid re-doing the mon deployment
[controller_10_1_2_230][DEBUG ] create the init path if it does not exist
[controller_10_1_2_230][DEBUG ] locating the `service` executable...
[controller_10_1_2_230][INFO  ] Running command: /sbin/service ceph -c /etc/ceph/ceph.conf start mon.controller_10_1_2_230
[controller_10_1_2_230][DEBUG ] === mon.controller_10_1_2_230 === 
[controller_10_1_2_230][DEBUG ] Starting Ceph mon.controller_10_1_2_230 on controller_10_1_2_230...
[controller_10_1_2_230][DEBUG ] Starting ceph-create-keys on controller_10_1_2_230...
[controller_10_1_2_230][INFO  ] Running command: chkconfig ceph on
[controller_10_1_2_230][INFO  ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.controller_10_1_2_230.asok mon_status
[controller_10_1_2_230][DEBUG ] ********************************************************************************
[controller_10_1_2_230][DEBUG ] status for monitor: mon.controller_10_1_2_230
[controller_10_1_2_230][DEBUG ] {
[controller_10_1_2_230][DEBUG ]   "election_epoch": 2, 
[controller_10_1_2_230][DEBUG ]   "extra_probe_peers": [], 
[controller_10_1_2_230][DEBUG ]   "monmap": {
[controller_10_1_2_230][DEBUG ]     "created": "0.000000", 
[controller_10_1_2_230][DEBUG ]     "epoch": 1, 
[controller_10_1_2_230][DEBUG ]     "fsid": "07462638-a00f-476f-8257-3f4c9ec12d6e", 
[controller_10_1_2_230][DEBUG ]     "modified": "0.000000", 
[controller_10_1_2_230][DEBUG ]     "mons": [
[controller_10_1_2_230][DEBUG ]       {
[controller_10_1_2_230][DEBUG ]         "addr": "10.1.2.230:6789/0", 
[controller_10_1_2_230][DEBUG ]         "name": "controller_10_1_2_230", 
[controller_10_1_2_230][DEBUG ]         "rank": 0
[controller_10_1_2_230][DEBUG ]       }
[controller_10_1_2_230][DEBUG ]     ]
[controller_10_1_2_230][DEBUG ]   }, 
[controller_10_1_2_230][DEBUG ]   "name": "controller_10_1_2_230", 
[controller_10_1_2_230][DEBUG ]   "outside_quorum": [], 
[controller_10_1_2_230][DEBUG ]   "quorum": [
[controller_10_1_2_230][DEBUG ]     0
[controller_10_1_2_230][DEBUG ]   ], 
[controller_10_1_2_230][DEBUG ]   "rank": 0, 
[controller_10_1_2_230][DEBUG ]   "state": "leader", 
[controller_10_1_2_230][DEBUG ]   "sync_provider": []
[controller_10_1_2_230][DEBUG ] }
[controller_10_1_2_230][DEBUG ] ********************************************************************************
[controller_10_1_2_230][INFO  ] monitor: mon.controller_10_1_2_230 is running
[controller_10_1_2_230][INFO  ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.controller_10_1_2_230.asok mon_status
[ceph_deploy.mon][INFO  ] processing monitor mon.controller_10_1_2_230
[controller_10_1_2_230][DEBUG ] connected to host: controller_10_1_2_230 
[controller_10_1_2_230][INFO  ] Running command: ceph --cluster=ceph --admin-daemon /var/run/ceph/ceph-mon.controller_10_1_2_230.asok mon_status
[ceph_deploy.mon][INFO  ] mon.controller_10_1_2_230 monitor has reached quorum!
[ceph_deploy.mon][INFO  ] all initial monitors are running and have formed quorum
[ceph_deploy.mon][INFO  ] Running gatherkeys...
[ceph_deploy.gatherkeys][DEBUG ] Checking controller_10_1_2_230 for /etc/ceph/ceph.client.admin.keyring
[controller_10_1_2_230][DEBUG ] connected to host: controller_10_1_2_230 
[controller_10_1_2_230][DEBUG ] detect platform information from remote host
[controller_10_1_2_230][DEBUG ] detect machine type
[controller_10_1_2_230][DEBUG ] fetch remote file
[ceph_deploy.gatherkeys][DEBUG ] Got ceph.client.admin.keyring key from controller_10_1_2_230.
[ceph_deploy.gatherkeys][DEBUG ] Have ceph.mon.keyring
[ceph_deploy.gatherkeys][DEBUG ] Checking controller_10_1_2_230 for /var/lib/ceph/bootstrap-osd/ceph.keyring
[controller_10_1_2_230][DEBUG ] connected to host: controller_10_1_2_230 
[controller_10_1_2_230][DEBUG ] detect platform information from remote host
[controller_10_1_2_230][DEBUG ] detect machine type
[controller_10_1_2_230][DEBUG ] fetch remote file
[ceph_deploy.gatherkeys][DEBUG ] Got ceph.bootstrap-osd.keyring key from controller_10_1_2_230.
[ceph_deploy.gatherkeys][DEBUG ] Checking controller_10_1_2_230 for /var/lib/ceph/bootstrap-mds/ceph.keyring
[controller_10_1_2_230][DEBUG ] connected to host: controller_10_1_2_230 
[controller_10_1_2_230][DEBUG ] detect platform information from remote host
[controller_10_1_2_230][DEBUG ] detect machine type
[controller_10_1_2_230][DEBUG ] fetch remote file
[ceph_deploy.gatherkeys][DEBUG ] Got ceph.bootstrap-mds.keyring key from controller_10_1_2_230.
Error in sys.exitfunc:
 
e、校验ceph状态
[root@controller_10_1_2_230 ceph-deploy]# ceph -s
    cluster 07462638-a00f-476f-8257-3f4c9ec12d6e
     health HEALTH_ERR 64 pgs stuck inactive; 64 pgs stuck unclean; no osds
     monmap e1: 1 mons at {controller_10_1_2_230=10.1.2.230:6789/0}, election epoch 2, quorum 0 controller_10_1_2_230
     osdmap e1: 0 osds: 0 up, 0 in
      pgmap v2: 64 pgs, 1 pools, 0 bytes data, 0 objects
            0 kB used, 0 kB / 0 kB avail
                  64 creating
PS:由于目前还没有OSD，所有集群目前的health健康状态为:HEALTH_ERR,添加之后，就不会报错了@@
7.添加OSD，按照准备条件，先格式化为xfs文件系统并挂载

a、创建目录
[root@controller_10_1_2_230 ceph-deploy]# ssh root@network_10_1_2_231 'mkdir -pv /data1/osd0'
mkdir: created directory `/data1/osd0'
[root@controller_10_1_2_230 ceph-deploy]# ssh compute1_10_1_2_232 'mkdir -pv /data1/osd1'                       
mkdir: created directory `/data1/osd1'
 
b、OSD准备
[root@controller_10_1_2_230 ceph-deploy]# ceph-deploy osd prepare network_10_1_2_231:/data1/osd0 compute1_10_1_2_232:/data1/osd1
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.19): /usr/bin/ceph-deploy osd prepare network_10_1_2_231:/data1/osd0 compute1_10_1_2_232:/data1/osd1
[ceph_deploy.osd][DEBUG ] Preparing cluster ceph disks network_10_1_2_231:/data1/osd0: compute1_10_1_2_232:/data1/osd1:
[network_10_1_2_231][DEBUG ] connected to host: network_10_1_2_231 
[network_10_1_2_231][DEBUG ] detect platform information from remote host
[network_10_1_2_231][DEBUG ] detect machine type
[ceph_deploy.osd][INFO  ] Distro info: CentOS 6.5 Final
[ceph_deploy.osd][DEBUG ] Deploying osd to network_10_1_2_231
[network_10_1_2_231][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[network_10_1_2_231][WARNIN] osd keyring does not exist yet, creating one
[network_10_1_2_231][DEBUG ] create a keyring file
[network_10_1_2_231][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host network_10_1_2_231 disk /data1/osd0 journal None activate False
[network_10_1_2_231][INFO  ] Running command: ceph-disk -v prepare --fs-type xfs --cluster ceph -- /data1/osd0
[network_10_1_2_231][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[network_10_1_2_231][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[network_10_1_2_231][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[network_10_1_2_231][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[network_10_1_2_231][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[network_10_1_2_231][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[network_10_1_2_231][WARNIN] DEBUG:ceph-disk:Preparing osd data dir /data1/osd0
[network_10_1_2_231][INFO  ] checking OSD status...
[network_10_1_2_231][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host network_10_1_2_231 is now ready for osd use.
[compute1_10_1_2_232][DEBUG ] connected to host: compute1_10_1_2_232 
[compute1_10_1_2_232][DEBUG ] detect platform information from remote host
[compute1_10_1_2_232][DEBUG ] detect machine type
[ceph_deploy.osd][INFO  ] Distro info: CentOS 6.5 Final
[ceph_deploy.osd][DEBUG ] Deploying osd to compute1_10_1_2_232
[compute1_10_1_2_232][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[compute1_10_1_2_232][WARNIN] osd keyring does not exist yet, creating one
[compute1_10_1_2_232][DEBUG ] create a keyring file
[compute1_10_1_2_232][INFO  ] Running command: udevadm trigger --subsystem-match=block --action=add
[ceph_deploy.osd][DEBUG ] Preparing host compute1_10_1_2_232 disk /data1/osd1 journal None activate False
[compute1_10_1_2_232][INFO  ] Running command: ceph-disk -v prepare --fs-type xfs --cluster ceph -- /data1/osd1
[compute1_10_1_2_232][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[compute1_10_1_2_232][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mkfs_options_xfs
[compute1_10_1_2_232][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mkfs_options_xfs
[compute1_10_1_2_232][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_mount_options_xfs
[compute1_10_1_2_232][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-conf --cluster=ceph --name=osd. --lookup osd_fs_mount_options_xfs
[compute1_10_1_2_232][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=osd_journal_size
[compute1_10_1_2_232][WARNIN] DEBUG:ceph-disk:Preparing osd data dir /data1/osd1
[compute1_10_1_2_232][INFO  ] checking OSD status...
[compute1_10_1_2_232][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[ceph_deploy.osd][DEBUG ] Host compute1_10_1_2_232 is now ready for osd use.
Error in sys.exitfunc:
 
c、激活OSD
[root@controller_10_1_2_230 ceph-deploy]# ceph-deploy osd activate network_10_1_2_231:/data1/osd0 compute1_10_1_2_232:/data1/osd1        
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.19): /usr/bin/ceph-deploy osd activate network_10_1_2_231:/data1/osd0 compute1_10_1_2_232:/data1/osd1
[ceph_deploy.osd][DEBUG ] Activating cluster ceph disks network_10_1_2_231:/data1/osd0: compute1_10_1_2_232:/data1/osd1:
[network_10_1_2_231][DEBUG ] connected to host: network_10_1_2_231 
[network_10_1_2_231][DEBUG ] detect platform information from remote host
[network_10_1_2_231][DEBUG ] detect machine type
[ceph_deploy.osd][INFO  ] Distro info: CentOS 6.5 Final
[ceph_deploy.osd][DEBUG ] activating host network_10_1_2_231 disk /data1/osd0
[ceph_deploy.osd][DEBUG ] will use init type: sysvinit
[network_10_1_2_231][INFO  ] Running command: ceph-disk -v activate --mark-init sysvinit --mount /data1/osd0
[network_10_1_2_231][DEBUG ] === osd.0 === 
[network_10_1_2_231][DEBUG ] Starting Ceph osd.0 on network_10_1_2_231...
[network_10_1_2_231][DEBUG ] starting osd.0 at :/0 osd_data /var/lib/ceph/osd/ceph-0 /var/lib/ceph/osd/ceph-0/journal
[network_10_1_2_231][WARNIN] DEBUG:ceph-disk:Cluster uuid is 07462638-a00f-476f-8257-3f4c9ec12d6e
[network_10_1_2_231][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[network_10_1_2_231][WARNIN] DEBUG:ceph-disk:Cluster name is ceph
[network_10_1_2_231][WARNIN] DEBUG:ceph-disk:OSD uuid is 6d298aa5-b33b-4d93-8ac4-efaa671200e5
[network_10_1_2_231][WARNIN] DEBUG:ceph-disk:Allocating OSD id...
[network_10_1_2_231][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 6d298aa5-b33b-4d93-8ac4-efaa671200e5
[network_10_1_2_231][WARNIN] DEBUG:ceph-disk:OSD id is 0
[network_10_1_2_231][WARNIN] DEBUG:ceph-disk:Initializing OSD...
[network_10_1_2_231][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /data1/osd0/activate.monmap
[network_10_1_2_231][WARNIN] got monmap epoch 1
[network_10_1_2_231][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster ceph --mkfs --mkkey -i 0 --monmap /data1/osd0/activate.monmap --osd-data /data1/osd0 --osd-journal /data1/osd0/journal --osd-uuid 6d298aa5-b33b-4d93-8ac4-efaa671200e5 --keyring /data1/osd0/keyring
[network_10_1_2_231][WARNIN] 2016-01-28 15:48:59.836760 7f2ec01337a0 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway
[network_10_1_2_231][WARNIN] 2016-01-28 15:49:00.053509 7f2ec01337a0 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway
[network_10_1_2_231][WARNIN] 2016-01-28 15:49:00.053971 7f2ec01337a0 -1 filestore(/data1/osd0) could not find 23c2fcde/osd_superblock/0//-1 in index: (2) No such file or directory
[network_10_1_2_231][WARNIN] 2016-01-28 15:49:00.262211 7f2ec01337a0 -1 created object store /data1/osd0 journal /data1/osd0/journal for osd.0 fsid 07462638-a00f-476f-8257-3f4c9ec12d6e
[network_10_1_2_231][WARNIN] 2016-01-28 15:49:00.262234 7f2ec01337a0 -1 auth: error reading file: /data1/osd0/keyring: can't open /data1/osd0/keyring: (2) No such file or directory
[network_10_1_2_231][WARNIN] 2016-01-28 15:49:00.262280 7f2ec01337a0 -1 created new key in keyring /data1/osd0/keyring
[network_10_1_2_231][WARNIN] DEBUG:ceph-disk:Marking with init system sysvinit
[network_10_1_2_231][WARNIN] DEBUG:ceph-disk:Authorizing OSD key...
[network_10_1_2_231][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.0 -i /data1/osd0/keyring osd allow * mon allow profile osd
[network_10_1_2_231][WARNIN] added key for osd.0
[network_10_1_2_231][WARNIN] DEBUG:ceph-disk:ceph osd.0 data dir is ready at /data1/osd0
[network_10_1_2_231][WARNIN] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/osd/ceph-0 -> /data1/osd0
[network_10_1_2_231][WARNIN] DEBUG:ceph-disk:Starting ceph osd.0...
[network_10_1_2_231][WARNIN] INFO:ceph-disk:Running command: /sbin/service ceph start osd.0
[network_10_1_2_231][WARNIN] create-or-move updating item name 'osd.0' weight 0.89 at location {host=network_10_1_2_231,root=default} to crush map
[network_10_1_2_231][INFO  ] checking OSD status...
[network_10_1_2_231][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[network_10_1_2_231][INFO  ] Running command: chkconfig ceph on
[compute1_10_1_2_232][DEBUG ] connected to host: compute1_10_1_2_232 
[compute1_10_1_2_232][DEBUG ] detect platform information from remote host
[compute1_10_1_2_232][DEBUG ] detect machine type
[ceph_deploy.osd][INFO  ] Distro info: CentOS 6.5 Final
[ceph_deploy.osd][DEBUG ] activating host compute1_10_1_2_232 disk /data1/osd1
[ceph_deploy.osd][DEBUG ] will use init type: sysvinit
[compute1_10_1_2_232][INFO  ] Running command: ceph-disk -v activate --mark-init sysvinit --mount /data1/osd1
[compute1_10_1_2_232][DEBUG ] === osd.1 === 
[compute1_10_1_2_232][DEBUG ] Starting Ceph osd.1 on compute1_10_1_2_232...
[compute1_10_1_2_232][DEBUG ] starting osd.1 at :/0 osd_data /var/lib/ceph/osd/ceph-1 /var/lib/ceph/osd/ceph-1/journal
[compute1_10_1_2_232][WARNIN] DEBUG:ceph-disk:Cluster uuid is 07462638-a00f-476f-8257-3f4c9ec12d6e
[compute1_10_1_2_232][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster=ceph --show-config-value=fsid
[compute1_10_1_2_232][WARNIN] DEBUG:ceph-disk:Cluster name is ceph
[compute1_10_1_2_232][WARNIN] DEBUG:ceph-disk:OSD uuid is 770582a6-e408-4fb8-b59b-8b781d5e226b
[compute1_10_1_2_232][WARNIN] DEBUG:ceph-disk:Allocating OSD id...
[compute1_10_1_2_232][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring osd create --concise 770582a6-e408-4fb8-b59b-8b781d5e226b
[compute1_10_1_2_232][WARNIN] DEBUG:ceph-disk:OSD id is 1
[compute1_10_1_2_232][WARNIN] DEBUG:ceph-disk:Initializing OSD...
[compute1_10_1_2_232][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring mon getmap -o /data1/osd1/activate.monmap
[compute1_10_1_2_232][WARNIN] got monmap epoch 1
[compute1_10_1_2_232][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph-osd --cluster ceph --mkfs --mkkey -i 1 --monmap /data1/osd1/activate.monmap --osd-data /data1/osd1 --osd-journal /data1/osd1/journal --osd-uuid 770582a6-e408-4fb8-b59b-8b781d5e226b --keyring /data1/osd1/keyring
[compute1_10_1_2_232][WARNIN] 2016-01-28 15:49:08.734889 7fad7e2a3800 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway
[compute1_10_1_2_232][WARNIN] 2016-01-28 15:49:08.976634 7fad7e2a3800 -1 journal FileJournal::_open: disabling aio for non-block journal.  Use journal_force_aio to force use of aio anyway
[compute1_10_1_2_232][WARNIN] 2016-01-28 15:49:08.976965 7fad7e2a3800 -1 filestore(/data1/osd1) could not find 23c2fcde/osd_superblock/0//-1 in index: (2) No such file or directory
[compute1_10_1_2_232][WARNIN] 2016-01-28 15:49:09.285254 7fad7e2a3800 -1 created object store /data1/osd1 journal /data1/osd1/journal for osd.1 fsid 07462638-a00f-476f-8257-3f4c9ec12d6e
[compute1_10_1_2_232][WARNIN] 2016-01-28 15:49:09.285287 7fad7e2a3800 -1 auth: error reading file: /data1/osd1/keyring: can't open /data1/osd1/keyring: (2) No such file or directory
[compute1_10_1_2_232][WARNIN] 2016-01-28 15:49:09.285358 7fad7e2a3800 -1 created new key in keyring /data1/osd1/keyring
[compute1_10_1_2_232][WARNIN] DEBUG:ceph-disk:Marking with init system sysvinit
[compute1_10_1_2_232][WARNIN] DEBUG:ceph-disk:Authorizing OSD key...
[compute1_10_1_2_232][WARNIN] INFO:ceph-disk:Running command: /usr/bin/ceph --cluster ceph --name client.bootstrap-osd --keyring /var/lib/ceph/bootstrap-osd/ceph.keyring auth add osd.1 -i /data1/osd1/keyring osd allow * mon allow profile osd
[compute1_10_1_2_232][WARNIN] added key for osd.1
[compute1_10_1_2_232][WARNIN] DEBUG:ceph-disk:ceph osd.1 data dir is ready at /data1/osd1
[compute1_10_1_2_232][WARNIN] DEBUG:ceph-disk:Creating symlink /var/lib/ceph/osd/ceph-1 -> /data1/osd1
[compute1_10_1_2_232][WARNIN] DEBUG:ceph-disk:Starting ceph osd.1...
[compute1_10_1_2_232][WARNIN] INFO:ceph-disk:Running command: /sbin/service ceph --cluster ceph start osd.1
[compute1_10_1_2_232][WARNIN] libust[36314/36314]: Warning: HOME environment variable not set. Disabling LTTng-UST per-user tracing. (in setup_local_apps() at lttng-ust-comm.c:305)
[compute1_10_1_2_232][WARNIN] create-or-move updating item name 'osd.1' weight 0.89 at location {host=compute1_10_1_2_232,root=default} to crush map
[compute1_10_1_2_232][WARNIN] libust[36363/36363]: Warning: HOME environment variable not set. Disabling LTTng-UST per-user tracing. (in setup_local_apps() at lttng-ust-comm.c:305)
[compute1_10_1_2_232][INFO  ] checking OSD status...
[compute1_10_1_2_232][INFO  ] Running command: ceph --cluster=ceph osd stat --format=json
[compute1_10_1_2_232][INFO  ] Running command: chkconfig ceph on
Error in sys.exitfunc:
 
d、再次校验ceph状态
[root@controller_10_1_2_230 ceph-deploy]# ceph -s
    cluster 07462638-a00f-476f-8257-3f4c9ec12d6e
     health HEALTH_OK                                #状态为OK了
     monmap e1: 1 mons at {controller_10_1_2_230=10.1.2.230:6789/0}, election epoch 2, quorum 0 controller_10_1_2_230   #monitor信息
     osdmap e8: 2 osds: 2 up, 2 in                   #osd信息，有2个OSD，2个状态都是up
      pgmap v14: 64 pgs, 1 pools, 0 bytes data, 0 objects
            10305 MB used, 1814 GB / 1824 GB avail
                  64 active+clean
8. 拷贝管理秘钥至其他节点

[root@controller_10_1_2_230 ceph-deploy]# ceph-deploy admin controller_10_1_2_230 network_10_1_2_231 compute1_10_1_2_232
[ceph_deploy.conf][DEBUG ] found configuration file at: /root/.cephdeploy.conf
[ceph_deploy.cli][INFO  ] Invoked (1.5.19): /usr/bin/ceph-deploy admin controller_10_1_2_230 network_10_1_2_231 compute1_10_1_2_232
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to controller_10_1_2_230
[controller_10_1_2_230][DEBUG ] connected to host: controller_10_1_2_230 
[controller_10_1_2_230][DEBUG ] detect platform information from remote host
[controller_10_1_2_230][DEBUG ] detect machine type
[controller_10_1_2_230][DEBUG ] get remote short hostname
[controller_10_1_2_230][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to network_10_1_2_231
[network_10_1_2_231][DEBUG ] connected to host: network_10_1_2_231 
[network_10_1_2_231][DEBUG ] detect platform information from remote host
[network_10_1_2_231][DEBUG ] detect machine type
[network_10_1_2_231][DEBUG ] get remote short hostname
[network_10_1_2_231][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
[ceph_deploy.admin][DEBUG ] Pushing admin keys and conf to compute1_10_1_2_232
[compute1_10_1_2_232][DEBUG ] connected to host: compute1_10_1_2_232 
[compute1_10_1_2_232][DEBUG ] detect platform information from remote host
[compute1_10_1_2_232][DEBUG ] detect machine type
[compute1_10_1_2_232][DEBUG ] get remote short hostname
[compute1_10_1_2_232][DEBUG ] write cluster configuration to /etc/ceph/{cluster}.conf
Error in sys.exitfunc:
 
其他节点也能够管理ceph集群
[root@network_10_1_2_231 ~]# ll /etc/ceph/ceph.client.admin.keyring 
-rw-r--r-- 1 root root 63 Jan 28 15:52 /etc/ceph/ceph.client.admin.keyring
[root@network_10_1_2_231 ~]# ceph health 
HEALTH_OK
[root@network_10_1_2_231 ~]# ceph -s
    cluster 07462638-a00f-476f-8257-3f4c9ec12d6e
     health HEALTH_OK
     monmap e1: 1 mons at {controller_10_1_2_230=10.1.2.230:6789/0}, election epoch 2, quorum 0 controller_10_1_2_230
     osdmap e8: 2 osds: 2 up, 2 in
      pgmap v15: 64 pgs, 1 pools, 0 bytes data, 0 objects
            10305 MB used, 1814 GB / 1824 GB avail
                  64 active+clean
至此，基本的ceph配置完毕！！！
4. 总结
  以上配置是ceph的最基本的配置，此处只是作为练习使用，在生产中，实际用ceph，需要考虑的因素非常多，如monitor需要有3个，ceph的集群网络和公共网络的规划，OSD策略的调整等等，请继续关注我的blog，后续将会以ceph系列文章的形式出现，此处只是做简单的演示，接下来的博客，将介绍ceph和glance，nova，以及cinder结合，敬请关注！

5.参考链接
http://docs.ceph.com/docs/master/start/quick-ceph-deploy

openstack运维实战系列(十)之nova指定compute节点和IP地址
2016-01-23
1. 背景需求
  在openstack中，nova负责openstack虚拟机的生命周期的管理，neutron则负责虚拟机的网络管理工作，默认情况下，创建一台虚拟机，nova会根据nova-scheduler调度算法，选择一台最合适的compute节点，同时会从neutron的地址池中分配一个ip地址给虚拟机，从而完成虚拟机的创建过程。
  然而，在一些特殊的场景中，如相同业务的虚拟机，希望落在不通的compute节点上，为虚拟机分配原有的ip地址，此时通过nova内置的调度是难以实现的，或者在一些应用场景，基于企业的CMDB联动，让相同业务的虚拟机，散落在openstack nova中的不同节点，从而减小down机的几率，减少故障的发生。本文以在指定的compute节点创建instance，并为其分配一个固定的ip地址为例。

2. 获取创建instance的基本信息
获取镜像image id

[root@controller ~]# glance image-list
+--------------------------------------+--------------------------+-------------+------------------+-------------+--------+
| ID                                   | Name                     | Disk Format | Container Format | Size        | Status |
+--------------------------------------+--------------------------+-------------+------------------+-------------+--------+
| 37aaedc7-6fe6-4fc8-b110-408d166b8e51 | cirrors                  | qcow2       | bare             | 13200896    | active |    #需要创建instance的image id
| ff0f7d03-a553-4357-a819-c74e913d649f | win2k8                   | qcow2       | bare             | 3391881216  | active |
+--------------------------------------+--------------------------+-------------+------------------+-------------+--------+
2. 获取套餐flavor id

[root@controller ~]# nova flavor-list
+--------------------------------------+------------------+-----------+------+-----------+------+-------+-------------+-----------+
| ID                                   | Name             | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |
+--------------------------------------+------------------+-----------+------+-----------+------+-------+-------------+-----------+
| 1                                    | m1.large         | 8192      | 100  | 10        |      | 4     | 1.0         | True      |
| 10                                   | V.GF1            | 8192      | 10   | 0         |      | 4     | 1.0         | True      |    #需要创建的flavor id
3. 获取安全组id

[root@controller ~]# nova secgroup-list
+--------------------------------------+---------+-------------+
| Id                                   | Name    | Description |
+--------------------------------------+---------+-------------+
| 663468d9-73b1-4b04-8d4c-dac1bf21a94d | default | default     |        #secgroup的id号码
+--------------------------------------+---------+-------------+
4. 获取keypair

[root@controller ~]# nova keypair-list
+------+-------------------------------------------------+
| Name | Fingerprint                                     |
+------+-------------------------------------------------+
| KEY  | 15:63:f2:de:74:53:a1:03:eb:36:27:e6:d9:00:41:22 |        #keypair的名字
+------+-------------------------------------------------+
5. 获取neutron网络id号码

[root@controller ~]# neutron net-list
+--------------------------------------+---------------+-------------------------------------------------------+
| id                                   | name          | subnets                                               |
+--------------------------------------+---------------+-------------------------------------------------------+
| 99c68a93-336a-4605-aa78-343d41ca1206 | vmTest        | 79cb82a1-eac1-4311-8e6d-badcabd22e44 192.168.100.0/24 |    #需要启动的网络id号
+--------------------------------------+---------------+-------------------------------------------------------+
6. 获取coompute的主机名和zone名称

[root@controller ~]# nova service-list
+------------------+-------------------------+-------------------+---------+-------+----------------------------+-----------------+
| Binary           | Host                    | Zone              | Status  | State | Updated_at                 | Disabled Reason |
+------------------+-------------------------+-------------------+---------+-------+----------------------------+-----------------+
| nova-conductor   | ChuangYiYuan_10_16_2_8  | internal          | enabled | up    | 2016-01-23T04:54:58.000000 | -               |
| nova-cert        | ChuangYiYuan_10_16_2_8  | internal          | enabled | up    | 2016-01-23T04:54:52.000000 | -               |
| nova-consoleauth | ChuangYiYuan_10_16_2_8  | internal          | enabled | up    | 2016-01-23T04:54:57.000000 | -               |
| nova-scheduler   | ChuangYiYuan_10_16_2_8  | internal          | enabled | up    | 2016-01-23T04:54:57.000000 | -               |
| nova-compute     | ChuangYiYuan_10_16_2_11 | ChuangYiYuanZone1 | enabled | up    | 2016-01-23T04:54:56.000000 | -               |    #需要启动的compute节点
| nova-compute     | ChuangYiYuan_10_16_2_14 | ChuangYiYuanZone1 | enabled | up    | 2016-01-23T04:54:56.000000 | -               |
3. 在指定的compute节点启动虚拟机，并指定固定ip地址

[root@controller ~]# nova boot --flavor 10  \            #flavor名字
--image 37aaedc7-6fe6-4fc8-b110-408d166b8e51 \            #镜像id   
--key-name  KEY  \                         #KEY名字
--availability-zone ChuangYiYuanZone1:ChuangYiYuan_10_16_2_11  \   #在制定的区域和主机名启动instance
--security-groups 663468d9-73b1-4b04-8d4c-dac1bf21a94d  \      #安全组
--nic net-id=99c68a93-336a-4605-aa78-343d41ca1206,v4-fixed-ip=192.168.100.200 happyblog_blog_51cto_com    #指定网络和IP地址
+--------------------------------------+------------------------------------------------+
| Property                             | Value                                          |
+--------------------------------------+------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                         |
| OS-EXT-AZ:availability_zone          | nova                                           |
| OS-EXT-SRV-ATTR:host                 | -                                              |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                              |
| OS-EXT-SRV-ATTR:instance_name        | instance-000001f9                              |
| OS-EXT-STS:power_state               | 0                                              |
| OS-EXT-STS:task_state                | scheduling                                     |
| OS-EXT-STS:vm_state                  | building                                       |
| OS-SRV-USG:launched_at               | -                                              |
| OS-SRV-USG:terminated_at             | -                                              |
| accessIPv4                           |                                                |
| accessIPv6                           |                                                |
| adminPass                            | 4T3vpfaXPTZe                                   |
| config_drive                         |                                                |
| created                              | 2016-01-23T04:44:51Z                           |
| flavor                               | V.GF1 (10)                                     |
| hostId                               |                                                |
| id                                   | 3f694eaf-aa87-456a-99ce-90dd9f4e45ee           |
| image                                | cirrors (37aaedc7-6fe6-4fc8-b110-408d166b8e51) |
| key_name                             | KEY                                            |
| metadata                             | {}                                             |
| name                                 | happyblog_blog_51cto_com                       |
| os-extended-volumes:volumes_attached | []                                             |
| progress                             | 0                                              |
| security_groups                      | 663468d9-73b1-4b04-8d4c-dac1bf21a94d           |
| status                               | BUILD                                          |
| tenant_id                            | 842ab3268a2c47e6a4b0d8774de805ae               |
| updated                              | 2016-01-23T04:44:52Z                           |
| user_id                              | bc5e46fc4204497185ae3ca6f8b7affb               |
+--------------------------------------+------------------------------------------------+
校验配置情况

[root@controller ~]# nova list | grep 3f694eaf-aa87-456a-99ce-90dd9f4e45ee
| 3f694eaf-aa87-456a-99ce-90dd9f4e45ee | happyblog_blog_51cto_com | ACTIVE | -          | Running     | vmTest=192.168.100.200   | ChuangYiYuan_10_16_2_11 |
[root@controller ~]# nova show 3f694eaf-aa87-456a-99ce-90dd9f4e45ee       
+--------------------------------------+----------------------------------------------------------+
| Property                             | Value                                                    |
+--------------------------------------+----------------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                                   |
| OS-EXT-AZ:availability_zone          | ChuangYiYuanZone1                                        |
| OS-EXT-SRV-ATTR:host                 | ChuangYiYuan_10_16_2_11                                  |    #指定的compute节点
| OS-EXT-SRV-ATTR:hypervisor_hostname  | ChuangYiYuan_10_16_2_11                                  |
| OS-EXT-SRV-ATTR:instance_name        | instance-000001f9                                        |
| OS-EXT-STS:power_state               | 1                                                        |
| OS-EXT-STS:task_state                | -                                                        |
| OS-EXT-STS:vm_state                  | active                                                   |
| OS-SRV-USG:launched_at               | 2016-01-23T04:45:06.000000                               |
| OS-SRV-USG:terminated_at             | -                                                        |
| accessIPv4                           |                                                          |
| accessIPv6                           |                                                          |
| config_drive                         |                                                          |
| created                              | 2016-01-23T04:44:51Z                                     |
| flavor                               | V.GF1 (10)                                               |
| hostId                               | b3a37b586ae2847a0b18c58ff7693b41762fa0bf6a3cc363c862761a |
| id                                   | 3f694eaf-aa87-456a-99ce-90dd9f4e45ee                     |
| image                                | cirrors (37aaedc7-6fe6-4fc8-b110-408d166b8e51)           |
| key_name                             | KEY                                                      |
| metadata                             | {}                                                       |
| name                                 | happyblog_blog_51cto_com                                 |
| os-extended-volumes:volumes_attached | []                                                       |
| progress                             | 0                                                        |
| security_groups                      | default                                                  |
| status                               | ACTIVE                                                   |
| tenant_id                            | 842ab3268a2c47e6a4b0d8774de805ae                         |
| updated                              | 2016-01-23T04:45:07Z                                     |
| user_id                              | bc5e46fc4204497185ae3ca6f8b7affb                         |
| vmTest network                       | 192.168.100.200                                          |    #指定的ip地址，good，完成
+--------------------------------------+----------------------------------------------------------+
4. 总结
   以上方法用于openstack的日常运维中，一般而言，不建议使用，建议使用nova内置的调度算法来完成。本人在实际的工作中，经常有指定compute机器和指定ip的需求。

5. 附录
  附录提供了nova boot用法相关的参数
[root@controller ~]# nova help boot
usage: nova boot [--flavor <flavor>] [--image <image>]
                 [--image-with <key=value>] [--boot-volume <volume_id>]
                 [--snapshot <snapshot_id>] [--num-instances <number>]
                 [--meta <key=value>] [--file <dst-path=src-path>]
                 [--key-name <key-name>] [--user-data <user-data>]
                 [--availability-zone <availability-zone>]
                 [--security-groups <security-groups>]
                 [--block-device-mapping <dev-name=mapping>]
                 [--block-device key1=value1[,key2=value2...]]
                 [--swap <swap_size>]
                 [--ephemeral size=<size>[,format=<format>]]
                 [--hint <key=value>]
                 [--nic <net-id=net-uuid,v4-fixed-ip=ip-addr,port-id=port-uuid>]
                 [--config-drive <value>] [--poll]
                 <name>

Boot a new server.

Positional arguments:
  <name>                Name for the new server

Optional arguments:
  --flavor <flavor>     Name or ID of flavor (see 'nova flavor-list').
  --image <image>       Name or ID of image (see 'nova image-list').
  --image-with <key=value>
                        Image metadata property (see 'nova image-show').
  --boot-volume <volume_id>
                        Volume ID to boot from.
  --snapshot <snapshot_id>
                        Snapshot ID to boot from (will create a volume).
  --num-instances <number>
                        boot multiple servers at a time (limited by quota).
  --meta <key=value>    Record arbitrary key/value metadata to /meta.js on the
                        new server. Can be specified multiple times.
  --file <dst-path=src-path>
                        Store arbitrary files from <src-path> locally to <dst-
                        path> on the new server. You may store up to 5 files.
  --key-name <key-name>
                        Key name of keypair that should be created earlier
                        with the command keypair-add
  --user-data <user-data>
                        user data file to pass to be exposed by the metadata
                        server.
  --availability-zone <availability-zone>
                        The availability zone for server placement.
  --security-groups <security-groups>
                        Comma separated list of security group names.
  --block-device-mapping <dev-name=mapping>
                        Block device mapping in the format <dev-
                        name>=<id>:<type>:<size(GB)>:<delete-on-terminate>.
  --block-device key1=value1[,key2=value2...]
                        Block device mapping with the keys: id=image_id,
                        snapshot_id or volume_id, source=source type (image,
                        snapshot, volume or blank), dest=destination type of
                        the block device (volume or local), bus=device's bus,
                        device=name of the device (e.g. vda, xda, ...),
                        size=size of the block device in GB, format=device
                        will be formatted (e.g. swap, ext3, ntfs, ...),
                        bootindex=integer used for ordering the boot disks,
                        type=device type (e.g. disk, cdrom, ...) and
                        shutdown=shutdown behaviour (either preserve or
                        remove).
  --swap <swap_size>    Create and attach a local swap block device of
                        <swap_size> MB.
  --ephemeral size=<size>[,format=<format>]
                        Create and attach a local ephemeral block device of
                        <size> GB and format it to <format>.
  --hint <key=value>    Send arbitrary key/value pairs to the scheduler for
                        custom use.
  --nic <net-id=net-uuid,v4-fixed-ip=ip-addr,port-id=port-uuid>
                        Create a NIC on the server. Specify option multiple
                        times to create multiple NICs. net-id: attach NIC to
                        network with this UUID (required if no port-id), v4
                        -fixed-ip: IPv4 fixed address for NIC (optional),
                        port-id: attach NIC to port with this UUID (required
                        if no net-id)
  --config-drive <value>
                        Enable config drive
  --poll                Blocks while server builds so progress can be
                        reported.

openstack运维实战系列(十三)之glance更改路径引发的"血案"

1. 背景说明
  glance在openstack中负责镜像相关的服务，支持将运行的虚拟机转换为快照，镜像和快照都存储在glance中，glance的后端支持多种存储方式，包括本地的文件系统，http，glusterfs，ceph，swift等等。
  默认情况下，glance采用本地文件系统的方式存储image，存储的路径为/var/lib/glance/images,随着时间的推移，当镜像越来越多的时候，根目录的空间将会越来越大，所以对于glance的路径来说，需要提前做好规划和准备，如划分一个单独的空间存储image，或者存放在分布式的文件系统，如ceph，swift上等。我所在的环境中，刚上线的时候，由于缺乏对glance的规划，采用默认的路径/var/lib/glance/images，后来因为空间的不够的问题，而采取更改路径，在更改的过程中，引发了"血案".

2. 血案现场
创建一台虚拟机

#获取镜像id
 
[root@controller ~]# glance image-list
+--------------------------------------+---------------+-------------+------------------+-------------+--------+
| ID                                   | Name          | Disk Format | Container Format | Size        | Status |
+--------------------------------------+---------------+-------------+------------------+-------------+--------+
| 37aaedc7-6fe6-4fc8-b110-408d166b8e51 | cirrors       | qcow2       | bare             | 13200896    | active |
 
#获取网络的id号
[root@controller ~]# neutron net-list
+--------------------------------------+---------------+-------------------------------------------------------+
| id                                   | name          | subnets                                               |
+--------------------------------------+---------------+-------------------------------------------------------+
| 99c68a93-336a-4605-aa78-343d41ca1206 | vmTest        | 79cb82a1-eac1-4311-8e6d-badcabd22e44 192.168.100.0/24 |
+--------------------------------------+---------------+-------------------------------------------------------+
 
#获取flavor的id号码
[root@controller ~]# nova flavor-list
+--------------------------------------+------------------+-----------+------+-----------+------+-------+-------------+-----------+
| ID                                   | Name             | Memory_MB | Disk | Ephemeral | Swap | VCPUs | RXTX_Factor | Is_Public |
+--------------------------------------+------------------+-----------+------+-----------+------+-------+-------------+-----------+
| 1                                    | m1.large         | 8192      | 100  | 10        |      | 4     | 1.0         | True      |
2. 创建instance

[root@controller ~]# nova boot --flavor m1.large --image 37aaedc7-6fe6-4fc8-b110-408d166b8e51 --nic net-id=99c68a93-336a-4605-aa78-343d41ca1206 glance_image_error_test
+--------------------------------------+------------------------------------------------+
| Property                             | Value                                          |
+--------------------------------------+------------------------------------------------+
| OS-DCF:diskConfig                    | MANUAL                                         |
| OS-EXT-AZ:availability_zone          | nova                                           |
| OS-EXT-SRV-ATTR:host                 | -                                              |
| OS-EXT-SRV-ATTR:hypervisor_hostname  | -                                              |
| OS-EXT-SRV-ATTR:instance_name        | instance-000001ff                              |
| OS-EXT-STS:power_state               | 0                                              |
| OS-EXT-STS:task_state                | scheduling                                     |
| OS-EXT-STS:vm_state                  | building                                       |
| OS-SRV-USG:launched_at               | -                                              |
| OS-SRV-USG:terminated_at             | -                                              |
| accessIPv4                           |                                                |
| accessIPv6                           |                                                |
| adminPass                            | X39vzn4RKwrL                                   |
| config_drive                         |                                                |
| created                              | 2016-01-27T11:14:46Z                           |
| flavor                               | m1.large (1)                                   |
| hostId                               |                                                |
| id                                   | b143fd7d-b1b7-49b4-ba20-7968777460bc           |
| image                                | cirrors (37aaedc7-6fe6-4fc8-b110-408d166b8e51) |
| key_name                             | -                                              |
| metadata                             | {}                                             |
| name                                 | glance_image_error_test                        |
| os-extended-volumes:volumes_attached | []                                             |
| progress                             | 0                                              |
| security_groups                      | default                                        |
| status                               | BUILD                                          |
| tenant_id                            | 842ab3268a2c47e6a4b0d8774de805ae               |
| updated                              | 2016-01-27T11:14:46Z                           |
| user_id                              | bc5e46fc4204497185ae3ca6f8b7affb               |
+--------------------------------------+------------------------------------------------+
3. 创建失败

[root@controller ~]# nova list |grep b143fd7d-b1b7-49b4-ba20-7968777460bc 
| b143fd7d-b1b7-49b4-ba20-7968777460bc | glance_image_error_test  | ERROR  | -          | NOSTATE     |                          | ChuangYiYuan_10_16_2_21 |
3.寻根究底
查看glance日志，包括glance-api和glance-registry

[root@controller ~]# tail -n 2 /var/log/glance/api.log
2016-01-27 19:15:22.917 2664 INFO urllib3.connectionpool [-] Starting new HTTP connection (1): controller
2016-01-27 19:15:22.948 2664 INFO glance.wsgi.server [89d3f8c3-9d66-4d75-b88c-eafe746f9a6b bc5e46fc4204497185ae3ca6f8b7affb 842ab3268a2c47e6a4b0d8774de805ae - - -] 10.16.2.8 - - [27/Jan/2016 19:15:22] "HEAD /v1/images/37aaedc7-6fe6-4fc8-b110-408d166b8e51 HTTP/1.1" 200 856 0.031628
 
[root@controller ~]# tail -n 2 /var/log/glance/registry.log
2016-01-27 19:15:22.946 2763 INFO glance.registry.api.v1.images [cca31ae2-f412-4605-a5db-0cc0a507955b bc5e46fc4204497185ae3ca6f8b7affb 842ab3268a2c47e6a4b0d8774de805ae - - -] Successfully retrieved image 37aaedc7-6fe6-4fc8-b110-408d166b8e51
2016-01-27 19:15:22.946 2763 INFO glance.wsgi.server [cca31ae2-f412-4605-a5db-0cc0a507955b bc5e46fc4204497185ae3ca6f8b7affb 842ab3268a2c47e6a4b0d8774de805ae - - -] 127.0.0.1 - - [27/Jan/2016 19:15:22] "GET /images/37aaedc7-6fe6-4fc8-b110-408d166b8e51 HTTP/1.1" 200 847 0.017350
#!!未发现有异常!!
2. 查看nova的日志，包括nova-api,nova-scheduler,nova-conductor和nova-compute节点日志

2016-01-09 17:42:09.653 2872 WARNING nova.openstack.common.loopingcall [-] task run outlasted interval by 9.578928 sec
2016-01-09 17:47:25.755 2872 WARNING nova.openstack.common.loopingcall [-] task run outlasted interval by 5.842983 sec
2016-01-27 19:14:49.762 2872 ERROR nova.scheduler.filter_scheduler [req-46235a89-6ed4-47e5-ac06-85f6dedc8985 bc5e46fc4204497185ae3ca6f8b7affb 842ab3268a2c47e6a4b0d8774de805ae]
[instance: b143fd7d-b1b7-49b4-ba20-7968777460bc] Error from last host: ChuangYiYuan_10_16_2_22 (node ChuangYiYuan_10_16_2_22): [u'Traceback (most recent call last):\n', u'  Fil
e "/usr/lib/python2.6/site-packages/nova/compute/manager.py", line 1328, in _build_instance\n    set_access_ip=set_access_ip)\n', u'  File "/usr/lib/python2.6/site-packages/nov
a/compute/manager.py", line 393, in decorated_function\n    return function(self, context, *args, **kwargs)\n', u'  File "/usr/lib/python2.6/site-packages/nova/compute/manager.
py", line 1740, in _spawn\n    LOG.exception(_(\'Instance failed to spawn\'), instance=instance)\n', u'  File "/usr/lib/python2.6/site-packages/nova/openstack/common/excutils.p
y", line 68, in __exit__\n    six.reraise(self.type_, self.value, self.tb)\n', u'  File "/usr/lib/python2.6/site-packages/nova/compute/manager.py", line 1737, in _spawn\n    bl
ock_device_info)\n', u'  File "/usr/lib/python2.6/site-packages/nova/virt/libvirt/driver.py", line 2287, in spawn\n    admin_pass=admin_password)\n', u'  File "/usr/lib/python2
.6/site-packages/nova/virt/libvirt/driver.py", line 2656, in _create_image\n    project_id=instance[\'project_id\'])\n', u'  File "/usr/lib/python2.6/site-packages/nova/virt/li
bvirt/imagebackend.py", line 192, in cache\n    *args, **kwargs)\n', u'  File "/usr/lib/python2.6/site-packages/nova/virt/libvirt/imagebackend.py", line 383, in create_image\n
   prepare_template(target=base, max_size=size, *args, **kwargs)\n', u'  File "/usr/lib/python2.6/site-packages/nova/openstack/common/lockutils.py", line 249, in inner\n    ret
urn f(*args, **kwargs)\n', u'  File "/usr/lib/python2.6/site-packages/nova/virt/libvirt/imagebackend.py", line 182, in fetch_func_sync\n    fetch_func(target=target, *args, **k
wargs)\n', u'  File "/usr/lib/python2.6/site-packages/nova/virt/libvirt/utils.py", line 653, in fetch_image\n    max_size=max_size)\n', u'  File "/usr/lib/python2.6/site-packag
es/nova/virt/images.py", line 78, in fetch_to_raw\n    max_size=max_size)\n', u'  File "/usr/lib/python2.6/site-packages/nova/virt/images.py", line 72, in fetch\n    image_serv
ice.download(context, image_id, dst_path=path)\n', u'  File "/usr/lib/python2.6/site-packages/nova/image/glance.py", line 331, in download\n    _reraise_translated_image_except
ion(image_id)\n', u'  File "/usr/lib/python2.6/site-packages/nova/image/glance.py", line 329, in download\n    image_chunks = self._client.call(context, 1, \'data\', image_id)\
n', u'  File "/usr/lib/python2.6/site-packages/nova/image/glance.py", line 209, in call\n    return getattr(client.images, method)(*args, **kwargs)\n', u'  File "/usr/lib/pytho
n2.6/site-packages/glanceclient/v1/images.py", line 127, in data\n    % urllib.quote(str(image_id)))\n', u'  File "/usr/lib/python2.6/site-packages/glanceclient/common/http.py"
, line 289, in raw_request\n    return self._http_request(url, method, **kwargs)\n', u'  File "/usr/lib/python2.6/site-packages/glanceclient/common/http.py", line 249, in _http
_request\n    raise exc.from_response(resp, body_str)\n', u'ImageNotFound: Image 37aaedc7-6fe6-4fc8-b110-408d166b8e51 could not be found.\n']
 
#在nova-scheduler和nova-compute的日志中查看到"ImageNotFound: Image 37aaedc7-6fe6-4fc8-b110-408d166b8e51 could not be found"的报错信息!
3.查看glance的服务状态

[root@controller ~]# /etc/init.d/openstack-glance-api status
openstack-glance-api (pid  2222) is running...
[root@controller ~]# /etc/init.d/openstack-glance-registry status
openstack-glance-registry (pid  2694) is running...
#状态正常
 
[root@controller ~]# glance image-list
+--------------------------------------+---------------+-------------+------------------+-------------+--------+
| ID                                   | Name          | Disk Format | Container Format | Size        | Status |
+--------------------------------------+---------------+-------------+------------------+-------------+--------+
| 37aaedc7-6fe6-4fc8-b110-408d166b8e51 | cirrors       | qcow2       | bare             | 13200896    | active |
+--------------------------------------+---------------+-------------+------------------+-------------+--------+
#正常工作，尝试upload一个镜像，也能够正常工作，原因何在呢？？
4.抓住元凶
   因为在运维过程中，修改过glance的默认路径由/var/lib/glance/images修改为/data1/glance,并且将/var/lib/glance/images下的镜像都mv至/data1/glance下了，而此时尽管数据已经前已过去了，但是image的元数据信息却牢牢的记录在glance的image_locations表中，查看得知:

mysql> select * from glance.image_locations where image_id='37aaedc7-6fe6-4fc8-b110-408d166b8e51'\G;                                                                        
*************************** 1. row ***************************
        id: 37
  image_id: 37aaedc7-6fe6-4fc8-b110-408d166b8e51
     value: file:///var/lib/glance/images/37aaedc7-6fe6-4fc8-b110-408d166b8e51    #元凶
created_at: 2015-12-21 06:10:24
updated_at: 2015-12-21 06:10:24
deleted_at: NULL
   deleted: 0
 meta_data: {}
    status: active
1 row in set (0.00 sec)
真像:原来原有目录/var/lib/glance/images目录下的镜像都已经mv至/data1/glance下，而数据库中却依旧记录着就的路径内容，从而，衍生的一个问题：当nova尝试启动一台instance的时候，nova会到instance镜像缓存路径，默认/var/lib/nova/_base下查找是否有该镜像，如果没有则向glance发起result api请求，请求下载指定image的镜像到本地，glance则根据数据库中image_locations所定义的值去查找镜像，从而导致失败！
解决方法:更新glance的元数据信息

mysql> update glance.image_locations set value='file:///data1/glance/37aaedc7-6fe6-4fc8-b110-408d166b8e51' where image_id='37aaedc7-6fe6-4fc8-b110-408d166b8e51'\G;             
Query OK, 1 row affected (0.05 sec)
Rows matched: 1  Changed: 1  Warnings: 0
 
#重建虚拟机，故障解决！！！
5. 进一步探索
  glance中，主要有两张表很重要:images和image_locations,其中image负责存储镜像相关的信息，而image_locations记录镜像的存储url路径。
images数据表

mysql> select * from glance.images limit 2\G;
*************************** 1. row ***************************
              id: 0267dcbf-9f72-4ce8-9976-7106e38ee948
            name: cirror1
            size: 6899532
          status: deleted
       is_public: 1
      created_at: 2015-12-02 01:45:13
      updated_at: 2015-12-02 01:46:41
      deleted_at: 2015-12-02 01:46:41
         deleted: 1
     disk_format: qcow2
container_format: bare
        checksum: 7c607794659403b970a5d0a00fb2c311
           owner: 842ab3268a2c47e6a4b0d8774de805ae
        min_disk: 0
         min_ram: 0
       protected: 0
    virtual_size: NULL
*************************** 2. row ***************************
              id: 2437cede-d03a-4680-b704-6d27c4d7198e
            name: test1
            size: 0
          status: deleted
       is_public: 0
      created_at: 2015-12-21 09:02:41
      updated_at: 2015-12-21 09:06:02
      deleted_at: 2015-12-21 09:06:02
         deleted: 1
     disk_format: qcow2
container_format: bare
        checksum: d41d8cd98f00b204e9800998ecf8427e
           owner: 842ab3268a2c47e6a4b0d8774de805ae
        min_disk: 0
         min_ram: 0
       protected: 0
    virtual_size: NULL
2 rows in set (0.00 sec)
#即记录着创建时候相关信息，还记得deleted字段的作用么？哈哈，删除镜像的原理？？额
2. image_locations表

mysql> select * from image_locations;
+----+--------------------------------------+--------------------------------------------------------------------+---------------------+---------------------+---------------------+---------+-----------+--------+
| id | image_id                             | value                                                              | created_at          | updated_at          | deleted_at          | deleted | meta_data | status |
+----+--------------------------------------+--------------------------------------------------------------------+---------------------+---------------------+---------------------+---------+-----------+--------+
|  1 | 437d860f-1c9f-4bb2-a3ca-8ec062441909 | file:///var/lib/glance/images/437d860f-1c9f-4bb2-a3ca-8ec062441909 | 2015-06-24 10:40:39 | 2015-12-01 11:52:20 | 2015-12-01 11:52:20 |       1 | {}        | active |
|  2 | 5ce414b0-660a-46e1-ad0a-b842b2afc0b7 | file:///var/lib/glance/images/5ce414b0-660a-46e1-ad0a-b842b2afc0b7 | 2015-06-25 02:49:33 | 2015-06-25 02:49:33 | NULL
6. 附录
images表的结构:

mysql> desc glance.images;
+------------------+--------------+------+-----+---------+-------+
| Field            | Type         | Null | Key | Default | Extra |
+------------------+--------------+------+-----+---------+-------+
| id               | varchar(36)  | NO   | PRI | NULL    |       |
| name             | varchar(255) | YES  |     | NULL    |       |
| size             | bigint(20)   | YES  |     | NULL    |       |
| status           | varchar(30)  | NO   |     | NULL    |       |
| is_public        | tinyint(1)   | NO   | MUL | NULL    |       |
| created_at       | datetime     | NO   |     | NULL    |       |
| updated_at       | datetime     | YES  |     | NULL    |       |
| deleted_at       | datetime     | YES  |     | NULL    |       |
| deleted          | tinyint(1)   | NO   | MUL | NULL    |       |
| disk_format      | varchar(20)  | YES  |     | NULL    |       |
| container_format | varchar(20)  | YES  |     | NULL    |       |
| checksum         | varchar(32)  | YES  | MUL | NULL    |       |
| owner            | varchar(255) | YES  | MUL | NULL    |       |
| min_disk         | int(11)      | NO   |     | NULL    |       |
| min_ram          | int(11)      | NO   |     | NULL    |       |
| protected        | tinyint(1)   | YES  |     | NULL    |       |
| virtual_size     | bigint(20)   | YES  |     | NULL    |       |
+------------------+--------------+------+-----+---------+-------+
17 rows in set (0.00 sec)
2. image_locations表结构

mysql> desc image_locations;
+------------+-------------+------+-----+---------+----------------+
| Field      | Type        | Null | Key | Default | Extra          |
+------------+-------------+------+-----+---------+----------------+
| id         | int(11)     | NO   | PRI | NULL    | auto_increment |
| image_id   | varchar(36) | NO   | MUL | NULL    |                |
| value      | text        | NO   |     | NULL    |                |
| created_at | datetime    | NO   |     | NULL    |                |
| updated_at | datetime    | YES  |     | NULL    |                |
| deleted_at | datetime    | YES  |     | NULL    |                |
| deleted    | tinyint(1)  | NO   | MUL | NULL    |                |
| meta_data  | text        | YES  |     | NULL    |                |
| status     | varchar(30) | NO   |     | active  |                |
+------------+-------------+------+-----+---------+----------------+
9 rows in set (0.00 sec)


openstack运维实战系列(二)之instance ERR故障处理

1. 故障现象
    在nova的运维过程中，有时候需要对instance进行迁移migrate，大小变更resize的功能，或者关机重启等操作，有可能会导致instance处于error的状态，原因存在多方面，如nova-compute不异常，instance本身有问题，或者迁移的过程出现故障。现象如下：

[root@controller ~]# nova list |grep 10.1.128.202
| c96c63e6-b289-433f-b919-422b8a90c900 | GAME_20151023_02 | ERROR  | -          | Running     | private_yz=10.1.128.202

2. 解决方法
    如下是解决类似故障的方法，先将instance的状态更改为正常状态active，然后才能执行正常的重启，关闭，硬重启等操作，具体如下：

[root@controller ~]# nova reset-state --active c96c63e6-b289-433f-b919-422b8a90c900
[root@controller ~]# nova reboot --hard c96c63e6-b289-433f-b919-422b8a90c900


openstack运维实战系列(九)之cinder与glusterfs结合

1. 概述
    cinder作为openstack的快存储服务，为instance提供永久的volume服务，cinder作为一种可插拔式的服务，能够支持各种存储类型，包括专业的FC存储，如EMC，NetApp，HP，IBM，huawei等商场的专业存储服务器，存储厂商只要开发对应的驱动和cinder对接即可；此外，cinder还支持开源的分布式存储，如glusterfs，ceph，sheepdog，nfs等，通过开源的分布式存储方案，能够达到廉价的IP-SAN存储。本文以glusterfs构建分布式存储，以供cinder使用。
2. 构建glusterfs存储
    glusterfs是一种开源的分布式存储解决方案，能够支持集中方式：1. replicate复制(类似于RAID1)，2.stripe分片(类似于RAID0),3. distribute-replicate分布式复制，4. distribute-replicate-stripe分布式复制和分片(类似于RAID10)，本文采用的方式。
环境说明
本文有两台机器组件glusterfs集群，分别是:10.1.112.55和10.1.112.56,两台机器分别有11块盘，每块3T，磁盘名字从/dev/sdb至/dev/sdl,挂载至/data2-/data12,如下:

[root@YiZhuang_10_1_112_55 ~]# df -h
Filesystem            Size  Used Avail Use% Mounted on
/dev/sda2             9.9G  2.6G  6.9G  27% /
tmpfs                 7.8G     0  7.8G   0% /dev/shm
/dev/sda1            1008M   82M  876M   9% /boot
/dev/sda4             257G  188M  244G   1% /data1
/dev/sdb1             2.8T  118M  2.8T   1% /data2
/dev/sdc1             2.8T  118M  2.8T   1% /data3
/dev/sdd1             2.8T  118M  2.8T   1% /data4
/dev/sde1             2.8T  118M  2.8T   1% /data5
/dev/sdf1             2.8T  118M  2.8T   1% /data6
/dev/sdg1             2.8T  118M  2.8T   1% /data7
/dev/sdh1             2.8T  118M  2.8T   1% /data8
/dev/sdi1             2.8T  117M  2.8T   1% /data9
/dev/sdj1             2.8T  118M  2.8T   1% /data10
/dev/sdk1             2.8T  118M  2.8T   1% /data11
/dev/sdl1             2.8T  118M  2.8T   1% /data12
架构如下:
wKiom1ah6q2SCFwzAAC7Fcrs0wc342.png
2. 激活glusterfs邻居peer

[root@YiZhuang_10_1_112_55 ~]# gluster peer probe 10.1.112.56
peer probe: success. 
#查看
[root@YiZhuang_10_1_112_55 ~]# gluster peer status
Number of Peers: 1
Hostname: 10.1.112.56
Uuid: a720fd05-4fa7-4ff7-924e-2d8a40e48c18
State: Peer in Cluster (Connected)

3. 基于brick创建volume，切割成11份，分别存储在两台机器(类似于RAID10)

[root@YiZhuang_10_1_112_55 ~]# gluster volume create openstack_cinder stripe 11 replica 2 transport tcp \
10.1.112.55:/data2/cinder 10.1.112.56:/data2/cinder \
10.1.112.55:/data3/cinder 10.1.112.56:/data3/cinder \
10.1.112.55:/data4/cinder 10.1.112.56:/data4/cinder \
10.1.112.55:/data5/cinder 10.1.112.56:/data5/cinder \
10.1.112.55:/data6/cinder 10.1.112.56:/data6/cinder \
10.1.112.55:/data7/cinder 10.1.112.56:/data7/cinder \
10.1.112.55:/data8/cinder 10.1.112.56:/data8/cinder \
10.1.112.55:/data9/cinder 10.1.112.56:/data9/cinder \
10.1.112.55:/data10/cinder 10.1.112.56:/data10/cinder \
10.1.112.55:/data11/cinder 10.1.112.56:/data11/cinder \
10.1.112.55:/data12/cinder 10.1.112.56:/data12/cinder
4. 查看glusterfs volume的结构

[root@YiZhuang_10_1_112_55 ~]# gluster volume info
  
Volume Name: openstack_cinder
Type: Striped-Replicate
Volume ID: c55ff01b-3be0-4514-b622-83677f95924a
Status: Started
Number of Bricks: 1 x 11 x 2 = 22
Transport-type: tcp
Bricks:
Brick1: 10.1.112.55:/data2/cinder
Brick2: 10.1.112.56:/data2/cinder
Brick3: 10.1.112.55:/data3/cinder
Brick4: 10.1.112.56:/data3/cinder
Brick5: 10.1.112.55:/data4/cinder
Brick6: 10.1.112.56:/data4/cinder
Brick7: 10.1.112.55:/data5/cinder
Brick8: 10.1.112.56:/data5/cinder
Brick9: 10.1.112.55:/data6/cinder
Brick10: 10.1.112.56:/data6/cinder
Brick11: 10.1.112.55:/data7/cinder
Brick12: 10.1.112.56:/data7/cinder
Brick13: 10.1.112.55:/data8/cinder
Brick14: 10.1.112.56:/data8/cinder
Brick15: 10.1.112.55:/data9/cinder
Brick16: 10.1.112.56:/data9/cinder
Brick17: 10.1.112.55:/data10/cinder
Brick18: 10.1.112.56:/data10/cinder
Brick19: 10.1.112.55:/data11/cinder
Brick20: 10.1.112.56:/data11/cinder
Brick21: 10.1.112.55:/data12/cinder
Brick22: 10.1.112.56:/data12/cinder
5. 启动glusterfs volume

[root@YiZhuang_10_1_112_55 ~]# gluster volume start openstack_cinder        #开启glusterfs volume
 
#查看glusterfs volume的状态
[root@YiZhuang_10_1_112_55 ~]# gluster volume status
Status of volume: openstack_cinder
Gluster process                                         Port    Online  Pid
------------------------------------------------------------------------------
Brick 10.1.112.55:/data2/cinder                         59152   Y       4121
Brick 10.1.112.56:/data2/cinder                         59152   Y       43596
Brick 10.1.112.55:/data3/cinder                         59153   Y       4132
Brick 10.1.112.56:/data3/cinder                         59153   Y       43607
Brick 10.1.112.55:/data4/cinder                         59154   Y       4143
Brick 10.1.112.56:/data4/cinder                         59154   Y       43618
Brick 10.1.112.55:/data5/cinder                         59155   Y       4154
Brick 10.1.112.56:/data5/cinder                         59155   Y       43629
Brick 10.1.112.55:/data6/cinder                         59156   Y       4165
Brick 10.1.112.56:/data6/cinder                         59156   Y       43640
Brick 10.1.112.55:/data7/cinder                         59157   Y       4176
Brick 10.1.112.56:/data7/cinder                         59157   Y       43651
Brick 10.1.112.55:/data8/cinder                         59158   Y       4187
Brick 10.1.112.56:/data8/cinder                         59158   Y       43662
Brick 10.1.112.55:/data9/cinder                         59159   Y       4198
Brick 10.1.112.56:/data9/cinder                         59159   Y       43673
Brick 10.1.112.55:/data10/cinder                        59160   Y       4209
Brick 10.1.112.56:/data10/cinder                        59160   Y       43684
Brick 10.1.112.55:/data11/cinder                        59161   Y       4220
Brick 10.1.112.56:/data11/cinder                        59161   Y       43695
Brick 10.1.112.55:/data12/cinder                        59162   Y       4231
Brick 10.1.112.56:/data12/cinder                        59162   Y       43706
NFS Server on localhost                                 2049    Y       4244
Self-heal Daemon on localhost                           N/A     Y       4251
NFS Server on 10.1.112.56                               2049    Y       43718
Self-heal Daemon on 10.1.112.56                         N/A     Y       43727
  
Task Status of Volume openstack_cinder
------------------------------------------------------------------------------
6. 挂载测试

[root@YiZhuang_10_1_112_56 ~]# mount.glusterfs 10.1.112.56:openstack_cinder /media/ 
[root@YiZhuang_10_1_112_56 ~]# df
Filesystem             1K-blocks    Used   Available Use% Mounted on
/dev/sda2               10321208 2488348     7308572  26% /
tmpfs                    8140364       0     8140364   0% /dev/shm
/dev/sda1                1032088   83596      896064   9% /boot
/dev/sda4              268751588  191660   254908060   1% /data1
/dev/sdb1             2928834296   32972  2928801324   1% /data2
/dev/sdc1             2928834296   32972  2928801324   1% /data3
/dev/sdd1             2928834296   32972  2928801324   1% /data4
/dev/sde1             2928834296   32972  2928801324   1% /data5
/dev/sdf1             2928834296   32972  2928801324   1% /data6
/dev/sdg1             2928834296   32972  2928801324   1% /data7
/dev/sdh1             2928834296   32972  2928801324   1% /data8
/dev/sdi1             2928834296   32972  2928801324   1% /data9
/dev/sdj1             2928834296   32972  2928801324   1% /data10
/dev/sdk1             2928834296   32972  2928801324   1% /data11
/dev/sdl1             2928834296   32972  2928801324   1% /data12
10.1.112.56:openstack_cinder
                     32217177216  362752 32216814464   1% /media        #已经挂载
3. cinder和glusterfs结合
cinder-volume端配置内容如下

[DEFAULT]
enabled_backends = glusterfs
[glusterfs]                                                          #最后添加
volume_driver = cinder.volume.drivers.glusterfs.GlusterfsDriver      #驱动  
glusterfs_shares_config = /etc/cinder/shares.conf                    #glusterfs存储
glusterfs_mount_point_base = /var/lib/cinder/volumes                 #挂载点
volume_backend_name = glusterfs                                      #后端名字，用于在controller上和cinder的type结合@@
2. 配置glusterfs存储配置

[root@YiZhuang_10_1_112_55 ~]# vim /etc/cinder/shares.conf 
10.1.112.55:/openstack_cinder
3. 重启cinder-volume服务

[root@YiZhuang_10_1_112_55 init.d]# chkconfig openstack-cinder-volume on
[root@YiZhuang_10_1_112_55 init.d]# service  openstack-cinder-volume restart
Stopping openstack-cinder-volume:                          [  OK  ]
Starting openstack-cinder-volume:                          [  OK  ]
@@@两台机器，执行相同的操作，并检查日志信息，看是否有错误/var/log/cinder/volume.log@@@@
4. controller节点检查服务状态

[root@controller ~]# cinder service-list
+------------------+--------------------------------+------+---------+-------+----------------------------+-----------------+
|      Binary      |              Host              | Zone |  Status | State |         Updated_at         | Disabled Reason |
+------------------+--------------------------------+------+---------+-------+----------------------------+-----------------+
| cinder-scheduler |        controller              | nova | enabled |   up  | 2016-01-22T08:52:14.000000 |       None      |
|  cinder-volume   | YiZhuang_10_1_112_55@glusterfs | nova | enabled |   up  | 2016-01-22T08:52:17.000000 |       None      |    #说明正常
|  cinder-volume   | YiZhuang_10_1_112_56@glusterfs | nova | enabled |   up  | 2016-01-22T08:52:04.000000 |       None      |
+------------------+--------------------------------+------+---------+-------+----------------------------+-----------------+
5. controller建立type

[root@controller ~]# cinder type-create glusterfs
+--------------------------------------+------------+
|                  ID                  |    Name    |
+--------------------------------------+------------+
| 6688e8f9-e744-4c21-b570-fd81b099d4c0 | glusterfs  |
+--------------------------------------+------------+
6. controller配置cinder-type和volume_backend_name联动

[root@controller ~]# cinder type-key set  6688e8f9-e744-4c21-b570-fd81b099d4c0 volume_backend_name=glusterfs
#查看type的设置情况
 
[root@controller~]# cinder extra-specs-list
+--------------------------------------+-----------+----------------------------------------+
|                  ID                  |    Name   |              extra_specs               |
+--------------------------------------+-----------+----------------------------------------+
| 6688e8f9-e744-4c21-b570-fd81b099d4c0 | glusterfs | {u'volume_backend_name': u'glusterfs'} |    #关联完毕
+--------------------------------------+-----------+----------------------------------------+
7. 重启controller的cinder服务

[root@LuGu_10_1_81_205 ~]# /etc/init.d/openstack-cinder-api  restart
Stopping openstack-cinder-api:                             [  OK  ]
Starting openstack-cinder-api:                             [  OK  ]
[root@LuGu_10_1_81_205 ~]# /etc/init.d/openstack-cinder-scheduler restart
Stopping openstack-cinder-scheduler:                       [  OK  ]
Starting openstack-cinder-scheduler:                       [  OK  ]
4. 功能测试
创建cinder volume

[root@controller ~]# cinder create --display-name "test1" --volume-type glusterfs 10        #执行cinder type的类型
+---------------------+--------------------------------------+
|       Property      |                Value                 |
+---------------------+--------------------------------------+
|     attachments     |                  []                  |
|  availability_zone  |                 nova                 |
|       bootable      |                false                 |
|      created_at     |      2016-01-22T09:01:48.978864      |
| display_description |                 None                 |
|     display_name    |                test1                 |
|      encrypted      |                False                 |
|          id         | 3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001 |
|       metadata      |                  {}                  |
|         size        |                  10                  |
|     snapshot_id     |                 None                 |
|     source_volid    |                 None                 |
|        status       |               creating               |
|     volume_type     |              glusterfs               |
+---------------------+--------------------------------------+
[root@controller ~]# cinder show  3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001 
+--------------------------------+--------------------------------------+
|            Property            |                Value                 |
+--------------------------------+--------------------------------------+
|          attachments           |                  []                  |
|       availability_zone        |                 nova                 |
|            bootable            |                false                 |
|           created_at           |      2016-01-22T09:01:48.000000      |
|      display_description       |                 None                 |
|          display_name          |                test1                 |
|           encrypted            |                False                 |
|               id               | 3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001 |
|            metadata            |                  {}                  |
|     os-vol-host-attr:host      |    YiZhuang_10_1_112_55@glusterfs    |        #落在10.1.112.55这台机器
| os-vol-mig-status-attr:migstat |                 None                 |
| os-vol-mig-status-attr:name_id |                 None                 |
|  os-vol-tenant-attr:tenant_id  |   a49b16d5324a4d20bde2217b17200485   |
|              size              |                  10                  |
|          snapshot_id           |                 None                 |
|          source_volid          |                 None                 |
|             status             |              available               |        #创建成功，状态为available
|          volume_type           |              glusterfs               |
+--------------------------------+--------------------------------------+
2. 校验glusterfs的切割情况

[root@YiZhuang_10_1_112_56 ~]# for num in {2..12}
> do
> ls -lh /data${num}/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
> done
-rw-rw-rw- 2 root root 931M Jan 22 17:01 /data2/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
-rw-rw-rw- 2 root root 931M Jan 22 17:01 /data3/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
-rw-rw-rw- 2 root root 931M Jan 22 17:01 /data4/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
-rw-rw-rw- 2 root root 931M Jan 22 17:01 /data5/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
-rw-rw-rw- 2 root root 931M Jan 22 17:01 /data6/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
-rw-rw-rw- 2 root root 931M Jan 22 17:01 /data7/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
-rw-rw-rw- 2 root root 931M Jan 22 17:01 /data8/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
-rw-rw-rw- 2 root root 931M Jan 22 17:01 /data9/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
-rw-rw-rw- 2 root root 931M Jan 22 17:01 /data10/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
-rw-rw-rw- 2 root root 931M Jan 22 17:01 /data11/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
-rw-rw-rw- 2 root root 931M Jan 22 17:01 /data12/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001    #10G的磁盘，切割为11份，分别落在11个磁盘上，达到负载均衡的效果，另外一台机器的数据是一模一样
3. 校验两台机器的数据的md5

[root@YiZhuang_10_1_112_56 ~]# for num in {2..12}; do md5sum /data${num}/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001; done      
e0f8c6646f8ce81fe6be0b12f1511aa1  /data2/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e0f8c6646f8ce81fe6be0b12f1511aa1  /data3/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e0f8c6646f8ce81fe6be0b12f1511aa1  /data4/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e18d850c6c53cdeb2e346fcd28c7a189  /data5/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e18d850c6c53cdeb2e346fcd28c7a189  /data6/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e18d850c6c53cdeb2e346fcd28c7a189  /data7/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e18d850c6c53cdeb2e346fcd28c7a189  /data8/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e18d850c6c53cdeb2e346fcd28c7a189  /data9/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e18d850c6c53cdeb2e346fcd28c7a189  /data10/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e18d850c6c53cdeb2e346fcd28c7a189  /data11/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e18d850c6c53cdeb2e346fcd28c7a189  /data12/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
 
另外一台机器
[root@YiZhuang_10_1_112_55 ~]# for num in {2..12} ; do md5sum /data${num}/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001; done      
e0f8c6646f8ce81fe6be0b12f1511aa1  /data2/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e0f8c6646f8ce81fe6be0b12f1511aa1  /data3/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e0f8c6646f8ce81fe6be0b12f1511aa1  /data4/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e18d850c6c53cdeb2e346fcd28c7a189  /data5/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e18d850c6c53cdeb2e346fcd28c7a189  /data6/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e18d850c6c53cdeb2e346fcd28c7a189  /data7/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e18d850c6c53cdeb2e346fcd28c7a189  /data8/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e18d850c6c53cdeb2e346fcd28c7a189  /data9/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e18d850c6c53cdeb2e346fcd28c7a189  /data10/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e18d850c6c53cdeb2e346fcd28c7a189  /data11/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
e18d850c6c53cdeb2e346fcd28c7a189  /data12/cinder/volume-3f0577c0-2e64-4c8d-a8a8-2b8da6b8d001
 
对比发现，两者的md5一模一样，说明两者是相同的文件，互为镜像，至此，glusterfs和cinder联动配置完毕！！
5. 总结
    cinder作为openstack中管理volume的一个服务，主要承担管理的角色，存储的功能，有专业的存储方案来完成，如本文的glusterfs开源分布式存储，此外，cinder还可以针对不同的后端设置不同的type，如后端可能是专业的存储服务器，或者是SSD构建的glusterfs，或者SATA构建的ceph存储，可以设置不同的type，分配给不同的虚拟机，已达到不同性能的需求，关于这些功能，参考openstack cinder的配置文档。

	
openstack运维实战系列(十一)之neutron替换instance的IP

1. 背景说明
  生产环境下openstack使用了vlan的的网络模式，针对业务需求不同，设置不同的网段，分别放置在不通的vlan中，通过vlan的方式，实现网络的隔离，网络相关的策略，可以通过现有的交换机,防火墙来设置。有时候，业务申请的虚拟机使用一段时间之后，需要将网络A切换至另外一个网络B，并且需要保留原有的虚拟机和数据，对于这种场景，就可以通过neutron端口替换的方式实现。
  neutron在openstack中负责虚拟机网络相关的服务，作为一种插件式的服务，neutron能够支持各种插件，包括OpenVswitch,Cisco插件，Boacade等网络厂商的插件，同时为了网络的可扩展性和tenant之间的流量隔离，neutron能够支持多种网络隔离技术，常见的隔离技术包括:vlan,gre,vxlan,gre,linux bridge等。根据使用场景的不同，可以选择不同的网络模式，我所在的工作环境中，使用的网络模式为vlan。关于网络的说明，请继续关注我的博客，此处不再赘述。
2. 实现过程
查看需要替换地址的instance

[root@controller ~]# nova list |grep 192.168.100.200
| 3f694eaf-aa87-456a-99ce-90dd9f4e45ee | happyblog_blog_51cto_com | ACTIVE | -          | Running     | vmTest=192.168.100.200   | ChuangYiYuan_10_16_2_11 |
 
#需要将192.168.100.200这台机器的ip，替换成10.16.4.x网段的ip地址
2. 将instance的port卸载

#将port从instance中detach 
[root@controller ~]# nova interface-detach 3f694eaf-aa87-456a-99ce-90dd9f4e45ee  4c158efa-6cba-4d62-99d7-590877586c09    
 
[root@controller ~]# nova list |grep 3f694eaf-aa87-456a-99ce-90dd9f4e45ee
| 3f694eaf-aa87-456a-99ce-90dd9f4e45ee | happyblog_blog_51cto_com | ACTIVE | -          | Running     |                         | ChuangYiYuan_10_16_2_11|
 
#发现instance 3f694eaf-aa87-456a-99ce-90dd9f4e45ee此时没有IP地址了！！
3. 基于指定的sunbet创建一个端口

[root@controller ~]# neutron subnet-list
+--------------------------------------+----------------+------------------+------------------------------------------------------+
| id                                   | name           | cidr             | allocation_pools                                     |
+--------------------------------------+----------------+------------------+------------------------------------------------------+
| 79cb82a1-eac1-4311-8e6d-badcabd22e44 | ForTest        | 192.168.100.0/24 | {"start": "192.168.100.2", "end": "192.168.100.254"} |
| 9654a807-d4fa-49f1-abb6-2e45d776c69f | Subnet_INSIDE  | 10.16.4.0/23     | {"start": "10.16.4.10", "end": "10.16.5.254"}        |#基于该subnet创建port
 
查看network情况
[root@controller ~]# neutron net-list
+--------------------------------------+---------------+-------------------------------------------------------+
| id                                   | name          | subnets                                               |
+--------------------------------------+---------------+-------------------------------------------------------+
| 43b5c341-c22d-445a-94d1-e2c84722ad4e | vmProdution   | 9654a807-d4fa-49f1-abb6-2e45d776c69f 10.16.4.0/23     |   #网络ID好，后续使用
| 99c68a93-336a-4605-aa78-343d41ca1206 | vmTest        | 79cb82a1-eac1-4311-8e6d-badcabd22e44 192.168.100.0/24 |
+--------------------------------------+---------------+-------------------------------------------------------+
 
#基于指定的subnet，创建一个端口port
[root@controller ~]# neutron port-create --name port-1  --fixed-ip subnet_id=9654a807-d4fa-49f1-abb6-2e45d776c69f,ip_address=10.16.4.58  \
--security-group 663468d9-73b1-4b04-8d4c-dac1bf21a94d  43b5c341-c22d-445a-94d1-e2c84722ad4e        
Created a new port:
+-----------------------+-----------------------------------------------------------------------------------+
| Field                 | Value                                                                             |
+-----------------------+-----------------------------------------------------------------------------------+
| admin_state_up        | True                                                                              |
| allowed_address_pairs |                                                                                   |
| binding:host_id       |                                                                                   |
| binding:profile       | {}                                                                                |
| binding:vif_details   | {}                                                                                |
| binding:vif_type      | unbound                                                                           |
| binding:vnic_type     | normal                                                                            |
| device_id             |                                                                                   |
| device_owner          |                                                                                   |
| fixed_ips             | {"subnet_id": "9654a807-d4fa-49f1-abb6-2e45d776c69f", "ip_address": "10.16.4.58"} |   #指定的ip地址了
| id                    | ae64c08e-ac2e-4a28-ae89-0d4d2fb67981                                              |   #ID号码，记住
| mac_address           | fa:16:3e:1d:c0:9a                                                                 |
| name                  | port-1                                                                            |
| network_id            | 43b5c341-c22d-445a-94d1-e2c84722ad4e                                              |
| security_groups       | 663468d9-73b1-4b04-8d4c-dac1bf21a94d                                              |
| status                | DOWN                                                                              |
| tenant_id             | 842ab3268a2c47e6a4b0d8774de805ae                                                  |
+-----------------------+-----------------------------------------------------------------------------------+
 
#查看端口的信息
[root@controller ~]# neutron port-list |grep  ae64c08e-ac2e-4a28-ae89-0d4d2fb67981 
| ae64c08e-ac2e-4a28-ae89-0d4d2fb67981 | port-1 | fa:16:3e:1d:c0:9a | {"subnet_id": "9654a807-d4fa-49f1-abb6-2e45d776c69f", "ip_address": "10.16.4.58"}  |
[root@controller ~]# 
[root@controller ~]# neutron port-show  ae64c08e-ac2e-4a28-ae89-0d4d2fb67981 
+-----------------------+-----------------------------------------------------------------------------------+
| Field                 | Value                                                                             |
+-----------------------+-----------------------------------------------------------------------------------+
| admin_state_up        | True                                                                              |
| allowed_address_pairs |                                                                                   |
| binding:host_id       |                                                                                   |
| binding:profile       | {}                                                                                |
| binding:vif_details   | {}                                                                                |
| binding:vif_type      | unbound                                                                           |
| binding:vnic_type     | normal                                                                            |
| device_id             |                                                                                   |
| device_owner          |                                                                                   |
| extra_dhcp_opts       |                                                                                   |
| fixed_ips             | {"subnet_id": "9654a807-d4fa-49f1-abb6-2e45d776c69f", "ip_address": "10.16.4.58"} |
| id                    | ae64c08e-ac2e-4a28-ae89-0d4d2fb67981                                              |
| mac_address           | fa:16:3e:1d:c0:9a                                                                 |
| name                  | port-1                                                                            |
| network_id            | 43b5c341-c22d-445a-94d1-e2c84722ad4e                                              |
| security_groups       | 663468d9-73b1-4b04-8d4c-dac1bf21a94d                                              |
| status                | DOWN                                                                              |
| tenant_id             | 842ab3268a2c47e6a4b0d8774de805ae                                                  |
+-----------------------+-----------------------------------------------------------------------------------+
4. 将所创建的port和instance关联

[root@controller ~]# nova list |grep happy
| 3f694eaf-aa87-456a-99ce-90dd9f4e45ee | happyblog_blog_51cto_com | ACTIVE | -          | Running     |                         | ChuangYiYuan_10_16_2_11 |
#执行关联操作，用法参考nova help interface-attach
[root@controller ~]# nova interface-attach --port-id ae64c08e-ac2e-4a28-ae89-0d4d2fb67981 3f694eaf-aa87-456a-99ce-90dd9f4e45ee
[root@controller ~]# nova list |grep 3f694eaf-aa87-456a-99ce-90dd9f4e45ee
| 3f694eaf-aa87-456a-99ce-90dd9f4e45ee | happyblog_blog_51cto_com | ACTIVE | -          | Running     | vmProdution=10.16.4.58   | ChuangYiYuan_10_16_2_11 |
#关联完毕!!!

3. 小结
   关于端口的替换，可以参考上面的例子，neutron作为可插拔式的服务，其port可以随便切换，这也是neutron的强大之处。

openstack运维实战系列(七)之cinder配额调整

1. 前言
    openstack默认为了防止用户随意使用存储空间，默认针对cinder做了限制，防止用户过度使用存储空间，和nova与neutron相类似，cinder的quota也是有一个专门的驱动来完成，当超过quota时，使用cinder将会失败。

2. cinder默认的quota
1. 默认配置

quota_driver=cinder.quota.DbQuotaDriver            quota的驱动，源代码的路径
quota_volumes=10                                   volume的个数
quota_snapshots=10                                 快照个数
quota_gigabytes=1000                               volume的空间大小，默认单位是GB，包括快照和volume的空间
reservation_expire=86400                           过期时长
max_age=0                                          刷新间隔
2. 查看tenant默认的配额

[root@controller ~]# cinder quota-defaults  7ff1dfb5a6f349958c3a949248e56236           
+-----------+-------+
|  Property | Value |
+-----------+-------+
| gigabytes |  1000 |                            允许使用的空间
| snapshots |  10   |                            快照的个数
|  volumes  |  10   |                            volume的个数
3. 查看quota的使用情况

[root@controller ~]# cinder quota-usage 7ff1dfb5a6f349958c3a949248e56236   
+-----------+--------+----------+-------+
|    Type   | In_use | Reserved | Limit |
+-----------+--------+----------+-------+
| gigabytes |   0    |    0     | 1000  |
| snapshots |   0    |    0     |  10   |
|  volumes  |   0    |    0     |  10   |
+-----------+--------+----------+-------+
3. 调整quota配置

[root@controller ~]# cinder quota-update --volumes 100 --snapshots 100 --gigabytes 5000 7ff1dfb5a6f349958c3a949248e56236
[root@controller ~]# cinder quota-show 7ff1dfb5a6f349958c3a949248e56236
+-----------+-------+
|  Property | Value |
+-----------+-------+
| gigabytes |  5000 |
| snapshots |  100  |
|  volumes  |  100  |
+-----------+-------+
[root@controller ~]# cinder quota-usage 7ff1dfb5a6f349958c3a949248e56236
+-----------+--------+----------+-------+
|    Type   | In_use | Reserved | Limit |
+-----------+--------+----------+-------+
| gigabytes |   0    |    0     |  5000 |
| snapshots |   0    |    0     |  100  |
|  volumes  |   0    |    0     |  100  |
+-----------+--------+----------+-------+
4. 总结
    关于cinder quota的配置，只需要借助cinder quota*相关的子命令即可完成配额的调整，在实际的生产环境中，随着volume的个数和存储空间的增大，当达到cinder的quota之后，将无法建立cinder的volume，具体的信息可以参考/var/log/cinder/cinder.api.log中查看即可，可以根据日志的提示内容，修改quota即可。

5. 附录
    关于cinder quota的源代码，可以参考如下内容

"""Quotas for volumes."""
import datetime
from oslo.config import cfg
from cinder import context
from cinder import db
from cinder import exception
from cinder.openstack.common import importutils
from cinder.openstack.common import log as logging
from cinder.openstack.common import timeutils
LOG = logging.getLogger(__name__)
'''
在cinder的配置文件中，注册quota的配置选项，包括volume,snpshots,gigabytes空间，driver等
'''
quota_opts = [
    cfg.IntOpt('quota_volumes',
               default=10,
               help='number of volumes allowed per project'),
    cfg.IntOpt('quota_snapshots',
               default=10,
               help='number of volume snapshots allowed per project'),
    cfg.IntOpt('quota_gigabytes',
               default=1000,
               help='number of volume gigabytes (snapshots are also included) '
                    'allowed per project'),
    cfg.IntOpt('reservation_expire',
               default=86400,
               help='number of seconds until a reservation expires'),
    cfg.IntOpt('until_refresh',
               default=0,
               help='count of reservations until usage is refreshed'),
    cfg.IntOpt('max_age',
               default=0,
               help='number of seconds between subsequent usage refreshes'),
    cfg.StrOpt('quota_driver',
               default='cinder.quota.DbQuotaDriver',
               help='default driver to use for quota checks'),
    cfg.BoolOpt('use_default_quota_class',
                default=True,
                help='whether to use default quota class for default quota'), ]
CONF = cfg.CONF
CONF.register_opts(quota_opts)
'''
cinder quota的管理驱动，即包含了增删改查，和cinder配额的校验函数，都封装在该class内，cinder的quota和nova的quota实现方式非常类似
'''
class DbQuotaDriver(object):
    """Driver to perform check to enforcement of quotas.
    Also allows to obtain quota information.
    The default driver utilizes the local database.
    """
     
    #获取tenant的quota信息，即cinder quota-show <tenant_id>的实现函数
    def get_by_project(self, context, project_id, resource_name):
        """Get a specific quota by project."""
        #从数据库中，调用nova.db.api下的quota_get()函数，返回数据库中关于tenant的配置信息
        return db.quota_get(context, project_id, resource_name)     
    def get_by_class(self, context, quota_class, resource_name):
        """Get a specific quota by quota class."""
        return db.quota_class_get(context, quota_class, resource_name)
    #获取默认的quota配置信息，通过调用quota_class_get_default()方法实现
    def get_default(self, context, resource):
        """Get a specific default quota for a resource."""
        default_quotas = db.quota_class_get_default(context)
        return default_quotas.get(resource.name, resource.default)
    #调用系统默认的cinder quota配置内容
    def get_defaults(self, context, resources):
        """Given a list of resources, retrieve the default quotas.
        Use the class quotas named `_DEFAULT_QUOTA_NAME` as default quotas,
        if it exists.
        :param context: The request context, for access checks.
        :param resources: A dictionary of the registered resources.
        """
        quotas = {}
        default_quotas = {}
        if CONF.use_default_quota_class:
            default_quotas = db.quota_class_get_default(context)
        for resource in resources.values():
            if resource.name not in default_quotas:
                LOG.deprecated(_("Default quota for resource: %(res)s is set "
                                 "by the default quota flag: quota_%(res)s, "
                                 "it is now deprecated. Please use the "
                                 "the default quota class for default "
                                 "quota.") % {'res': resource.name})
            quotas[resource.name] = default_quotas.get(resource.name,
                                                       resource.default)
        return quotas
    def get_class_quotas(self, context, resources, quota_class,
                         defaults=True):
        """Given list of resources, retrieve the quotas for given quota class.
        :param context: The request context, for access checks.
        :param resources: A dictionary of the registered resources.
        :param quota_class: The name of the quota class to return
                            quotas for.
        :param defaults: If True, the default value will be reported
                         if there is no specific value for the
                         resource.
        """
        quotas = {}
        default_quotas = {}
        class_quotas = db.quota_class_get_all_by_name(context, quota_class)
        if defaults:
            default_quotas = db.quota_class_get_default(context)
        for resource in resources.values():
            if resource.name in class_quotas:
                quotas[resource.name] = class_quotas[resource.name]
                continue
            if defaults:
                quotas[resource.name] = default_quotas.get(resource.name,
                                                           resource.default)
        return quotas
    '''
            获取tenant的配额信息
    '''
    def get_project_quotas(self, context, resources, project_id,
                           quota_class=None, defaults=True,
                           usages=True):
        """Given a list of resources, retrieve the quotas for the given
        project.
        :param context: The request context, for access checks.
        :param resources: A dictionary of the registered resources.
        :param project_id: The ID of the project to return quotas for.
        :param quota_class: If project_id != context.project_id, the
                            quota class cannot be determined.  This
                            parameter allows it to be specified.  It
                            will be ignored if project_id ==
                            context.project_id.
        :param defaults: If True, the quota class value (or the
                         default value, if there is no value from the
                         quota class) will be reported if there is no
                         specific value for the resource.
        :param usages: If True, the current in_use and reserved counts
                       will also be returned.
        """
        quotas = {}
        project_quotas = db.quota_get_all_by_project(context, project_id)
        if usages:
            project_usages = db.quota_usage_get_all_by_project(context,
                                                               project_id)
        # Get the quotas for the appropriate class.  If the project ID
        # matches the one in the context, we use the quota_class from
        # the context, otherwise, we use the provided quota_class (if
        # any)
        if project_id == context.project_id:
            quota_class = context.quota_class
        if quota_class:
            class_quotas = db.quota_class_get_all_by_name(context, quota_class)
        else:
            class_quotas = {}
        default_quotas = self.get_defaults(context, resources)
        for resource in resources.values():
            # Omit default/quota class values
            if not defaults and resource.name not in project_quotas:
                continue
            quotas[resource.name] = dict(
                limit=project_quotas.get(
                    resource.name,
                    class_quotas.get(resource.name,
                                     default_quotas[resource.name])),
            )
            # Include usages if desired.  This is optional because one
            # internal consumer of this interface wants to access the
            # usages directly from inside a transaction.
            if usages:
                usage = project_usages.get(resource.name, {})
                quotas[resource.name].update(
                    in_use=usage.get('in_use', 0),
                    reserved=usage.get('reserved', 0), )
        return quotas
    def _get_quotas(self, context, resources, keys, has_sync, project_id=None):
        """A helper method which retrieves the quotas for specific resources.
        This specific resource is identified by keys, and which apply to the
        current context.
        :param context: The request context, for access checks.
        :param resources: A dictionary of the registered resources.
        :param keys: A list of the desired quotas to retrieve.
        :param has_sync: If True, indicates that the resource must
                         have a sync attribute; if False, indicates
                         that the resource must NOT have a sync
                         attribute.
        :param project_id: Specify the project_id if current context
                           is admin and admin wants to impact on
                           common user's tenant.
        """
        # Filter resources
        if has_sync:
            sync_filt = lambda x: hasattr(x, 'sync')
        else:
            sync_filt = lambda x: not hasattr(x, 'sync')
        desired = set(keys)
        sub_resources = dict((k, v) for k, v in resources.items()
                             if k in desired and sync_filt(v))
        # Make sure we accounted for all of them...
        if len(keys) != len(sub_resources):
            unknown = desired - set(sub_resources.keys())
            raise exception.QuotaResourceUnknown(unknown=sorted(unknown))
        # Grab and return the quotas (without usages)
        quotas = self.get_project_quotas(context, sub_resources,
                                         project_id,
                                         context.quota_class, usages=False)
        return dict((k, v['limit']) for k, v in quotas.items())
    '''
                配额的检查函数
    '''
    def limit_check(self, context, resources, values, project_id=None):
        """Check simple quota limits.
        For limits--those quotas for which there is no usage
        synchronization function--this method checks that a set of
        proposed values are permitted by the limit restriction.
        This method will raise a QuotaResourceUnknown exception if a
        given resource is unknown or if it is not a simple limit
        resource.
        If any of the proposed values is over the defined quota, an
        OverQuota exception will be raised with the sorted list of the
        resources which are too high.  Otherwise, the method returns
        nothing.
        :param context: The request context, for access checks.
        :param resources: A dictionary of the registered resources.
        :param values: A dictionary of the values to check against the
                       quota.
        :param project_id: Specify the project_id if current context
                           is admin and admin wants to impact on
                           common user's tenant.
        """
        # Ensure no value is less than zero
        unders = [key for key, val in values.items() if val < 0]
        if unders:
            raise exception.InvalidQuotaValue(unders=sorted(unders))
        # If project_id is None, then we use the project_id in context
        if project_id is None:
            project_id = context.project_id
        # Get the applicable quotas
        quotas = self._get_quotas(context, resources, values.keys(),
                                  has_sync=False, project_id=project_id)
        # Check the quotas and construct a list of the resources that
        # would be put over limit by the desired values
        overs = [key for key, val in values.items()
                 if quotas[key] >= 0 and quotas[key] < val]
        if overs:
            raise exception.OverQuota(overs=sorted(overs), quotas=quotas,
                                      usages={})
    def reserve(self, context, resources, deltas, expire=None,
                project_id=None):
        """Check quotas and reserve resources.
        For counting quotas--those quotas for which there is a usage
        synchronization function--this method checks quotas against
        current usage and the desired deltas.
        This method will raise a QuotaResourceUnknown exception if a
        given resource is unknown or if it does not have a usage
        synchronization function.
        If any of the proposed values is over the defined quota, an
        OverQuota exception will be raised with the sorted list of the
        resources which are too high.  Otherwise, the method returns a
        list of reservation UUIDs which were created.
        :param context: The request context, for access checks.
        :param resources: A dictionary of the registered resources.
        :param deltas: A dictionary of the proposed delta changes.
        :param expire: An optional parameter specifying an expiration
                       time for the reservations.  If it is a simple
                       number, it is interpreted as a number of
                       seconds and added to the current time; if it is
                       a datetime.timedelta object, it will also be
                       added to the current time.  A datetime.datetime
                       object will be interpreted as the absolute
                       expiration time.  If None is specified, the
                       default expiration time set by
                       --default-reservation-expire will be used (this
                       value will be treated as a number of seconds).
        :param project_id: Specify the project_id if current context
                           is admin and admin wants to impact on
                           common user's tenant.
        """
        # Set up the reservation expiration
        if expire is None:
            expire = CONF.reservation_expire
        if isinstance(expire, (int, long)):
            expire = datetime.timedelta(seconds=expire)
        if isinstance(expire, datetime.timedelta):
            expire = timeutils.utcnow() + expire
        if not isinstance(expire, datetime.datetime):
            raise exception.InvalidReservationExpiration(expire=expire)
        # If project_id is None, then we use the project_id in context
        if project_id is None:
            project_id = context.project_id
        # Get the applicable quotas.
        # NOTE(Vek): We're not worried about races at this point.
        #            Yes, the admin may be in the process of reducing
        #            quotas, but that's a pretty rare thing.
        quotas = self._get_quotas(context, resources, deltas.keys(),
                                  has_sync=True, project_id=project_id)
        # NOTE(Vek): Most of the work here has to be done in the DB
        #            API, because we have to do it in a transaction,
        #            which means access to the session.  Since the
        #            session isn't available outside the DBAPI, we
        #            have to do the work there.
        return db.quota_reserve(context, resources, quotas, deltas, expire,
                                CONF.until_refresh, CONF.max_age,
                                project_id=project_id)
    '''
                提交确认函数
    '''
    def commit(self, context, reservations, project_id=None):
        """Commit reservations.
        :param context: The request context, for access checks.
        :param reservations: A list of the reservation UUIDs, as
                             returned by the reserve() method.
        :param project_id: Specify the project_id if current context
                           is admin and admin wants to impact on
                           common user's tenant.
        """
        # If project_id is None, then we use the project_id in context
        if project_id is None:
            project_id = context.project_id
        db.reservation_commit(context, reservations, project_id=project_id)
    #回滚函数
    def rollback(self, context, reservations, project_id=None):
        """Roll back reservations.
        :param context: The request context, for access checks.
        :param reservations: A list of the reservation UUIDs, as
                             returned by the reserve() method.
        :param project_id: Specify the project_id if current context
                           is admin and admin wants to impact on
                           common user's tenant.
        """
        # If project_id is None, then we use the project_id in context
        if project_id is None:
            project_id = context.project_id
        db.reservation_rollback(context, reservations, project_id=project_id)
    #删除tenant的配额，即执行cinder quota-delete的操作，恢复到默认值
    def destroy_all_by_project(self, context, project_id):
        """Destroy all that is associated with a project.
        This includes quotas, usages and reservations.
        :param context: The request context, for access checks.
        :param project_id: The ID of the project being deleted.
        """
        db.quota_destroy_all_by_project(context, project_id)
    #保留过期的cinder
    def expire(self, context):
        """Expire reservations.
        Explores all currently existing reservations and rolls back
        any that have expired.
        :param context: The request context, for access checks.
        """
        db.reservation_expire(context)

openstack运维实战系列(六)之neutron配额调整

1. 前言
    neutron在安装配置完成之后，openstack为了实现对所有tenant对网络资源的使用，针对neutron设置有专门的配额，以防止租户使用过多的资源，而对其他的tenant造成影响。和nova的quota相类似，neutron也使用单独的一个驱动来实现网络neutron的配额控制。

2. neutron默认的配额
    neutron默认的配额针对network，port，router，subnet，floatingip做了配额方面的限定，参考neutron的配置文件，获取quota的配额内容为:

[root@controller ~]# vim /etc/neutron/neutron.conf 
[quotas]
quota_driver = neutron.db.quota_db.DbQuotaDriver        配额驱动
quota_items = network,subnet,port                       quota限定的范畴
default_quota = -1                                      默认的quota，-1表示没有限制(未启用)
quota_network = 10                                      建立的network个数
quota_subnet = 10                                       建立的subnet个数
quota_port = 50                                         允许的port个数
quota_security_group = 10                               安全组的个数
quota_security_group_rule = 100                         安全组规规则条数
quota_vip = 10                                          vip个数，以下的quota_member和quota_health_monitors 都用于LBaaS场景
quota_pool = 10                                         pool个数
quota_member = -1                                       member个数
quota_health_monitors = -1                              monitor个数
quota_router = 10                                       router的个数
quota_floatingip = 50                                   floating-ip个数
3. 修改neutron的配额
查看neutron默认的配额

[root@controller ~]# keystone tenant-list
+----------------------------------+----------+---------+
|                id                |   name   | enabled |
+----------------------------------+----------+---------+
| 842ab3268a2c47e6a4b0d8774de805ae |  admin   |   True  |
| 7ff1dfb5a6f349958c3a949248e56236 | companyA |   True  |        #得到tenant的uuid号
| 10d1465c00d049fab88dec1af0f56b1b |   demo   |   True  |
| 3b57a14f7c354a979c9f62b60f31a331 | service  |   True  |
+----------------------------------+----------+---------+
 
[root@controller ~]# neutron quota-show --tenant-id 7ff1dfb5a6f349958c3a949248e56236
+---------------------+-------+
| Field               | Value |
+---------------------+-------+
| floatingip          | 50    |
| health_monitor      | -1    |
| member              | -1    |
| network             | 10    |
| pool                | 10    |
| port                | 50    |            #port，每台虚拟机都需要一个ip，即一个port，很容易就超过配额
| router              | 10    |
| security_group      | 10    |
| security_group_rule | 100   |
| subnet              | 10    |
| vip                 | 10    |
+---------------------+-------+
2. 修改neutron配额

[root@controller ~]# neutron quota-update --network 20 --subnet 20 --port 100  --router 5 --floatingip 100  --security-group 10 --security-group-rule 100 --tenant-id 7ff1dfb5a6f349958c3a949248e56236 
+---------------------+-------+
| Field               | Value |
+---------------------+-------+
| floatingip          | 100   |
| health_monitor      | -1    |
| member              | -1    |
| network             | 20    |
| pool                | 10    |
| port                | 100   |
| router              | 5     |
| security_group      | 10    |
| security_group_rule | 100   |
| subnet              | 20    |
| vip                 | 10    |
+---------------------+-------+
 
校验neutron的quota配置
[root@controller ~]# neutron quota-show --tenant-id 7ff1dfb5a6f349958c3a949248e56236 
+---------------------+-------+
| Field               | Value |
+---------------------+-------+
| floatingip          | 100   |
| health_monitor      | -1    |
| member              | -1    |
| network             | 20    |
| pool                | 10    |
| port                | 100   |
| router              | 5     |
| security_group      | 10    |
| security_group_rule | 100   |
| subnet              | 20    |
| vip                 | 10    |
+---------------------+-------+
4. 统计port的个数

[root@controller ~]# neutron port-list
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------------+
| id                                   | name | mac_address       | fixed_ips                                                                             |
+--------------------------------------+------+-------------------+---------------------------------------------------------------------------------------+
| 0060ec4a-957d-4571-b730-6b4a9bb3baf8 |      | fa:16:3e:48:42:3d | {"subnet_id": "9654a807-d4fa-49f1-abb6-2e45d776c69f", "ip_address": "10.16.4.19"}     |
| 00942be0-a3a9-471d-a4ba-336db0ee1539 |      | fa:16:3e:73:75:03 | {"subnet_id": "ad4a5ffc-3ccc-42c4-89a1-61e7b18632a3", "ip_address": "10.16.6.96"}     |
| 0119045c-8219-4744-bd58-a7e77294832c |      | fa:16:3e:10:ed:7f | {"subnet_id": "9654a807-d4fa-49f1-abb6-2e45d776c69f", "ip_address": "10.16.4.71"}     |
| 04f7d8ea-1849-4938-9ef7-e8114893132f |      | fa:16:3e:50:86:1b | {"subnet_id": "ad4a5ffc-3ccc-42c4-89a1-61e7b18632a3", "ip_address": "10.16.6.27"}     |
 
[root@controller ~]# neutron port-list |wc -l            #超过配额时，需要修改
194

5. 总结
    随着时间的推移，当越来越多得instance加入到openstack中，port也会相应增加，一个ip对应一个port，所以当port达到配额时，openstack会组织用户继续分配虚拟机，此时，就需要修改neutron的配额了，关于neutron配额的报错，可以参考neutron的日志/var/log/neutron/neutron-server.log，可以根据日志的信息，定位到报错的原因，具体不赘述。

6. 附录
    neutron实现quota的代码解读

[root@controller ~]# vim /usr/lib/python2.6/site-packages/neutron/db/quota_db.py
 
import sqlalchemy as sa
 
from neutron.common import exceptions
from neutron.db import model_base
from neutron.db import models_v2
 
'''
quota数据库表的表结构，tenant默认集成的配额从这里获取
mysql> desc quotas;
+-----------+--------------+------+-----+---------+-------+
| Field     | Type         | Null | Key | Default | Extra |
+-----------+--------------+------+-----+---------+-------+
| id        | varchar(36)  | NO   | PRI | NULL    |       |
| tenant_id | varchar(255) | YES  | MUL | NULL    |       |
| resource  | varchar(255) | YES  |     | NULL    |       |
| limit     | int(11)      | YES  |     | NULL    |       |
+-----------+--------------+------+-----+---------+-------+
'''
class Quota(model_base.BASEV2, models_v2.HasId):
    """Represent a single quota override for a tenant.
 
    If there is no row for a given tenant id and resource, then the
    default for the quota class is used.
    """
    tenant_id = sa.Column(sa.String(255), index=True)
    resource = sa.Column(sa.String(255))
    limit = sa.Column(sa.Integer)
 
'''
quota配额的具体实现，根据数据库的配置内容，实现quota的控制，即quota的增删改查方法
'''
class DbQuotaDriver(object):
    """Driver to perform necessary checks to enforce quotas and obtain quota
    information.
 
    The default driver utilizes the local database.
    """
 
    '''
    得到租户tenant的quota，执行neutron quota-show --tenant-id uuid时调用的方法
    '''
    @staticmethod
    def get_tenant_quotas(context, resources, tenant_id):
        """Given a list of resources, retrieve the quotas for the given
        tenant.
 
        :param context: The request context, for access checks.
        :param resources: A dictionary of the registered resource keys.
        :param tenant_id: The ID of the tenant to return quotas for.
        :return dict: from resource name to dict of name and limit
        """
 
        # init with defaults    得到quota默认的配额项item，即所谓的network，subnet，port和router等，以及对应的值
        tenant_quota = dict((key, resource.default)
                            for key, resource in resources.items())
 
        # update with tenant specific limits    从数据库中获取最新的quota配置信息，并更新
        q_qry = context.session.query(Quota).filter_by(tenant_id=tenant_id)
        tenant_quota.update((q['resource'], q['limit']) for q in q_qry)
 
        return tenant_quota
 
    '''
    quota的删除，即执行neutron quota-delete 的方法，删除之后，tenant将会集成默认的的quota配置
    '''
    @staticmethod
    def delete_tenant_quota(context, tenant_id):
        """Delete the quota entries for a given tenant_id.
 
        Atfer deletion, this tenant will use default quota values in conf.
        """
        #从neutron。quotas数据库中查询到所有的quota配置之后，过略某个具体的tenant的quota，之后执行delete()方法将其删除
        with context.session.begin():
            tenant_quotas = context.session.query(Quota)
            tenant_quotas = tenant_quotas.filter_by(tenant_id=tenant_id)
            tenant_quotas.delete()
 
    '''
    得到所有租户tenant的配额资源，即执行neutron quota-list所查看的内容
    '''
    @staticmethod
    def get_all_quotas(context, resources):
        """Given a list of resources, retrieve the quotas for the all tenants.
 
        :param context: The request context, for access checks.
        :param resources: A dictionary of the registered resource keys.
        :return quotas: list of dict of tenant_id:, resourcekey1:
        resourcekey2: ...
        """
        tenant_default = dict((key, resource.default)
                              for key, resource in resources.items())
 
        all_tenant_quotas = {}
 
        for quota in context.session.query(Quota):
            tenant_id = quota['tenant_id']
 
            # avoid setdefault() because only want to copy when actually req'd
            #如果quotas表中，没有找到配置选项，说明使用默认的quota配置，直接用默认的copy过来即可，有配置则继承quotas表中的配置
            tenant_quota = all_tenant_quotas.get(tenant_id)
            if tenant_quota is None:
                tenant_quota = tenant_default.copy()
                tenant_quota['tenant_id'] = tenant_id
                all_tenant_quotas[tenant_id] = tenant_quota
 
            tenant_quota[quota['resource']] = quota['limit']
 
        return all_tenant_quotas.values()
 
    '''
                更新quota的配置，即执行neutron quota-update命令的具体实现
    '''
    @staticmethod
    def update_quota_limit(context, tenant_id, resource, limit):
        with context.session.begin():
            tenant_quota = context.session.query(Quota).filter_by(
                tenant_id=tenant_id, resource=resource).first()
 
            #有配置内容，则更新，没有则根据资源的配置内容，在数据库中添加对应的条目
            if tenant_quota:
                tenant_quota.update({'limit': limit})
            else:
                tenant_quota = Quota(tenant_id=tenant_id,
                                     resource=resource,
                                     limit=limit)
                context.session.add(tenant_quota)
 
    def _get_quotas(self, context, tenant_id, resources, keys):
        """Retrieves the quotas for specific resources.
 
        A helper method which retrieves the quotas for the specific
        resources identified by keys, and which apply to the current
        context.
 
        :param context: The request context, for access checks.
        :param tenant_id: the tenant_id to check quota.
        :param resources: A dictionary of the registered resources.
        :param keys: A list of the desired quotas to retrieve.
 
        """
        desired = set(keys)
        sub_resources = dict((k, v) for k, v in resources.items()
                             if k in desired)
 
        # Make sure we accounted for all of them...
        if len(keys) != len(sub_resources):
            unknown = desired - set(sub_resources.keys())
            raise exceptions.QuotaResourceUnknown(unknown=sorted(unknown))
 
        # Grab and return the quotas (without usages)
        quotas = DbQuotaDriver.get_tenant_quotas(
            context, sub_resources, tenant_id)
 
        return dict((k, v) for k, v in quotas.items())
 
    '''
    neutron quota的校验，即在执行过程中，调用该方法，确认tenant的quota是否在合理的范围内
    '''
    def limit_check(self, context, tenant_id, resources, values):
        """Check simple quota limits.
 
        For limits--those quotas for which there is no usage
        synchronization function--this method checks that a set of
        proposed values are permitted by the limit restriction.
 
        This method will raise a QuotaResourceUnknown exception if a
        given resource is unknown or if it is not a simple limit
        resource.
 
        If any of the proposed values is over the defined quota, an
        OverQuota exception will be raised with the sorted list of the
        resources which are too high.  Otherwise, the method returns
        nothing.
 
        :param context: The request context, for access checks.
        :param tenant_id: The tenant_id to check the quota.
        :param resources: A dictionary of the registered resources.
        :param values: A dictionary of the values to check against the
                       quota.
        """
 
        # Ensure no value is less than zero    quota的配置值不能为负数
        unders = [key for key, val in values.items() if val < 0]
        if unders:
            raise exceptions.InvalidQuotaValue(unders=sorted(unders))
 
        # Get the applicable quotas
        quotas = self._get_quotas(context, tenant_id, resources, values.keys())
 
        # Check the quotas and construct a list of the resources that
        # would be put over limit by the desired values
        overs = [key for key, val in values.items()
                 if quotas[key] >= 0 and quotas[key] < val]
        if overs:
            raise exceptions.OverQuota(overs=sorted(overs))

openstack运维实战系列(五)之nova quota调整

1. 前言
    安装完openstack之后，为了对资源的限制，openstack内置了几种配额机制:nova计算资源的配额，cinder存储资源的配额，neutron网络资源的配额，防止资源的分过分配，默认的quota配置很低，比如nova默认只允许建立10个instance。未能能够正常使用openstack系统资源，需要调整quota的配置。本文主要讲述nova的配额修改，关于cinder和neutron的配额修改，请参考后续的的博文。
2. nova默认的配额
    nova默认的配额定义在/etc/nova/nova.conf中，初始用户创建之后，会集成该配置中的配额选项，nova的配额条目定义内容如下：

[root@controller ~]# vim /etc/nova/nova.conf 
quota_driver=nova.quota.DbQuotaDriver            nova配额使用的驱动，参考里面可以查看到配额源码的实现
max_age=0                                        利用率刷新的时间间隔
quota_instances=10                               instance个数
quota_cores=20                                   vcpus的个数
quota_ram=5120000                                内存显示，单位是MB 
quota_floating_ips=10                            floating-ip的个数
quota_fixed_ips=-1                               fixed-ip的个数
quota_metadata_items=128                         metadata的个数
quota_injected_files=10                          注入文件个数
quota_injected_file_content_bytes=10240          每个注入文件的大小
quota_injected_file_path_bytes=255               注入文件路径长度
quota_security_groups=10                         安全组的个数
quota_security_group_rules=20                    每个安全组中的规则
quota_key_pairs=100                              keys的个数
 
查看默认的quota:
[root@controller ~]# nova quota-defaults --tenant compayA    #最好用uuid的方式表示
+-----------------------------+---------+
| Quota                       | Limit   |
+-----------------------------+---------+
| instances                   | 10      |
| cores                       | 20      |
| ram                         | 1572864 |
| floating_ips                | 10      |
| fixed_ips                   | -1      |
| metadata_items              | 128     |
| injected_files              | 10      |
| injected_file_content_bytes | 10240   |
| injected_file_path_bytes    | 255     |
| key_pairs                   | 100     |
| security_groups             | 10      |
| security_group_rules        | 20      |
+-----------------------------+---------+
3. 修改nova的配额

1. 获取tenant的uuid号码
[root@controller ~]# keystone tenant-list
+----------------------------------+----------+---------+
|                id                |   name   | enabled |
+----------------------------------+----------+---------+
| 842ab3268a2c47e6a4b0d8774de805ae |  admin   |   True  |
| 7ff1dfb5a6f349958c3a949248e56236 | companyA |   True  |        #uuid号码
| 10d1465c00d049fab88dec1af0f56b1b |   demo   |   True  |
| 3b57a14f7c354a979c9f62b60f31a331 | service  |   True  |
+----------------------------------+----------+---------+
 
2. 修改nova的配额
[root@controller ~]# nova quota-update --instances 50  --cores 200 --ram 204800 --floating-ips 50  --fixed-ips -1 --metadata-items 256  --injected-files 2 --key-pairs 10 --security-groups 10 --security-group-rules 20  7ff1dfb5a6f349958c3a949248e56236     #没有设置的内容，将会从default中继承
 
3. 校验nova的配额
[root@controller ~]# nova quota-show --tenant  7ff1dfb5a6f349958c3a949248e56236 
+-----------------------------+--------+
| Quota                       | Limit  |
+-----------------------------+--------+
| instances                   | 50     |            #instance
| cores                       | 200    |            #vcpus
| ram                         | 204800 |            #memroy
| floating_ips                | 50     |            #floating-ip
| fixed_ips                   | -1     |        
| metadata_items              | 256    |
| injected_files              | 2      |
| injected_file_content_bytes | 10240  |
| injected_file_path_bytes    | 255    |
| key_pairs                   | 10     |
| security_groups             | 10     |
| security_group_rules        | 20     |
+-----------------------------+--------+
 
@@@修改完毕@@@@

4. 总结
    nova的配额在使用过程中，当quota达到限制之后，将无法建立虚拟机，关于报错信息，可以在nova的日志/var/log/nova/nova-api.log中查看到，具体不赘述，在运维的过程中，查看下日志即可获知是磁盘配额导致，修改tenant的配额即可。关于cinder和neutron的配额，请参考后续的博客。

5. 附录
    nova关于quota的代码实现，仅供参考

[root@controller ~]# vim /usr/lib/python2.6/site-packages/nova/quota.py
"""Quotas for instances, and floating ips."""
 
import datetime
 
from oslo.config import cfg
import six
 
from nova import db
from nova import exception
from nova.objects import keypair as keypair_obj
from nova.openstack.common.gettextutils import _
from nova.openstack.common import importutils
from nova.openstack.common import log as logging
from nova.openstack.common import timeutils
 
LOG = logging.getLogger(__name__)
 
'''
定义配置文件的内容，即关于quota的资源配置，配置的关键字和对应的值，包括instance个数，vcpus，内存，floating-ip和fixed-ip等
'''
quota_opts = [
    cfg.IntOpt('quota_instances',
               default=10,
               help='Number of instances allowed per project'),
    cfg.IntOpt('quota_cores',
               default=20,
               help='Number of instance cores allowed per project'),
    cfg.IntOpt('quota_ram',
               default=50 * 1024,
               help='Megabytes of instance RAM allowed per project'),
    cfg.IntOpt('quota_floating_ips',
               default=10,
               help='Number of floating IPs allowed per project'),
    cfg.IntOpt('quota_fixed_ips',
               default=-1,
               help=('Number of fixed IPs allowed per project (this should be '
                     'at least the number of instances allowed)')),
    cfg.IntOpt('quota_metadata_items',
               default=128,
               help='Number of metadata items allowed per instance'),
    cfg.IntOpt('quota_injected_files',
               default=5,
               help='Number of injected files allowed'),
    cfg.IntOpt('quota_injected_file_content_bytes',
               default=10 * 1024,
               help='Number of bytes allowed per injected file'),
    cfg.IntOpt('quota_injected_file_path_bytes',
               default=255,
               help='Number of bytes allowed per injected file path'),
    cfg.IntOpt('quota_security_groups',
               default=10,
               help='Number of security groups per project'),
    cfg.IntOpt('quota_security_group_rules',
               default=20,
               help='Number of security rules per security group'),
    cfg.IntOpt('quota_key_pairs',
               default=100,
               help='Number of key pairs per user'),
    cfg.IntOpt('reservation_expire',
               default=86400,
               help='Number of seconds until a reservation expires'),
    cfg.IntOpt('until_refresh',
               default=0,
               help='Count of reservations until usage is refreshed'),
    cfg.IntOpt('max_age',
               default=0,
               help='Number of seconds between subsequent usage refreshes'),
    cfg.StrOpt('quota_driver',
               default='nova.quota.DbQuotaDriver',
               help='Default driver to use for quota checks'),
    ]
 
CONF = cfg.CONF
CONF.register_opts(quota_opts)
 
'''
nova quota处理相关的驱动管理类，包含了quota的增删改查相关的操作，都封装在该类里面
'''
class DbQuotaDriver(object):
    """Driver to perform necessary checks to enforce quotas and obtain
    quota information.  The default driver utilizes the local
    database.
    """
    #获取enant中user的quota配额信息，即nova quota-show [--tenant <tenant-id>] [--user <user-id>]
    def get_by_project_and_user(self, context, project_id, user_id, resource):
        """Get a specific quota by project and user."""
 
        return db.quota_get(context, project_id, resource, user_id=user_id)     #调用数据库，返回用户的配额信息
 
    #获取tenant的quota配置，即nova quota-show携带tenant的id号码，和上面相比，不懈怠用户的uuid号码
    def get_by_project(self, context, project_id, resource):
        """Get a specific quota by project."""
 
        return db.quota_get(context, project_id, resource)      #调用数据库，获取quota的配置
 
    def get_by_class(self, context, quota_class, resource):
        """Get a specific quota by quota class."""
 
        return db.quota_class_get(context, quota_class, resource)
 
    '''
                得到quota的默认配置
    '''
    def get_defaults(self, context, resources):
        """Given a list of resources, retrieve the default quotas.
        Use the class quotas named `_DEFAULT_QUOTA_NAME` as default quotas,
        if it exists.
 
        :param context: The request context, for access checks.
        :param resources: A dictionary of the registered resources.
        """
 
        quotas = {}
        default_quotas = db.quota_class_get_default(context)                #得到quota默认的配置
        for resource in resources.values():
            quotas[resource.name] = default_quotas.get(resource.name,
                                                       resource.default)
 
        return quotas
 
    def get_class_quotas(self, context, resources, quota_class,
                         defaults=True):
        """Given a list of resources, retrieve the quotas for the given
        quota class.
 
        :param context: The request context, for access checks.
        :param resources: A dictionary of the registered resources.
        :param quota_class: The name of the quota class to return
                            quotas for.
        :param defaults: If True, the default value will be reported
                         if there is no specific value for the
                         resource.
        """
 
        quotas = {}
        class_quotas = db.quota_class_get_all_by_name(context, quota_class)
        for resource in resources.values():
            if defaults or resource.name in class_quotas:
                quotas[resource.name] = class_quotas.get(resource.name,
                                                         resource.default)
 
        return quotas
 
    def _process_quotas(self, context, resources, project_id, quotas,
                        quota_class=None, defaults=True, usages=None,
                        remains=False):
        modified_quotas = {}
        # Get the quotas for the appropriate class.  If the project ID
        # matches the one in the context, we use the quota_class from
        # the context, otherwise, we use the provided quota_class (if
        # any)
        if project_id == context.project_id:
            quota_class = context.quota_class
        if quota_class:
            class_quotas = db.quota_class_get_all_by_name(context, quota_class)
        else:
            class_quotas = {}
 
        default_quotas = self.get_defaults(context, resources)
 
        for resource in resources.values():
            # Omit default/quota class values
            if not defaults and resource.name not in quotas:
                continue
 
            limit = quotas.get(resource.name, class_quotas.get(
                        resource.name, default_quotas[resource.name]))
            modified_quotas[resource.name] = dict(limit=limit)
 
            # Include usages if desired.  This is optional because one
            # internal consumer of this interface wants to access the
            # usages directly from inside a transaction.
            if usages:
                usage = usages.get(resource.name, {})
                modified_quotas[resource.name].update(
                    in_use=usage.get('in_use', 0),
                    reserved=usage.get('reserved', 0),
                    )
            # Initialize remains quotas.
            if remains:
                modified_quotas[resource.name].update(remains=limit)
 
        if remains:
            all_quotas = db.quota_get_all(context, project_id)
            for quota in all_quotas:
                if quota.resource in modified_quotas:
                    modified_quotas[quota.resource]['remains'] -= \
                            quota.hard_limit
 
        return modified_quotas
 
    def get_user_quotas(self, context, resources, project_id, user_id,
                        quota_class=None, defaults=True,
                        usages=True, project_quotas=None,
                        user_quotas=None):
        """Given a list of resources, retrieve the quotas for the given
        user and project.
 
        :param context: The request context, for access checks.
        :param resources: A dictionary of the registered resources.
        :param project_id: The ID of the project to return quotas for.
        :param user_id: The ID of the user to return quotas for.
        :param quota_class: If project_id != context.project_id, the
                            quota class cannot be determined.  This
                            parameter allows it to be specified.  It
                            will be ignored if project_id ==
                            context.project_id.
        :param defaults: If True, the quota class value (or the
                         default value, if there is no value from the
                         quota class) will be reported if there is no
                         specific value for the resource.
        :param usages: If True, the current in_use and reserved counts
                       will also be returned.
        :param project_quotas: Quotas dictionary for the specified project.
        :param user_quotas: Quotas dictionary for the specified project
                            and user.
        """
        user_quotas = user_quotas or db.quota_get_all_by_project_and_user(
            context, project_id, user_id)
        # Use the project quota for default user quota.
        proj_quotas = project_quotas or db.quota_get_all_by_project(
            context, project_id)
        for key, value in proj_quotas.iteritems():
            if key not in user_quotas.keys():
                user_quotas[key] = value
        user_usages = None
        if usages:
            user_usages = db.quota_usage_get_all_by_project_and_user(context,
                                                         project_id,
                                                         user_id)
        return self._process_quotas(context, resources, project_id,
                                    user_quotas, quota_class,
                                    defaults=defaults, usages=user_usages)
 
    def get_project_quotas(self, context, resources, project_id,
                           quota_class=None, defaults=True,
                           usages=True, remains=False, project_quotas=None):
        """Given a list of resources, retrieve the quotas for the given
        project.
 
        :param context: The request context, for access checks.
        :param resources: A dictionary of the registered resources.
        :param project_id: The ID of the project to return quotas for.
        :param quota_class: If project_id != context.project_id, the
                            quota class cannot be determined.  This
                            parameter allows it to be specified.  It
                            will be ignored if project_id ==
                            context.project_id.
        :param defaults: If True, the quota class value (or the
                         default value, if there is no value from the
                         quota class) will be reported if there is no
                         specific value for the resource.
        :param usages: If True, the current in_use and reserved counts
                       will also be returned.
        :param remains: If True, the current remains of the project will
                        will be returned.
        :param project_quotas: Quotas dictionary for the specified project.
        """
        project_quotas = project_quotas or db.quota_get_all_by_project(
            context, project_id)
        project_usages = None
        if usages:
            project_usages = db.quota_usage_get_all_by_project(context,
                                                               project_id)
        return self._process_quotas(context, resources, project_id,
                                    project_quotas, quota_class,
                                    defaults=defaults, usages=project_usages,
                                    remains=remains)
 
    def get_settable_quotas(self, context, resources, project_id,
                            user_id=None):
        """Given a list of resources, retrieve the range of settable quotas for
        the given user or project.
 
        :param context: The request context, for access checks.
        :param resources: A dictionary of the registered resources.
        :param project_id: The ID of the project to return quotas for.
        :param user_id: The ID of the user to return quotas for.
        """
        settable_quotas = {}
        db_proj_quotas = db.quota_get_all_by_project(context, project_id)
        project_quotas = self.get_project_quotas(context, resources,
                                                 project_id, remains=True,
                                                 project_quotas=db_proj_quotas)
        if user_id:
            setted_quotas = db.quota_get_all_by_project_and_user(context,
                                                     project_id,
                                                     user_id)
            user_quotas = self.get_user_quotas(context, resources,
                                               project_id, user_id,
                                               project_quotas=db_proj_quotas,
                                               user_quotas=setted_quotas)
            for key, value in user_quotas.items():
                maximum = project_quotas[key]['remains'] +\
                        setted_quotas.get(key, 0)
                settable_quotas[key] = dict(
                        minimum=value['in_use'] + value['reserved'],
                        maximum=maximum
                        )
        else:
            for key, value in project_quotas.items():
                minimum = max(int(value['limit'] - value['remains']),
                              int(value['in_use'] + value['reserved']))
                settable_quotas[key] = dict(minimum=minimum, maximum=-1)
        return settable_quotas
 
    def _get_quotas(self, context, resources, keys, has_sync, project_id=None,
                    user_id=None, project_quotas=None):
        """A helper method which retrieves the quotas for the specific
        resources identified by keys, and which apply to the current
        context.
 
        :param context: The request context, for access checks.
        :param resources: A dictionary of the registered resources.
        :param keys: A list of the desired quotas to retrieve.
        :param has_sync: If True, indicates that the resource must
                         have a sync function; if False, indicates
                         that the resource must NOT have a sync
                         function.
        :param project_id: Specify the project_id if current context
                           is admin and admin wants to impact on
                           common user's tenant.
        :param user_id: Specify the user_id if current context
                        is admin and admin wants to impact on
                        common user.
        :param project_quotas: Quotas dictionary for the specified project.
        """
 
        # Filter resources
        if has_sync:
            sync_filt = lambda x: hasattr(x, 'sync')
        else:
            sync_filt = lambda x: not hasattr(x, 'sync')
        desired = set(keys)
        sub_resources = dict((k, v) for k, v in resources.items()
                             if k in desired and sync_filt(v))
 
        # Make sure we accounted for all of them...
        if len(keys) != len(sub_resources):
            unknown = desired - set(sub_resources.keys())
            raise exception.QuotaResourceUnknown(unknown=sorted(unknown))
 
        if user_id:
            # Grab and return the quotas (without usages)
            quotas = self.get_user_quotas(context, sub_resources,
                                          project_id, user_id,
                                          context.quota_class, usages=False,
                                          project_quotas=project_quotas)
        else:
            # Grab and return the quotas (without usages)
            quotas = self.get_project_quotas(context, sub_resources,
                                             project_id,
                                             context.quota_class,
                                             usages=False,
                                             project_quotas=project_quotas)
 
        return dict((k, v['limit']) for k, v in quotas.items())
 
    def limit_check(self, context, resources, values, project_id=None,
                    user_id=None):
        """Check simple quota limits.
 
        For limits--those quotas for which there is no usage
        synchronization function--this method checks that a set of
        proposed values are permitted by the limit restriction.
 
        This method will raise a QuotaResourceUnknown exception if a
        given resource is unknown or if it is not a simple limit
        resource.
 
        If any of the proposed values is over the defined quota, an
        OverQuota exception will be raised with the sorted list of the
        resources which are too high.  Otherwise, the method returns
        nothing.
 
        :param context: The request context, for access checks.
        :param resources: A dictionary of the registered resources.
        :param values: A dictionary of the values to check against the
                       quota.
        :param project_id: Specify the project_id if current context
                           is admin and admin wants to impact on
                           common user's tenant.
        :param user_id: Specify the user_id if current context
                        is admin and admin wants to impact on
                        common user.
        """
 
        # Ensure no value is less than zero
        unders = [key for key, val in values.items() if val < 0]
        if unders:
            raise exception.InvalidQuotaValue(unders=sorted(unders))
 
        # If project_id is None, then we use the project_id in context
        if project_id is None:
            project_id = context.project_id
        # If user id is None, then we use the user_id in context
        if user_id is None:
            user_id = context.user_id
 
        # Get the applicable quotas
        project_quotas = db.quota_get_all_by_project(context, project_id)
        quotas = self._get_quotas(context, resources, values.keys(),
                                  has_sync=False, project_id=project_id,
                                  project_quotas=project_quotas)
        user_quotas = self._get_quotas(context, resources, values.keys(),
                                       has_sync=False, project_id=project_id,
                                       user_id=user_id,
                                       project_quotas=project_quotas)
 
        # Check the quotas and construct a list of the resources that
        # would be put over limit by the desired values
        overs = [key for key, val in values.items()
                 if quotas[key] >= 0 and quotas[key] < val or
                 (user_quotas[key] >= 0 and user_quotas[key] < val)]
        if overs:
            headroom = {}
            # Check project_quotas:
            for key in quotas:
                if quotas[key] >= 0 and quotas[key] < val:
                    headroom[key] = quotas[key]
            # Check user quotas:
            for key in user_quotas:
                if (user_quotas[key] >= 0 and user_quotas[key] < val and
                        headroom.get(key) > user_quotas[key]):
                    headroom[key] = user_quotas[key]
 
            raise exception.OverQuota(overs=sorted(overs), quotas=quotas,
                                      usages={}, headroom=headroom)
 
    def reserve(self, context, resources, deltas, expire=None,
                project_id=None, user_id=None):
        """Check quotas and reserve resources.
 
        For counting quotas--those quotas for which there is a usage
        synchronization function--this method checks quotas against
        current usage and the desired deltas.
 
        This method will raise a QuotaResourceUnknown exception if a
        given resource is unknown or if it does not have a usage
        synchronization function.
 
        If any of the proposed values is over the defined quota, an
        OverQuota exception will be raised with the sorted list of the
        resources which are too high.  Otherwise, the method returns a
        list of reservation UUIDs which were created.
 
        :param context: The request context, for access checks.
        :param resources: A dictionary of the registered resources.
        :param deltas: A dictionary of the proposed delta changes.
        :param expire: An optional parameter specifying an expiration
                       time for the reservations.  If it is a simple
                       number, it is interpreted as a number of
                       seconds and added to the current time; if it is
                       a datetime.timedelta object, it will also be
                       added to the current time.  A datetime.datetime
                       object will be interpreted as the absolute
                       expiration time.  If None is specified, the
                       default expiration time set by
                       --default-reservation-expire will be used (this
                       value will be treated as a number of seconds).
        :param project_id: Specify the project_id if current context
                           is admin and admin wants to impact on
                           common user's tenant.
        :param user_id: Specify the user_id if current context
                        is admin and admin wants to impact on
                        common user.
        """
 
        # Set up the reservation expiration
        if expire is None:
            expire = CONF.reservation_expire
        if isinstance(expire, (int, long)):
            expire = datetime.timedelta(seconds=expire)
        if isinstance(expire, datetime.timedelta):
            expire = timeutils.utcnow() + expire
        if not isinstance(expire, datetime.datetime):
            raise exception.InvalidReservationExpiration(expire=expire)
 
        # If project_id is None, then we use the project_id in context
        if project_id is None:
            project_id = context.project_id
        # If user_id is None, then we use the project_id in context
        if user_id is None:
            user_id = context.user_id
 
        # Get the applicable quotas.
        # NOTE(Vek): We're not worried about races at this point.
        #            Yes, the admin may be in the process of reducing
        #            quotas, but that's a pretty rare thing.
        project_quotas = db.quota_get_all_by_project(context, project_id)
        quotas = self._get_quotas(context, resources, deltas.keys(),
                                  has_sync=True, project_id=project_id,
                                  project_quotas=project_quotas)
        user_quotas = self._get_quotas(context, resources, deltas.keys(),
                                       has_sync=True, project_id=project_id,
                                       user_id=user_id,
                                       project_quotas=project_quotas)
 
        # NOTE(Vek): Most of the work here has to be done in the DB
        #            API, because we have to do it in a transaction,
        #            which means access to the session.  Since the
        #            session isn't available outside the DBAPI, we
        #            have to do the work there.
        return db.quota_reserve(context, resources, quotas, user_quotas,
                                deltas, expire,
                                CONF.until_refresh, CONF.max_age,
                                project_id=project_id, user_id=user_id)
 
    def commit(self, context, reservations, project_id=None, user_id=None):
        """Commit reservations.
 
        :param context: The request context, for access checks.
        :param reservations: A list of the reservation UUIDs, as
                             returned by the reserve() method.
        :param project_id: Specify the project_id if current context
                           is admin and admin wants to impact on
                           common user's tenant.
        :param user_id: Specify the user_id if current context
                        is admin and admin wants to impact on
                        common user.
        """
        # If project_id is None, then we use the project_id in context
        if project_id is None:
            project_id = context.project_id
        # If user_id is None, then we use the user_id in context
        if user_id is None:
            user_id = context.user_id
 
        db.reservation_commit(context, reservations, project_id=project_id,
                              user_id=user_id)
 
    def rollback(self, context, reservations, project_id=None, user_id=None):
        """Roll back reservations.
 
        :param context: The request context, for access checks.
        :param reservations: A list of the reservation UUIDs, as
                             returned by the reserve() method.
        :param project_id: Specify the project_id if current context
                           is admin and admin wants to impact on
                           common user's tenant.
        :param user_id: Specify the user_id if current context
                        is admin and admin wants to impact on
                        common user.
        """
        # If project_id is None, then we use the project_id in context
        if project_id is None:
            project_id = context.project_id
        # If user_id is None, then we use the user_id in context
        if user_id is None:
            user_id = context.user_id
 
        db.reservation_rollback(context, reservations, project_id=project_id,
                                user_id=user_id)
 
    def usage_reset(self, context, resources):
        """Reset the usage records for a particular user on a list of
        resources.  This will force that user's usage records to be
        refreshed the next time a reservation is made.
 
        Note: this does not affect the currently outstanding
        reservations the user has; those reservations must be
        committed or rolled back (or expired).
 
        :param context: The request context, for access checks.
        :param resources: A list of the resource names for which the
                          usage must be reset.
        """
 
        # We need an elevated context for the calls to
        # quota_usage_update()
        elevated = context.elevated()
 
        for resource in resources:
            try:
                # Reset the usage to -1, which will force it to be
                # refreshed
                db.quota_usage_update(elevated, context.project_id,
                                      context.user_id,
                                      resource, in_use=-1)
            except exception.QuotaUsageNotFound:
                # That means it'll be refreshed anyway
                pass
 
    def destroy_all_by_project_and_user(self, context, project_id, user_id):
        """Destroy all quotas, usages, and reservations associated with a
        project and user.
 
        :param context: The request context, for access checks.
        :param project_id: The ID of the project being deleted.
        :param user_id: The ID of the user being deleted.
        """
 
        db.quota_destroy_all_by_project_and_user(context, project_id, user_id)
 
    def destroy_all_by_project(self, context, project_id):
        """Destroy all quotas, usages, and reservations associated with a
        project.
 
        :param context: The request context, for access checks.
        :param project_id: The ID of the project being deleted.
        """
 
        db.quota_destroy_all_by_project(context, project_id)
 
    def expire(self, context):
        """Expire reservations.
 
        Explores all currently existing reservations and rolls back
        any that have expired.
 
        :param context: The request context, for access checks.
        """
 
        db.reservation_expire(context)

openstack运维实战系列(四)之删除computes节点

前言
    因为虚拟机业务的撤销，compute上面的instance越来越少，随着虚拟机的迁移和下线，compute节点可能处于资源空闲的状态，为了进一步利用系统资源，节约成本，需要将空闲的计算节点从openstack中删除。在删除compute节点之前，首先需要确保该计算节点上没有业务，可以通过virsh list --all查看是否还有虚拟机业务，同时保守起见，建议将compute节点的服务关闭关闭，通过service openstack-nova-compute stop && chkconfig openstack-nova-compute off。
2. 删除nova计算节点
    openstack建议在删除nova节点的时候，建议通过操作nova.instances中的deleted字段来实现，而不是直接将compute节点的条目从数据库中删除，一来是为了安全考虑，而是能够实现快速的回滚操作，具体的操作如下：
校验nova-compute的状态

[root@controller ~]# nova service-list
+------------------+-----------------------+---------------+---------+-------+----------------------------+-----------------+
| Binary           | Host                  | Zone          | Status  | State | Updated_at                 | Disabled Reason |
+------------------+-----------------------+---------------+---------+-------+----------------------------+-----------------+
| nova-scheduler   | controller            | internal      | enabled | up    | 2016-01-20T04:12:33.000000 | -               |
| nova-cert        | controller            | internal      | enabled | up    | 2016-01-20T04:12:31.000000 | -               |
| nova-conductor   | controller            | internal      | enabled | up    | 2016-01-20T04:12:27.000000 | -               |
| nova-consoleauth | controller            | internal      | enabled | up    | 2016-01-20T04:12:33.000000 | -               |
| nova-compute     | YiZhuang_10_1_112_105 | YiZhuangZone2 | enabled | down  | 2016-01-16T09:11:57.000000 | -               |    #已经处于down的状态
| nova-compute     | YiZhuang_10_1_112_106 | YiZhuangZone2 | enabled | up    | 2016-01-20T04:12:24.000000 | -               |
2. 删除compute service节点

mysql> select * from nova.services where host='YiZhuang_10_1_112_105';  
+---------------------+---------------------+------------+----+-----------------------+--------------+---------+--------------+----------+---------+-----------------+
| created_at          | updated_at          | deleted_at | id | host                  | binary       | topic   | report_count | disabled | deleted | disabled_reason |
+---------------------+---------------------+------------+----+-----------------------+--------------+---------+--------------+----------+---------+-----------------+
| 2015-09-21 03:21:40 | 2016-01-16 09:11:57 | NULL       | 37 | YiZhuang_10_1_112_105 | nova-compute | compute |      1011705 |        0 |       0 | NULL            |
+---------------------+---------------------+------------+----+-----------------------+--------------+---------+--------------+----------+---------+-----------------+
1 row in set (0.00 sec)
mysql> update nova.services set deleted=1 where host='YiZhuang_10_1_112_105';             #设置deleted字段，即标志位            
Query OK, 1 row affected (0.00 sec)
Rows matched: 1  Changed: 1  Warnings: 0
mysql> select * from nova.services where host='YiZhuang_10_1_112_105';                   #检查deleted字段设置情况
+---------------------+---------------------+------------+----+-----------------------+--------------+---------+--------------+----------+---------+-----------------+
| created_at          | updated_at          | deleted_at | id | host                  | binary       | topic   | report_count | disabled | deleted | disabled_reason |
+---------------------+---------------------+------------+----+-----------------------+--------------+---------+--------------+----------+---------+-----------------+
| 2015-09-21 03:21:40 | 2016-01-16 09:11:57 | NULL       | 37 | YiZhuang_10_1_112_105 | nova-compute | compute |      1011705 |        0 |       1 | NULL            |   
+---------------------+---------------------+------------+----+-----------------------+--------------+---------+--------------+----------+---------+-----------------+
1 row in set (0.00 sec)
3. 确认节点的service状态

[root@controller ~]# nova service-list
+------------------+-----------------------+---------------+---------+-------+----------------------------+-----------------+
| Binary           | Host                  | Zone          | Status  | State | Updated_at                 | Disabled Reason |
+------------------+-----------------------+---------------+---------+-------+----------------------------+-----------------+
| nova-scheduler   | controller            | internal      | enabled | up    | 2016-01-20T04:12:33.000000 | -               |
| nova-cert        | controller            | internal      | enabled | up    | 2016-01-20T04:12:31.000000 | -               |
| nova-conductor   | controller            | internal      | enabled | up    | 2016-01-20T04:12:27.000000 | -               |
| nova-consoleauth | controller            | internal      | enabled | up    | 2016-01-20T04:12:33.000000 | -               |
| nova-compute     | YiZhuang_10_1_112_106 | YiZhuangZone2 | enabled | up    | 2016-01-20T04:12:24.000000 | -               |    #service已经删除
4. 查看hypervisor情况

[root@controller ~]# nova hypervisor-list
+----+-----------------------+
| ID | Hypervisor hostname   |
+----+-----------------------+
| 25 | YiZhuang_10_1_112_105 |          #services已经删除，但hypervisor依旧在，类似的方法，将nova.compute_nodes中的deleted字段修改，查看步骤5    
| 27 | YiZhuang_10_1_112_106 |
5. 删除compute_nodes节点

mysql> update nova.compute_nodes set deleted=1 where hypervisor_hostname='YiZhuang_10_1_112_105'\G;       #设置deleted标志位           
Query OK, 1 row affected (0.01 sec)
Rows matched: 1  Changed: 1  Warnings: 0
 
mysql> select * from nova.compute_nodes where hypervisor_hostname='YiZhuang_10_1_112_105'\G;               #确认设置情况    
*************************** 1. row ***************************
          created_at: 2015-09-21 03:21:40
          updated_at: 2016-01-16 09:11:36
          deleted_at: NULL
                  id: 25
          service_id: 37
               vcpus: 24
           memory_mb: 64396
            local_gb: 3062
          vcpus_used: 0
      memory_mb_used: 1024
       local_gb_used: 20
     hypervisor_type: QEMU
  hypervisor_version: 12001
            cpu_info: {"vendor": "Intel", "model": "SandyBridge", "arch": "x86_64", "features": ["vme", "dtes64", "vmx", "erms", "xtpr", "smep", "pcid", "est", "monitor", "smx", "tm", "acpi", "osxsave", "ht", "dca", "pdcm", "pdpe1gb", "fsgsbase", "f16c", "ds", "tm2", "ss", "pbe", "ds_cpl", "rdrand"], "topology": {"cores": 6, "threads": 2, "sockets": 1}}
disk_available_least: 2671
         free_ram_mb: 63372
        free_disk_gb: 3042
    current_workload: 0
         running_vms: 0
 hypervisor_hostname: YiZhuang_10_1_112_105
             deleted: 1                        #设置成功
             host_ip: 0.0.0.0
 supported_instances: [["i686", "qemu", "hvm"], ["i686", "kvm", "hvm"], ["x86_64", "qemu", "hvm"], ["x86_64", "kvm", "hvm"]]
           pci_stats: []
             metrics: []
     extra_resources: NULL
               stats: {"num_task_None": 2, "io_workload": 0, "num_instances": 2, "num_vm_stopped": 2, "num_proj_a49b16d5324a4d20bde2217b17200485": 2, "num_vcpus_used": 8, "num_os_type_None": 2}
1 row in set (0.00 sec)
6. 确认hypervisor node是否删除

[root@controller ~]# nova hypervisor-list        #YiZhuang_10_1_112_105已被删除
+----+-----------------------+
| ID | Hypervisor hostname   |
+----+-----------------------+  
| 27 | YiZhuang_10_1_112_106 |
3. 删除neutron agent服务
    以上将nova的service和hypervisor从数据库中删除，但是compute节点在安装过程中，neutron-openvswitch-agent会建立一个OVS的agent，该agent也记录在数据库中，通过neutron agent-list可以查看到agent的情况，当compute node删除之后，neutron也应该删除，方法和上面相类似，具体如下：
确认agent的情况

[root@controller ~]# neutron agent-list
+--------------------------------------+--------------------+-----------------------+-------+----------------+
| id                                   | agent_type         | host                  | alive | admin_state_up |
+--------------------------------------+--------------------+-----------------------+-------+----------------+
| 0160e0b7-22fb-42d9-8e43-0d13d98db594 | L3 agent           | LuGu_10_1_81_209      | :-)   | True           |
| 4e1c9957-98e5-4516-af2d-6c67a00ecd77 | Open vSwitch agent | YiZhuang_10_1_112_105 | xxx   | True           |    #计算节点已经关闭，所以agent状态变为xxx，即不可用,:-）可用
| 6a9e2647-6eed-459d-8f60-e9ea40ae04df | Open vSwitch agent | YiZhuang_10_1_112_106 | :-)   | True           |
2. 删除OVS agent

mysql> select * from neutron.agents where host='YiZhuang_10_1_112_105'\G;        #没有deleted标志位，所以只能从数据库中删除
*************************** 1. row ***************************
                 id: 4e1c9957-98e5-4516-af2d-6c67a00ecd77
         agent_type: Open vSwitch agent
             binary: neutron-openvswitch-agent
              topic: N/A
               host: YiZhuang_10_1_112_105
     admin_state_up: 1
         created_at: 2015-09-21 03:22:01
         started_at: 2016-01-19 09:15:45
heartbeat_timestamp: 2016-01-19 09:15:45
        description: NULL
     configurations: {"tunnel_types": [], "tunneling_ip": "", "bridge_mappings": {"physnet1": "br-eth1", "physnet0": "br-eth0"}, "l2_population": false, "devices": 0}
1 row in set (0.00 sec)
 
 
mysql> delete from neutron.agents where host='YiZhuang_10_1_112_105'\G;         #删除         
Query OK, 1 row affected (0.00 sec)
 
校验：
[root@controller ~]# neutron agent-list
+--------------------------------------+--------------------+-----------------------+-------+----------------+
| id                                   | agent_type         | host                  | alive | admin_state_up |
+--------------------------------------+--------------------+-----------------------+-------+----------------+
| 0160e0b7-22fb-42d9-8e43-0d13d98db594 | L3 agent           | LuGu_10_1_81_209      | :-)   | True           |
| 6a9e2647-6eed-459d-8f60-e9ea40ae04df | Open vSwitch agent | YiZhuang_10_1_112_106 | :-)   | True           |
4. 总结
   以上是将compute_nodes从opentack中删除的方法，在Juno以上的版本，可以通过nova service-delete <hostname> 的方式将compute nodes删除，在Icehouse以下的版本，暂时未能提供，解决的方法可以通过调用nova db的api，将其删除，后续再补充。或者通过编写的脚本的方式，也可以实现，建议通过调用API的方式，直接修改数据库，而不建议直接修改数据库的方法。